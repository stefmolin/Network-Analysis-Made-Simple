{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hey, thanks for stopping by! Network Analysis Made Simple is a collection of Jupyter notebooks designed to help you get up and running with the NetworkX package in the Python programming langauge. It's written by programmers for programmers, and will give you a basic introduction to graph theory, applied network science, and advanced topics to help kickstart your learning journey. There's even case studies to help those of you for whom example narratives help a ton! We hope you enjoy learning from it. Introduction Videos At the beginning of each \"chapter\", there's an introduction video just like the one you'll see embedded below. Those videos will give you an overview of the chapter, particularly what to look out for and what the learning goals are, and are designed to orient you on the right path. If you're not the audio/visual kind, feel free to skip past them :). Because they're hosted on YouTube, if you need captions, hit the captions button to get access to them. Using the book There are three ways to use this website/web book. Firstly, you can view everything online at this site. Use the navigation to help you get around, or search for a specific topic that you're interested in. Secondly, you can launch a binder session . Binder lets you execute the notebook code inside the book. Click on the Binder button below to get started! Finally, you can pick up the official EPUB/MOBI/PDF version of the book on LeanPub ! Purchasing a copy helps support the authors, and funds future improvements and updates to the book, which you will continue to receive as we make updates! Feedback If you have feedback for the eBook, please head over to our GitHub repository and raise an issue there. Support us! If you find the book useful, you can support the creators in the following ways: Star the repository ! It costs you nothing, and helps raise the profile of the book. Share the website with your colleagues! It also costs you nothing, and helps share the good stuff with those you think might benefit from it. Take the official companion courses and projects on DataCamp! It does cost some money, so we totally understand if you'd prefer not to, but it does buy us coffee :). Support Eric Ma on Patreon with a monthly coffee pledge to keep him caffeinated, which helps him make other good material to share. Follow Eric and Mridul on Twitter at @ericmjl and @Mridul_Seth Purchase the companion book on LeanPub and fund coffee that way too!","title":"Welcome"},{"location":"#introduction-videos","text":"At the beginning of each \"chapter\", there's an introduction video just like the one you'll see embedded below. Those videos will give you an overview of the chapter, particularly what to look out for and what the learning goals are, and are designed to orient you on the right path. If you're not the audio/visual kind, feel free to skip past them :). Because they're hosted on YouTube, if you need captions, hit the captions button to get access to them.","title":"Introduction Videos"},{"location":"#using-the-book","text":"There are three ways to use this website/web book. Firstly, you can view everything online at this site. Use the navigation to help you get around, or search for a specific topic that you're interested in. Secondly, you can launch a binder session . Binder lets you execute the notebook code inside the book. Click on the Binder button below to get started! Finally, you can pick up the official EPUB/MOBI/PDF version of the book on LeanPub ! Purchasing a copy helps support the authors, and funds future improvements and updates to the book, which you will continue to receive as we make updates!","title":"Using the book"},{"location":"#feedback","text":"If you have feedback for the eBook, please head over to our GitHub repository and raise an issue there.","title":"Feedback"},{"location":"#support-us","text":"If you find the book useful, you can support the creators in the following ways: Star the repository ! It costs you nothing, and helps raise the profile of the book. Share the website with your colleagues! It also costs you nothing, and helps share the good stuff with those you think might benefit from it. Take the official companion courses and projects on DataCamp! It does cost some money, so we totally understand if you'd prefer not to, but it does buy us coffee :). Support Eric Ma on Patreon with a monthly coffee pledge to keep him caffeinated, which helps him make other good material to share. Follow Eric and Mridul on Twitter at @ericmjl and @Mridul_Seth Purchase the companion book on LeanPub and fund coffee that way too!","title":"Support us!"},{"location":"learn-more/","text":"Thank you for making it this far! We hope you've enjoyed the book. If you want to further your learning, here's a few resources to keep you going. Academic Books Statistics \"Statistical Analysis of Network Data\" is an incredible resource for learning how to analyze graph data from a statistical viewpoint. It is written by Boston University's professor of mathematics Eric D. Kolaczyck. I used it during graduate school as part of my personnal learning journey. The book's website can be found here , and is available on Amazon (click on the book link below). Network Science This is a book by Prof. Albert-Laszlo Barabasi, and is freely available online . In it, he explores network analysis from the perspective of an applied academic discipline, showing universal properties and processes that underly networks. Think Complexity This is a book by Prof. Allen Downey at the Olin College of Engineering. In fact, this was the first book that exposed me (Eric Ma) to network science and its ideas, which thus inspired my thesis topic, which then gave me the impetus to learn graph theory and make this tutorial. I hope it becomes a useful thing for you too. You can find the book at Green Tea Press for free, but do consider purchasing a copy to support Allen's work! Online Resources Snacks Snacks is a repository of network analysis learning tools curated in the same spirit as the \"Awesome-X\" repositories that show up on GitHub. You can find it here .","title":"Further Learning"},{"location":"learn-more/#academic-books","text":"Statistics \"Statistical Analysis of Network Data\" is an incredible resource for learning how to analyze graph data from a statistical viewpoint. It is written by Boston University's professor of mathematics Eric D. Kolaczyck. I used it during graduate school as part of my personnal learning journey. The book's website can be found here , and is available on Amazon (click on the book link below). Network Science This is a book by Prof. Albert-Laszlo Barabasi, and is freely available online . In it, he explores network analysis from the perspective of an applied academic discipline, showing universal properties and processes that underly networks. Think Complexity This is a book by Prof. Allen Downey at the Olin College of Engineering. In fact, this was the first book that exposed me (Eric Ma) to network science and its ideas, which thus inspired my thesis topic, which then gave me the impetus to learn graph theory and make this tutorial. I hope it becomes a useful thing for you too. You can find the book at Green Tea Press for free, but do consider purchasing a copy to support Allen's work!","title":"Academic Books"},{"location":"learn-more/#online-resources","text":"Snacks Snacks is a repository of network analysis learning tools curated in the same spirit as the \"Awesome-X\" repositories that show up on GitHub. You can find it here .","title":"Online Resources"},{"location":"00-preface/01-setup/","text":"Introduction In order to get the most of this book, you will want to be able to execute the examples in the notebooks, modify them, break the code, and fix it. Pedagogically, that is the best way for you to learn the concepts. Here are the recommended ways in which you can get set up. Binder We recommend the use of Binder! This is because Binder will automagically setup an isolated and ephemeral compute environment for you with all of the packages needed to run the code in your notebooks. As such, you won't have to wrestle with anything at the terminal. To go there, click on the following button: Once you're in there, do a final setup step, by opening up a terminal in the Jupyter session, and installing the custom package nams that we wrote, which contains data loaders and solutions. # In the root directory of the repository python setup.py develop conda environments We also recommend the use of conda environments! If you are feeling confident enough to set up a conda environment at the terminal, then follow along. (We'll be assuming you've already cloned the repository locally.) Leverage the Makefile We've provided a Makefile with a single command: make conda On most *nix systems, that should get you most of the way to having the environment setup. Alternative: Execute individual commands If you encounter errors, then you should know that the Makefile command make conda basically wraps the following steps. Firstly, it creates the conda environment based on the environment.yml file: conda env create -f environment.yml Next, it activates the environment: conda activate nams We have a custom module for the project, which is called nams , that you will have to install. # In the root directory of the cloned repository python setup.py develop Finally, it runs a check on the environment to make sure everything is installed correctly: python checkenv.py venv environments If you're not a conda user, then you can use venv to create your environment. Leverage the Makefile As with the conda commands, you should be able to execute a single Makefile command at your terminal: make venv Special heartfelt thanks goes out to GitHub user @matt-land who contributed the venv script.","title":"Get Setup"},{"location":"00-preface/01-setup/#introduction","text":"In order to get the most of this book, you will want to be able to execute the examples in the notebooks, modify them, break the code, and fix it. Pedagogically, that is the best way for you to learn the concepts. Here are the recommended ways in which you can get set up.","title":"Introduction"},{"location":"00-preface/01-setup/#binder","text":"We recommend the use of Binder! This is because Binder will automagically setup an isolated and ephemeral compute environment for you with all of the packages needed to run the code in your notebooks. As such, you won't have to wrestle with anything at the terminal. To go there, click on the following button: Once you're in there, do a final setup step, by opening up a terminal in the Jupyter session, and installing the custom package nams that we wrote, which contains data loaders and solutions. # In the root directory of the repository python setup.py develop","title":"Binder"},{"location":"00-preface/01-setup/#conda-environments","text":"We also recommend the use of conda environments! If you are feeling confident enough to set up a conda environment at the terminal, then follow along. (We'll be assuming you've already cloned the repository locally.)","title":"conda environments"},{"location":"00-preface/01-setup/#leverage-the-makefile","text":"We've provided a Makefile with a single command: make conda On most *nix systems, that should get you most of the way to having the environment setup.","title":"Leverage the Makefile"},{"location":"00-preface/01-setup/#alternative-execute-individual-commands","text":"If you encounter errors, then you should know that the Makefile command make conda basically wraps the following steps. Firstly, it creates the conda environment based on the environment.yml file: conda env create -f environment.yml Next, it activates the environment: conda activate nams We have a custom module for the project, which is called nams , that you will have to install. # In the root directory of the cloned repository python setup.py develop Finally, it runs a check on the environment to make sure everything is installed correctly: python checkenv.py","title":"Alternative: Execute individual commands"},{"location":"00-preface/01-setup/#venv-environments","text":"If you're not a conda user, then you can use venv to create your environment.","title":"venv environments"},{"location":"00-preface/01-setup/#leverage-the-makefile_1","text":"As with the conda commands, you should be able to execute a single Makefile command at your terminal: make venv Special heartfelt thanks goes out to GitHub user @matt-land who contributed the venv script.","title":"Leverage the Makefile"},{"location":"00-preface/02-prereqs/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); To get maximum benefit from this book, you should know how to program in Python. (Hint: it's an extremely useful skill to know!) In particular, knowing how to: use dictionaries, write list comprehensions, and handle pandas DataFrames, will help you a ton during the tutorial. Exercises We have a few exercises below that should help you get warmed up. Exercise 1 Given the following line of code: [ s for s in my_fav_things if s [ \u2018 name \u2019 ] == \u2018 raindrops on roses \u2019 ] What are plausible data structures for s and my_fav_things ? Exercise 2 Given the following data: names = [ { 'name' : 'Eric' , 'surname' : 'Ma' }, { 'name' : 'Jeffrey' , 'surname' : 'Elmer' }, { 'name' : 'Mike' , 'surname' : 'Lee' }, { 'name' : 'Jennifer' , 'surname' : 'Elmer' } ] Write a function that takes in the names list of dictionaries and returns the dictionaries in which the surname value matches exactly some query_surname . def find_persons_with_surname ( persons , query_surname ): # Assert that the persons parameter is a list. # This is a good defensive programming practice. assert isinstance ( persons , list ) results = [] for ______ in ______ : if ___________ == __________ : results . append ( ________ ) return results To test your implementation, check it with the following code. No errors should be raised. # Test your result below. # results = find_persons_with_surname(names, 'Lee') # assert len(results) == 1 # results = find_persons_with_surname(names, 'Elmer') # assert len(results) == 2","title":"Prerequisites"},{"location":"00-preface/02-prereqs/#exercises","text":"We have a few exercises below that should help you get warmed up.","title":"Exercises"},{"location":"00-preface/02-prereqs/#exercise-1","text":"Given the following line of code: [ s for s in my_fav_things if s [ \u2018 name \u2019 ] == \u2018 raindrops on roses \u2019 ] What are plausible data structures for s and my_fav_things ?","title":"Exercise 1"},{"location":"00-preface/02-prereqs/#exercise-2","text":"Given the following data: names = [ { 'name' : 'Eric' , 'surname' : 'Ma' }, { 'name' : 'Jeffrey' , 'surname' : 'Elmer' }, { 'name' : 'Mike' , 'surname' : 'Lee' }, { 'name' : 'Jennifer' , 'surname' : 'Elmer' } ] Write a function that takes in the names list of dictionaries and returns the dictionaries in which the surname value matches exactly some query_surname . def find_persons_with_surname ( persons , query_surname ): # Assert that the persons parameter is a list. # This is a good defensive programming practice. assert isinstance ( persons , list ) results = [] for ______ in ______ : if ___________ == __________ : results . append ( ________ ) return results To test your implementation, check it with the following code. No errors should be raised. # Test your result below. # results = find_persons_with_surname(names, 'Lee') # assert len(results) == 1 # results = find_persons_with_surname(names, 'Elmer') # assert len(results) == 2","title":"Exercise 2"},{"location":"00-preface/03-goals/","text":"Our learning goals for you with this book can be split into the technical and the intellectual. Technical Takeaways Firstly, we would like to equip you to be familiar with the NetworkX application programming interface (API). The reason for choosing NetworkX is because it is extremely beginner-friendly, and has an API that matches graph theory concepts very closely. Secondly, we would like to show you how you can visualize graph data in a fashion that doesn't involve showing mere hairballs. Throughout the book, you will see examples of what we call rational graph visualizations . One of our authors, Eric Ma, has developed a companion package, nxviz , that provides a declarative and convenient API (in other words an attempt at a \"grammar\") for graph visualization. Thirdly, in this book, you will be introduced to basic graph algorithms, such as finding special graph structures, or finding paths in a graph. Graph algorithms will show you how to \"think on graphs\", and knowing how to do so will broaden your ability to interact with graph data structures. Fourthly, you will also be equipped with the connection between graph theory and other areas of math and computing, such as statistical inference and linear algebra. Intellectual Goals Beyond the technical takeaways, we hope to broaden how you think about data. The first idea we hope to give you the ability to think about your data in terms of \"relationships\". As you will learn, relationships are what give rise to the interestingness of graphs. That's where relational insights can come to fore. The second idea we hope to give you is the ability to \"think on graphs\". This comes with practice. Once you master it, though, you will find yourself becoming more and more familiar with algorithmic thinking . which is where you look at a problem in terms of the algorithm that solves it.","title":"Learning Goals"},{"location":"00-preface/03-goals/#technical-takeaways","text":"Firstly, we would like to equip you to be familiar with the NetworkX application programming interface (API). The reason for choosing NetworkX is because it is extremely beginner-friendly, and has an API that matches graph theory concepts very closely. Secondly, we would like to show you how you can visualize graph data in a fashion that doesn't involve showing mere hairballs. Throughout the book, you will see examples of what we call rational graph visualizations . One of our authors, Eric Ma, has developed a companion package, nxviz , that provides a declarative and convenient API (in other words an attempt at a \"grammar\") for graph visualization. Thirdly, in this book, you will be introduced to basic graph algorithms, such as finding special graph structures, or finding paths in a graph. Graph algorithms will show you how to \"think on graphs\", and knowing how to do so will broaden your ability to interact with graph data structures. Fourthly, you will also be equipped with the connection between graph theory and other areas of math and computing, such as statistical inference and linear algebra.","title":"Technical Takeaways"},{"location":"00-preface/03-goals/#intellectual-goals","text":"Beyond the technical takeaways, we hope to broaden how you think about data. The first idea we hope to give you the ability to think about your data in terms of \"relationships\". As you will learn, relationships are what give rise to the interestingness of graphs. That's where relational insights can come to fore. The second idea we hope to give you is the ability to \"think on graphs\". This comes with practice. Once you master it, though, you will find yourself becoming more and more familiar with algorithmic thinking . which is where you look at a problem in terms of the algorithm that solves it.","title":"Intellectual Goals"},{"location":"00-preface/preface/","text":"Hey, thanks for picking up this e-Book. We had a ton of fun making the material, and we hope you have a ton of fun learning new things from it too. Applied network analysis, and graph theory concepts, are getting more and more relevant in our world. Graph problems are abound. Once you pick up how to use graphs in an applied setting, you'll find your view of data problems change tremendously. We hope this book can become part of your learning journey. The act of purchasing this book means you've chosen to support us, the authors. It means a ton to us, as this book is the culmination of 5 years of learning and teaching applied network analysis at conferences around the world. The reason we went with LeanPub to publish this book is this: For as long as we issue updates to the book, you will also receive an updated copy of it. And because the book is digital, it's easy for us to get updates out to you. Just so you know, the full text of the book is available online too, at the accompanying website, https://ericmjl.github.io/Network-Analysis-Made-Simple. On there, you'll find a link to Binder so you can interact with the code, and through the act of playing around with the code and breaking it yourself, learn new things. (Breaking code and fixing it is something you should be doing - it's one of the best ways to learn!) If you have questions about the content, or find an errata that you'd like to point out, please head over to https://github.com/ericmjl/Network-Analysis-Made-Simple/, and post an issue up there. We'll be sure to address it and acknowledge it appropriately. We hope that this book becomes a stepping stone in your learning journey. Enjoy! Eric & Mridul","title":"Preface"},{"location":"01-introduction/01-graphs/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"k4KHoLC7TFE\" , width = \"100%\" ) In our world, networks are an immensely useful data modelling tool to model complex relational problems. Building on top of a network-oriented data model, they have been put to great use in a wide variety of settings. A formal definition of networks Before we explore examples of networks, we want to first give you a more formal definition of what networks are. The reason is that knowing a formal definition helps us refine our application of networks. So bear with me for a moment. In the slightly more academic literature, networks are more formally referred to as graphs . Graphs are comprised of two sets of objects: A node set : the \"entities\" in a graph. An edge set : the record of \"relationships\" between the entities in the graph. For example, if a node set n n is comprised of elements: n = \\{a, b, c, d, ...\\} n = \\{a, b, c, d, ...\\} Then, the edge set e e would be represented as tuples of pairs of elements: e = \\{(a, b), (a, c), (c, d), ...\\} e = \\{(a, b), (a, c), (c, d), ...\\} If you extracted every node from the edge set e e , it should form at least a subset of the node set n n . (It is at least a subset because not every node in n n might participate in an edge.) If you draw out a network, the \"nodes\" are commonly represented as shapes, such as circles, while the \"edges\" are the lines between the shapes. Examples of Networks Now that we have a proper definition of a graph, let's move on to explore examples of graphs. One example I (Eric Ma) am fond of, based on my background as a biologist, is a protein-protein interaction network. Here, the graph can be defined in the following way: nodes/entities are the proteins, edges/relationships are defined as \"one protein is known to bind with another\". A more colloquial example of networks is an air transportation network. Here, the graph can be defined in the following way: nodes/entities are airports edges/relationships are defined as \"at least one flight carrier flies between the airports\". And another even more relatable example would be our ever-prevalent social networks! With Twitter, the graph can be defined in the following way: nodes/entities are individual users edges/relationships are defined as \"one user has decided to follow another\". Now that you've seen the framework for defining a graph, we'd like to invite you to answer the following question: What examples of networks have you seen before in your profession? Go ahead and list it out. Types of Graphs As you probably can see, graphs are a really flexible data model for modelling the world, as long as the nodes and edges are strictly defined. (If the nodes and edges are sloppily defined, well, we run into a lot of interpretability problems later on.) If you are a member of both LinkedIn and Twitter, you might intuitively think that there's a slight difference in the structure of the two \"social graphs\". You'd be absolutely correct on that count! Twitter is an example of what we would intuitively call a directed graph. Why is this so? The key here lies in how interactions are modelled. One user can follow another, but the other need not necessarily follow back. As such, there is a directionality to the relationship. LinkedIn is an example of what we would intuitively call an undirected graph. Why is this so? The key here is that when two users are LinkedIn connections, we automatically assign a bi-directional edge between them. As such, for convenience, we can collapse the bi-directional edge into an undirected edge, thus yielding an undirected graph. If we wanted to turn LinkedIn into a directed graph, we might want to keep information on who initiated the invitation. In that way, the relationship is automatically bi-directional. Edges define the interesting part of a graph While in graduate school, I (Eric Ma) once sat in a seminar organized by one of the professors on my thesis committee. The speaker that day was John Quackenbush, a faculty member of the Harvard School of Public Health. While the topic of the day remained fuzzy in my memory, one quote stood out: The heart of a graph lies in its edges, not in its nodes. (John Quackenbush, Harvard School of Public Health) Indeed, this is a key point to remember! Without edges, the nodes are merely collections of entities. In a data table, they would correspond to the rows. That alone can be interesting, but doesn't yield relational insights between the entities.","title":"Chapter 1: Introduction to Graphs"},{"location":"01-introduction/01-graphs/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"k4KHoLC7TFE\" , width = \"100%\" ) In our world, networks are an immensely useful data modelling tool to model complex relational problems. Building on top of a network-oriented data model, they have been put to great use in a wide variety of settings.","title":"Introduction"},{"location":"01-introduction/01-graphs/#a-formal-definition-of-networks","text":"Before we explore examples of networks, we want to first give you a more formal definition of what networks are. The reason is that knowing a formal definition helps us refine our application of networks. So bear with me for a moment. In the slightly more academic literature, networks are more formally referred to as graphs . Graphs are comprised of two sets of objects: A node set : the \"entities\" in a graph. An edge set : the record of \"relationships\" between the entities in the graph. For example, if a node set n n is comprised of elements: n = \\{a, b, c, d, ...\\} n = \\{a, b, c, d, ...\\} Then, the edge set e e would be represented as tuples of pairs of elements: e = \\{(a, b), (a, c), (c, d), ...\\} e = \\{(a, b), (a, c), (c, d), ...\\} If you extracted every node from the edge set e e , it should form at least a subset of the node set n n . (It is at least a subset because not every node in n n might participate in an edge.) If you draw out a network, the \"nodes\" are commonly represented as shapes, such as circles, while the \"edges\" are the lines between the shapes.","title":"A formal definition of networks"},{"location":"01-introduction/01-graphs/#examples-of-networks","text":"Now that we have a proper definition of a graph, let's move on to explore examples of graphs. One example I (Eric Ma) am fond of, based on my background as a biologist, is a protein-protein interaction network. Here, the graph can be defined in the following way: nodes/entities are the proteins, edges/relationships are defined as \"one protein is known to bind with another\". A more colloquial example of networks is an air transportation network. Here, the graph can be defined in the following way: nodes/entities are airports edges/relationships are defined as \"at least one flight carrier flies between the airports\". And another even more relatable example would be our ever-prevalent social networks! With Twitter, the graph can be defined in the following way: nodes/entities are individual users edges/relationships are defined as \"one user has decided to follow another\". Now that you've seen the framework for defining a graph, we'd like to invite you to answer the following question: What examples of networks have you seen before in your profession? Go ahead and list it out.","title":"Examples of Networks"},{"location":"01-introduction/01-graphs/#types-of-graphs","text":"As you probably can see, graphs are a really flexible data model for modelling the world, as long as the nodes and edges are strictly defined. (If the nodes and edges are sloppily defined, well, we run into a lot of interpretability problems later on.) If you are a member of both LinkedIn and Twitter, you might intuitively think that there's a slight difference in the structure of the two \"social graphs\". You'd be absolutely correct on that count! Twitter is an example of what we would intuitively call a directed graph. Why is this so? The key here lies in how interactions are modelled. One user can follow another, but the other need not necessarily follow back. As such, there is a directionality to the relationship. LinkedIn is an example of what we would intuitively call an undirected graph. Why is this so? The key here is that when two users are LinkedIn connections, we automatically assign a bi-directional edge between them. As such, for convenience, we can collapse the bi-directional edge into an undirected edge, thus yielding an undirected graph. If we wanted to turn LinkedIn into a directed graph, we might want to keep information on who initiated the invitation. In that way, the relationship is automatically bi-directional.","title":"Types of Graphs"},{"location":"01-introduction/01-graphs/#edges-define-the-interesting-part-of-a-graph","text":"While in graduate school, I (Eric Ma) once sat in a seminar organized by one of the professors on my thesis committee. The speaker that day was John Quackenbush, a faculty member of the Harvard School of Public Health. While the topic of the day remained fuzzy in my memory, one quote stood out: The heart of a graph lies in its edges, not in its nodes. (John Quackenbush, Harvard School of Public Health) Indeed, this is a key point to remember! Without edges, the nodes are merely collections of entities. In a data table, they would correspond to the rows. That alone can be interesting, but doesn't yield relational insights between the entities.","title":"Edges define the interesting part of a graph"},{"location":"01-introduction/02-networkx-intro/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' Unable to revert mtime: /usr/share/fonts Unable to revert mtime: /usr/share/fonts/truetype Unable to revert mtime: /usr/share/fonts/truetype/dejavu Unable to revert mtime: /usr/share/fonts/truetype/lato Unable to revert mtime: /usr/share/fonts/truetype/liberation Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = 'sdF0uJo2KdU' , width = \"100%\" ) In this chapter, we will introduce you to the NetworkX API. This will allow you to create and manipulate graphs in your computer memory, thus giving you a language to more concretely explore graph theory ideas. Throughout the book, we will be using different graph datasets to help us anchor ideas. In this section, we will work with a social network of seventh graders. Here, nodes are individual students, and edges represent their relationships. Edges between individuals show how often the seventh graders indicated other seventh graders as their favourite. The data are taken from the Konect graph data repository Data Model In NetworkX, graph data are stored in a dictionary-like fashion. They are placed under a Graph object, canonically instantiated with the variable G as follows: G = nx . Graph () Of course, you are free to name the graph anything you want! Nodes are part of the attribute G.nodes . There, the node data are housed in a dictionary-like container, where the key is the node itself and the values are a dictionary of attributes. Node data are accessible using syntax that looks like: G . nodes [ node1 ] Edges are part of the attribute G.edges , which is also stored in a dictionary-like container. Edge data are accessible using syntax that looks like: G . edges [ node1 , node2 ] Because of the dictionary-like implementation of the graph, any hashable object can be a node. This means strings and tuples, but not lists and sets. Load Data Let's load some real network data to get a feel for the NetworkX API. This dataset comes from a study of 7th grade students. This directed network contains proximity ratings between students from 29 seventh grade students from a school in Victoria. Among other questions the students were asked to nominate their preferred classmates for three different activities. A node represents a student. An edge between two nodes shows that the left student picked the right student as his or her answer. The edge weights are between 1 and 3 and show how often the left student chose the right student as his/her favourite. In the original dataset, students were from an all-boys school. However, I have modified the dataset to instead be a mixed-gender school. import networkx as nx from datetime import datetime import matplotlib.pyplot as plt import numpy as np import warnings from nams import load_data as cf warnings . filterwarnings ( 'ignore' ) G = cf . load_seventh_grader_network () Understanding a graph's basic statistics When you get graph data, one of the first things you'll want to do is to check its basic graph statistics: the number of nodes and the number of edges that are represented in the graph. This is a basic sanity-check on your data that you don't want to skip out on. Querying graph type The first thing you need to know is the type of the graph: type ( G ) networkx.classes.digraph.DiGraph Because the graph is a DiGraph , this tells us that the graph is a directed one. If it were undirected, the type would change: H = nx . Graph () type ( H ) networkx.classes.graph.Graph Querying node information Let's now query for the nodeset: list ( G . nodes ())[ 0 : 5 ] [1, 2, 3, 4, 5] G.nodes() returns a \"view\" on the nodes. We can't actually slice into the view and grab out a sub-selection, but we can at least see what nodes are present. For brevity, we have sliced into G.nodes() passed into a list() constructor, so that we don't pollute the output. Because a NodeView is iterable, though, we can query it for its length: len ( G . nodes ()) 29 If our nodes have metadata attached to them, we can view the metadata at the same time by passing in data=True : list ( G . nodes ( data = True ))[ 0 : 5 ] [(1, {'gender': 'male'}), (2, {'gender': 'male'}), (3, {'gender': 'male'}), (4, {'gender': 'male'}), (5, {'gender': 'male'})] G.nodes(data=True) returns a NodeDataView , which you can see is dictionary-like. Additionally, we can select out individual nodes: G . nodes [ 1 ] {'gender': 'male'} Now, because a NodeDataView is dictionary-like, looping over G.nodes(data=True) is very much like looping over key-value pairs of a dictionary. As such, we can write things like: for n , d in G . nodes ( data = True ): # n is the node # d is the metadata dictionary ... This is analogous to how we would loop over a dictionary: for k , v in dictionary . items (): # do stuff in the loop Naturally, this leads us to our first exercise. Exercise: Summarizing node metadata Can you count how many males and females are represented in the graph? from nams.solutions.intro import node_metadata #### REPLACE THE NEXT LINE WITH YOUR ANSWER mf_counts = node_metadata ( G ) Test your implementation by checking it against the test_answer function below. from typing import Dict def test_answer ( mf_counts : Dict ): assert mf_counts [ 'female' ] == 17 assert mf_counts [ 'male' ] == 12 test_answer ( mf_counts ) With this dictionary-like syntax, we can query back the metadata that's associated with any node. Querying edge information Now that you've learned how to query for node information, let's now see how to query for all of the edges in the graph: list ( G . edges ())[ 0 : 5 ] [(1, 2), (1, 3), (1, 4), (1, 5), (1, 6)] Similar to the NodeView , G.edges() returns an EdgeView that is also iterable. As with above, we have abbreviated the output inside a sliced list to keep things readable. Because G.edges() is iterable, we can get its length to see the number of edges that are present in a graph. len ( G . edges ()) 376 Likewise, we can also query for all of the edge's metadata: list ( G . edges ( data = True ))[ 0 : 5 ] [(1, 2, {'count': 1}), (1, 3, {'count': 1}), (1, 4, {'count': 2}), (1, 5, {'count': 2}), (1, 6, {'count': 3})] Additionally, it is possible for us to select out individual edges, as long as they exist in the graph: G . edges [ 15 , 10 ] {'count': 2} This yields the metadata dictionary for that edge. If the edge does not exist, then we get an error: >>> G . edges [ 15 , 16 ] --------------------------------------------------------------------------- KeyError Traceback ( most recent call last ) < ipython - input - 21 - ce014cab875a > in < module > ----> 1 G . edges [ 15 , 16 ] ~/ anaconda / envs / nams / lib / python3 .7 / site - packages / networkx / classes / reportviews . py in __getitem__ ( self , e ) 928 def __getitem__ ( self , e ): 929 u , v = e --> 930 return self . _adjdict [ u ][ v ] 931 932 # EdgeDataView methods KeyError : 16 As with the NodeDataView , the EdgeDataView is dictionary-like, with the difference being that the keys are 2-tuple-like instead of being single hashable objects. Thus, we can write syntax like the following to loop over the edgelist: for n1 , n2 , d in G . edges ( data = True ): # n1, n2 are the nodes # d is the metadata dictionary ... Naturally, this leads us to our next exercise. Exercise: Summarizing edge metadata Can you write code to verify that the maximum times any student rated another student as their favourite is 3 times? from nams.solutions.intro import edge_metadata #### REPLACE THE NEXT LINE WITH YOUR ANSWER maxcount = edge_metadata ( G ) Likewise, you can test your answer using the test function below: def test_maxcount ( maxcount ): assert maxcount == 3 test_maxcount ( maxcount ) Manipulating the graph Great stuff! You now know how to query a graph for: its node set, optionally including metadata individual node metadata its edge set, optionally including metadata, and individual edges' metadata Now, let's learn how to manipulate the graph. Specifically, we'll learn how to add nodes and edges to a graph. Adding Nodes The NetworkX graph API lets you add a node easily: G . add_node ( node , node_data1 = some_value , node_data2 = some_value ) Adding Edges It also allows you to add an edge easily: G . add_edge ( node1 , node2 , edge_data1 = some_value , edge_data2 = some_value ) Metadata by Keyword Arguments In both cases, the keyword arguments that are passed into .add_node() are automatically collected into the metadata dictionary. Knowing this gives you enough knowledge to tackle the next exercise. Exercise: adding students to the graph We found out that there are two students that we left out of the network, student no. 30 and 31. They are one male (30) and one female (31), and they are a pair that just love hanging out with one another and with individual 7 (i.e. count=3 ), in both directions per pair. Add this information to the graph. from nams.solutions.intro import adding_students #### REPLACE THE NEXT LINE WITH YOUR ANSWER G = adding_students ( G ) You can verify that the graph has been correctly created by executing the test function below. def test_graph_integrity ( G ): assert 30 in G . nodes () assert 31 in G . nodes () assert G . nodes [ 30 ][ 'gender' ] == 'male' assert G . nodes [ 31 ][ 'gender' ] == 'female' assert G . has_edge ( 30 , 31 ) assert G . has_edge ( 30 , 7 ) assert G . has_edge ( 31 , 7 ) assert G . edges [ 30 , 7 ][ 'count' ] == 3 assert G . edges [ 7 , 30 ][ 'count' ] == 3 assert G . edges [ 31 , 7 ][ 'count' ] == 3 assert G . edges [ 7 , 31 ][ 'count' ] == 3 assert G . edges [ 30 , 31 ][ 'count' ] == 3 assert G . edges [ 31 , 30 ][ 'count' ] == 3 print ( 'All tests passed.' ) test_graph_integrity ( G ) All tests passed. Coding Patterns These are some recommended coding patterns when doing network analysis using NetworkX, which stem from my personal experience with the package. Iterating using List Comprehensions I would recommend that you use the following for compactness: [ d [ 'attr' ] for n , d in G . nodes ( data = True )] And if the node is unimportant, you can do: [ d [ 'attr' ] for _ , d in G . nodes ( data = True )] Iterating over Edges using List Comprehensions A similar pattern can be used for edges: [ n2 for n1 , n2 , d in G . edges ( data = True )] or [ n2 for _ , n2 , d in G . edges ( data = True )] If the graph you are constructing is a directed graph, with a \"source\" and \"sink\" available, then I would recommend the following naming of variables instead: [( sc , sk ) for sc , sk , d in G . edges ( data = True )] or [ d [ 'attr' ] for sc , sk , d in G . edges ( data = True )] Further Reading For a deeper look at the NetworkX API, be sure to check out the NetworkX docs . Further Exercises Here's some further exercises that you can use to get some practice. Exercise: Unrequited Friendships Try figuring out which students have \"unrequited\" friendships, that is, they have rated another student as their favourite at least once, but that other student has not rated them as their favourite at least once. Hint: the goal here is to get a list of edges for which the reverse edge is not present. Hint: You may need the class method G.has_edge(n1, n2) . This returns whether a graph has an edge between the nodes n1 and n2 . from nams.solutions.intro import unrequitted_friendships_v1 #### REPLACE THE NEXT LINE WITH YOUR ANSWER unrequitted_friendships = unrequitted_friendships_v1 ( G ) assert len ( unrequitted_friendships ) == 124 In a previous session at ODSC East 2018, a few other class participants provided the following solutions, which you can take a look at by uncommenting the following cells. This first one by @schwanne is the list comprehension version of the above solution: from nams.solutions.intro import unrequitted_friendships_v2 # unrequitted_friendships_v2?? This one by @end0 is a unique one involving sets. from nams.solutions.intro import unrequitted_friendships_v3 # unrequitted_friendships_v3?? Solution Answers Here are the answers to the exercises above. import nams.solutions.intro as solutions import inspect print ( inspect . getsource ( solutions )) \"\"\" Solutions to Intro Chapter. \"\"\" def node_metadata(G): \"\"\"Counts of students of each gender.\"\"\" from collections import Counter mf_counts = Counter([d[\"gender\"] for n, d in G.nodes(data=True)]) return mf_counts def edge_metadata(G): \"\"\"Maximum number of times that a student rated another student.\"\"\" counts = [d[\"count\"] for n1, n2, d in G.edges(data=True)] maxcount = max(counts) return maxcount def adding_students(G): \"\"\"How to nodes and edges to a graph.\"\"\" G = G.copy() G.add_node(30, gender=\"male\") G.add_node(31, gender=\"female\") G.add_edge(30, 31, count=3) G.add_edge(31, 30, count=3) # reverse is optional in undirected network G.add_edge(30, 7, count=3) # but this network is directed G.add_edge(7, 30, count=3) G.add_edge(31, 7, count=3) G.add_edge(7, 31, count=3) return G def unrequitted_friendships_v1(G): \"\"\"Answer to unrequitted friendships problem.\"\"\" unrequitted_friendships = [] for n1, n2 in G.edges(): if not G.has_edge(n2, n1): unrequitted_friendships.append((n1, n2)) return unrequitted_friendships def unrequitted_friendships_v2(G): \"\"\"Alternative answer to unrequitted friendships problem. By @schwanne.\"\"\" return len([(n1, n2) for n1, n2 in G.edges() if not G.has_edge(n2, n1)]) def unrequitted_friendships_v3(G): \"\"\"Alternative answer to unrequitted friendships problem. By @end0.\"\"\" links = ((n1, n2) for n1, n2, d in G.edges(data=True)) reverse_links = ((n2, n1) for n1, n2, d in G.edges(data=True)) return len(list(set(links) - set(reverse_links)))","title":"Chapter 2: The NetworkX API"},{"location":"01-introduction/02-networkx-intro/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = 'sdF0uJo2KdU' , width = \"100%\" ) In this chapter, we will introduce you to the NetworkX API. This will allow you to create and manipulate graphs in your computer memory, thus giving you a language to more concretely explore graph theory ideas. Throughout the book, we will be using different graph datasets to help us anchor ideas. In this section, we will work with a social network of seventh graders. Here, nodes are individual students, and edges represent their relationships. Edges between individuals show how often the seventh graders indicated other seventh graders as their favourite. The data are taken from the Konect graph data repository","title":"Introduction"},{"location":"01-introduction/02-networkx-intro/#data-model","text":"In NetworkX, graph data are stored in a dictionary-like fashion. They are placed under a Graph object, canonically instantiated with the variable G as follows: G = nx . Graph () Of course, you are free to name the graph anything you want! Nodes are part of the attribute G.nodes . There, the node data are housed in a dictionary-like container, where the key is the node itself and the values are a dictionary of attributes. Node data are accessible using syntax that looks like: G . nodes [ node1 ] Edges are part of the attribute G.edges , which is also stored in a dictionary-like container. Edge data are accessible using syntax that looks like: G . edges [ node1 , node2 ] Because of the dictionary-like implementation of the graph, any hashable object can be a node. This means strings and tuples, but not lists and sets.","title":"Data Model"},{"location":"01-introduction/02-networkx-intro/#load-data","text":"Let's load some real network data to get a feel for the NetworkX API. This dataset comes from a study of 7th grade students. This directed network contains proximity ratings between students from 29 seventh grade students from a school in Victoria. Among other questions the students were asked to nominate their preferred classmates for three different activities. A node represents a student. An edge between two nodes shows that the left student picked the right student as his or her answer. The edge weights are between 1 and 3 and show how often the left student chose the right student as his/her favourite. In the original dataset, students were from an all-boys school. However, I have modified the dataset to instead be a mixed-gender school. import networkx as nx from datetime import datetime import matplotlib.pyplot as plt import numpy as np import warnings from nams import load_data as cf warnings . filterwarnings ( 'ignore' ) G = cf . load_seventh_grader_network ()","title":"Load Data"},{"location":"01-introduction/02-networkx-intro/#understanding-a-graphs-basic-statistics","text":"When you get graph data, one of the first things you'll want to do is to check its basic graph statistics: the number of nodes and the number of edges that are represented in the graph. This is a basic sanity-check on your data that you don't want to skip out on.","title":"Understanding a graph's basic statistics"},{"location":"01-introduction/02-networkx-intro/#querying-graph-type","text":"The first thing you need to know is the type of the graph: type ( G ) networkx.classes.digraph.DiGraph Because the graph is a DiGraph , this tells us that the graph is a directed one. If it were undirected, the type would change: H = nx . Graph () type ( H ) networkx.classes.graph.Graph","title":"Querying graph type"},{"location":"01-introduction/02-networkx-intro/#querying-node-information","text":"Let's now query for the nodeset: list ( G . nodes ())[ 0 : 5 ] [1, 2, 3, 4, 5] G.nodes() returns a \"view\" on the nodes. We can't actually slice into the view and grab out a sub-selection, but we can at least see what nodes are present. For brevity, we have sliced into G.nodes() passed into a list() constructor, so that we don't pollute the output. Because a NodeView is iterable, though, we can query it for its length: len ( G . nodes ()) 29 If our nodes have metadata attached to them, we can view the metadata at the same time by passing in data=True : list ( G . nodes ( data = True ))[ 0 : 5 ] [(1, {'gender': 'male'}), (2, {'gender': 'male'}), (3, {'gender': 'male'}), (4, {'gender': 'male'}), (5, {'gender': 'male'})] G.nodes(data=True) returns a NodeDataView , which you can see is dictionary-like. Additionally, we can select out individual nodes: G . nodes [ 1 ] {'gender': 'male'} Now, because a NodeDataView is dictionary-like, looping over G.nodes(data=True) is very much like looping over key-value pairs of a dictionary. As such, we can write things like: for n , d in G . nodes ( data = True ): # n is the node # d is the metadata dictionary ... This is analogous to how we would loop over a dictionary: for k , v in dictionary . items (): # do stuff in the loop Naturally, this leads us to our first exercise.","title":"Querying node information"},{"location":"01-introduction/02-networkx-intro/#exercise-summarizing-node-metadata","text":"Can you count how many males and females are represented in the graph? from nams.solutions.intro import node_metadata #### REPLACE THE NEXT LINE WITH YOUR ANSWER mf_counts = node_metadata ( G ) Test your implementation by checking it against the test_answer function below. from typing import Dict def test_answer ( mf_counts : Dict ): assert mf_counts [ 'female' ] == 17 assert mf_counts [ 'male' ] == 12 test_answer ( mf_counts ) With this dictionary-like syntax, we can query back the metadata that's associated with any node.","title":"Exercise: Summarizing node metadata"},{"location":"01-introduction/02-networkx-intro/#querying-edge-information","text":"Now that you've learned how to query for node information, let's now see how to query for all of the edges in the graph: list ( G . edges ())[ 0 : 5 ] [(1, 2), (1, 3), (1, 4), (1, 5), (1, 6)] Similar to the NodeView , G.edges() returns an EdgeView that is also iterable. As with above, we have abbreviated the output inside a sliced list to keep things readable. Because G.edges() is iterable, we can get its length to see the number of edges that are present in a graph. len ( G . edges ()) 376 Likewise, we can also query for all of the edge's metadata: list ( G . edges ( data = True ))[ 0 : 5 ] [(1, 2, {'count': 1}), (1, 3, {'count': 1}), (1, 4, {'count': 2}), (1, 5, {'count': 2}), (1, 6, {'count': 3})] Additionally, it is possible for us to select out individual edges, as long as they exist in the graph: G . edges [ 15 , 10 ] {'count': 2} This yields the metadata dictionary for that edge. If the edge does not exist, then we get an error: >>> G . edges [ 15 , 16 ] --------------------------------------------------------------------------- KeyError Traceback ( most recent call last ) < ipython - input - 21 - ce014cab875a > in < module > ----> 1 G . edges [ 15 , 16 ] ~/ anaconda / envs / nams / lib / python3 .7 / site - packages / networkx / classes / reportviews . py in __getitem__ ( self , e ) 928 def __getitem__ ( self , e ): 929 u , v = e --> 930 return self . _adjdict [ u ][ v ] 931 932 # EdgeDataView methods KeyError : 16 As with the NodeDataView , the EdgeDataView is dictionary-like, with the difference being that the keys are 2-tuple-like instead of being single hashable objects. Thus, we can write syntax like the following to loop over the edgelist: for n1 , n2 , d in G . edges ( data = True ): # n1, n2 are the nodes # d is the metadata dictionary ... Naturally, this leads us to our next exercise.","title":"Querying edge information"},{"location":"01-introduction/02-networkx-intro/#exercise-summarizing-edge-metadata","text":"Can you write code to verify that the maximum times any student rated another student as their favourite is 3 times? from nams.solutions.intro import edge_metadata #### REPLACE THE NEXT LINE WITH YOUR ANSWER maxcount = edge_metadata ( G ) Likewise, you can test your answer using the test function below: def test_maxcount ( maxcount ): assert maxcount == 3 test_maxcount ( maxcount )","title":"Exercise: Summarizing edge metadata"},{"location":"01-introduction/02-networkx-intro/#manipulating-the-graph","text":"Great stuff! You now know how to query a graph for: its node set, optionally including metadata individual node metadata its edge set, optionally including metadata, and individual edges' metadata Now, let's learn how to manipulate the graph. Specifically, we'll learn how to add nodes and edges to a graph.","title":"Manipulating the graph"},{"location":"01-introduction/02-networkx-intro/#adding-nodes","text":"The NetworkX graph API lets you add a node easily: G . add_node ( node , node_data1 = some_value , node_data2 = some_value )","title":"Adding Nodes"},{"location":"01-introduction/02-networkx-intro/#adding-edges","text":"It also allows you to add an edge easily: G . add_edge ( node1 , node2 , edge_data1 = some_value , edge_data2 = some_value )","title":"Adding Edges"},{"location":"01-introduction/02-networkx-intro/#metadata-by-keyword-arguments","text":"In both cases, the keyword arguments that are passed into .add_node() are automatically collected into the metadata dictionary. Knowing this gives you enough knowledge to tackle the next exercise.","title":"Metadata by Keyword Arguments"},{"location":"01-introduction/02-networkx-intro/#exercise-adding-students-to-the-graph","text":"We found out that there are two students that we left out of the network, student no. 30 and 31. They are one male (30) and one female (31), and they are a pair that just love hanging out with one another and with individual 7 (i.e. count=3 ), in both directions per pair. Add this information to the graph. from nams.solutions.intro import adding_students #### REPLACE THE NEXT LINE WITH YOUR ANSWER G = adding_students ( G ) You can verify that the graph has been correctly created by executing the test function below. def test_graph_integrity ( G ): assert 30 in G . nodes () assert 31 in G . nodes () assert G . nodes [ 30 ][ 'gender' ] == 'male' assert G . nodes [ 31 ][ 'gender' ] == 'female' assert G . has_edge ( 30 , 31 ) assert G . has_edge ( 30 , 7 ) assert G . has_edge ( 31 , 7 ) assert G . edges [ 30 , 7 ][ 'count' ] == 3 assert G . edges [ 7 , 30 ][ 'count' ] == 3 assert G . edges [ 31 , 7 ][ 'count' ] == 3 assert G . edges [ 7 , 31 ][ 'count' ] == 3 assert G . edges [ 30 , 31 ][ 'count' ] == 3 assert G . edges [ 31 , 30 ][ 'count' ] == 3 print ( 'All tests passed.' ) test_graph_integrity ( G ) All tests passed.","title":"Exercise: adding students to the graph"},{"location":"01-introduction/02-networkx-intro/#coding-patterns","text":"These are some recommended coding patterns when doing network analysis using NetworkX, which stem from my personal experience with the package.","title":"Coding Patterns"},{"location":"01-introduction/02-networkx-intro/#iterating-using-list-comprehensions","text":"I would recommend that you use the following for compactness: [ d [ 'attr' ] for n , d in G . nodes ( data = True )] And if the node is unimportant, you can do: [ d [ 'attr' ] for _ , d in G . nodes ( data = True )]","title":"Iterating using List Comprehensions"},{"location":"01-introduction/02-networkx-intro/#iterating-over-edges-using-list-comprehensions","text":"A similar pattern can be used for edges: [ n2 for n1 , n2 , d in G . edges ( data = True )] or [ n2 for _ , n2 , d in G . edges ( data = True )] If the graph you are constructing is a directed graph, with a \"source\" and \"sink\" available, then I would recommend the following naming of variables instead: [( sc , sk ) for sc , sk , d in G . edges ( data = True )] or [ d [ 'attr' ] for sc , sk , d in G . edges ( data = True )]","title":"Iterating over Edges using List Comprehensions"},{"location":"01-introduction/02-networkx-intro/#further-reading","text":"For a deeper look at the NetworkX API, be sure to check out the NetworkX docs .","title":"Further Reading"},{"location":"01-introduction/02-networkx-intro/#further-exercises","text":"Here's some further exercises that you can use to get some practice.","title":"Further Exercises"},{"location":"01-introduction/02-networkx-intro/#exercise-unrequited-friendships","text":"Try figuring out which students have \"unrequited\" friendships, that is, they have rated another student as their favourite at least once, but that other student has not rated them as their favourite at least once. Hint: the goal here is to get a list of edges for which the reverse edge is not present. Hint: You may need the class method G.has_edge(n1, n2) . This returns whether a graph has an edge between the nodes n1 and n2 . from nams.solutions.intro import unrequitted_friendships_v1 #### REPLACE THE NEXT LINE WITH YOUR ANSWER unrequitted_friendships = unrequitted_friendships_v1 ( G ) assert len ( unrequitted_friendships ) == 124 In a previous session at ODSC East 2018, a few other class participants provided the following solutions, which you can take a look at by uncommenting the following cells. This first one by @schwanne is the list comprehension version of the above solution: from nams.solutions.intro import unrequitted_friendships_v2 # unrequitted_friendships_v2?? This one by @end0 is a unique one involving sets. from nams.solutions.intro import unrequitted_friendships_v3 # unrequitted_friendships_v3??","title":"Exercise: Unrequited Friendships"},{"location":"01-introduction/02-networkx-intro/#solution-answers","text":"Here are the answers to the exercises above. import nams.solutions.intro as solutions import inspect print ( inspect . getsource ( solutions )) \"\"\" Solutions to Intro Chapter. \"\"\" def node_metadata(G): \"\"\"Counts of students of each gender.\"\"\" from collections import Counter mf_counts = Counter([d[\"gender\"] for n, d in G.nodes(data=True)]) return mf_counts def edge_metadata(G): \"\"\"Maximum number of times that a student rated another student.\"\"\" counts = [d[\"count\"] for n1, n2, d in G.edges(data=True)] maxcount = max(counts) return maxcount def adding_students(G): \"\"\"How to nodes and edges to a graph.\"\"\" G = G.copy() G.add_node(30, gender=\"male\") G.add_node(31, gender=\"female\") G.add_edge(30, 31, count=3) G.add_edge(31, 30, count=3) # reverse is optional in undirected network G.add_edge(30, 7, count=3) # but this network is directed G.add_edge(7, 30, count=3) G.add_edge(31, 7, count=3) G.add_edge(7, 31, count=3) return G def unrequitted_friendships_v1(G): \"\"\"Answer to unrequitted friendships problem.\"\"\" unrequitted_friendships = [] for n1, n2 in G.edges(): if not G.has_edge(n2, n1): unrequitted_friendships.append((n1, n2)) return unrequitted_friendships def unrequitted_friendships_v2(G): \"\"\"Alternative answer to unrequitted friendships problem. By @schwanne.\"\"\" return len([(n1, n2) for n1, n2 in G.edges() if not G.has_edge(n2, n1)]) def unrequitted_friendships_v3(G): \"\"\"Alternative answer to unrequitted friendships problem. By @end0.\"\"\" links = ((n1, n2) for n1, n2, d in G.edges(data=True)) reverse_links = ((n2, n1) for n1, n2, d in G.edges(data=True)) return len(list(set(links) - set(reverse_links)))","title":"Solution Answers"},{"location":"01-introduction/03-viz/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"v9HrR_AF5Zc\" , width = \"100%\" ) In this chapter, We want to introduce you to the wonderful world of graph visualization. You probably have seen graphs that are visualized as hairballs. Apart from communicating how complex the graph is, hairballs don't really communicate much else. As such, my goal by the end of this chapter is to introduce you to what I call rational graph visualization . But before we can do that, let's first make sure we understand how to use NetworkX's drawing facilities to draw graphs to the screen. In a pinch, and for small graphs, it's very handy to have. Hairballs The node-link diagram is the canonical diagram we will see in publications. Nodes are commonly drawn as circles, while edges are drawn s lines. Node-link diagrams are common, and there's a good reason for this: it's convenient to draw! In NetworkX, we can draw node-link diagrams using: from nams import load_data as cf import networkx as nx import matplotlib.pyplot as plt G = cf . load_seventh_grader_network () nx . draw ( G ) Nodes more tightly connected with one another are clustered together. Initial node placement is done typically at random, so really it's tough to deterministically generate the same figure. If the network is small enough to visualize, and the node labels are small enough to fit in a circle, then you can use the with_labels=True argument to bring some degree of informativeness to the drawing: G . is_directed () True nx . draw ( G , with_labels = True ) The downside to drawing graphs this way is that large graphs end up looking like hairballs. Can you imagine a graph with more than the 28 nodes that we have? As you probably can imagine, the default nx.draw(G) is probably not suitable for generating visual insights. Matrix Plot A different way that we can visualize a graph is by visualizing it in its matrix form. The nodes are on the x- and y- axes, and a filled square represent an edge between the nodes. We can draw a graph's matrix form conveniently by using nxviz.MatrixPlot : import nxviz as nv from nxviz import annotate nv . matrix ( G , group_by = \"gender\" , node_color_by = \"gender\" ) annotate . matrix_group ( G , group_by = \"gender\" ) What can you tell from the graph visualization? A few things are immediately obvious: The diagonal is empty: no student voted for themselves as their favourite. The matrix is asymmetric about the diagonal: this is a directed graph! (An undirected graph would be symmetric about the diagonal.) You might go on to suggest that there is some clustering happening, but without applying a proper clustering algorithm on the adjacency matrix, we would be hard-pressed to know for sure. After all, we can simply re-order the node ordering along the axes to produce a seemingly-random matrix. Arc Plot The Arc Plot is another rational graph visualization. Here, we line up the nodes along a horizontal axis, and draw arcs between nodes if they are connected by an edge. We can also optionally group and colour them by some metadata. In the case of this student graph, we group and colour them by \"gender\". # a = ArcPlot(G, node_color='gender', node_grouping='gender') nv . arc ( G , node_color_by = \"gender\" , group_by = \"gender\" ) annotate . arc_group ( G , group_by = \"gender\" ) The Arc Plot forms the basis of the next visualization, the highly popular Circos plot. Circos Plot The Circos Plot was developed by Martin Krzywinski at the BC Cancer Research Center. The nxviz.CircosPlot takes inspiration from the original by joining the two ends of the Arc Plot into a circle. Likewise, we can colour and order nodes by node metadata: nv . circos ( G , group_by = \"gender\" , node_color_by = \"gender\" ) annotate . circos_group ( G , group_by = \"gender\" ) Generally speaking, you can think of a Circos Plot as being a more compact and aesthetically pleasing version of Arc Plots. Hive Plot The final plot we'll show is, Hive Plots. from nxviz import plots import matplotlib.pyplot as plt nv . hive ( G , group_by = \"gender\" , node_color_by = \"gender\" ) annotate . hive_group ( G , group_by = \"gender\" ) As you can see, with Hive Plots, we first group nodes along two or three radial axes. In this case, we have the boys along one radial axis and the girls along the other. We can also order the nodes along each axis if we so choose to. In this case, no particular ordering is chosen. Next, we draw edges. We start first with edges between groups. That is shown on the left side of the figure, joining nodes in the \"yellow\" and \"green\" (boys/girls) groups. We then proceed to edges within groups. This is done by cloning the node radial axis before drawing edges. Principles of Rational Graph Viz While I was implementing these visualizations in nxviz , I learned an important lesson in implementing graph visualizations in general: To be most informative and communicative, a graph visualization should first prioritize node placement in a fashion that makes sense. In some ways, this makes a ton of sense. The nodes are the \"entities\" in a graph, corresponding to people, proteins, and ports. For \"entities\", we have natural ways to group, order and summarize (reduce). (An example of a \"reduction\" is counting the number of things.) Prioritizing node placement allows us to appeal to our audience's natural sense of grouping, ordering and reduction. So the next time you see a hairball, I hope you're able to critique it for what it doesn't communicate, and possibly use the same principle to design a better visualization!","title":"Chapter 3: Graph Visualization"},{"location":"01-introduction/03-viz/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"v9HrR_AF5Zc\" , width = \"100%\" ) In this chapter, We want to introduce you to the wonderful world of graph visualization. You probably have seen graphs that are visualized as hairballs. Apart from communicating how complex the graph is, hairballs don't really communicate much else. As such, my goal by the end of this chapter is to introduce you to what I call rational graph visualization . But before we can do that, let's first make sure we understand how to use NetworkX's drawing facilities to draw graphs to the screen. In a pinch, and for small graphs, it's very handy to have.","title":"Introduction"},{"location":"01-introduction/03-viz/#hairballs","text":"The node-link diagram is the canonical diagram we will see in publications. Nodes are commonly drawn as circles, while edges are drawn s lines. Node-link diagrams are common, and there's a good reason for this: it's convenient to draw! In NetworkX, we can draw node-link diagrams using: from nams import load_data as cf import networkx as nx import matplotlib.pyplot as plt G = cf . load_seventh_grader_network () nx . draw ( G ) Nodes more tightly connected with one another are clustered together. Initial node placement is done typically at random, so really it's tough to deterministically generate the same figure. If the network is small enough to visualize, and the node labels are small enough to fit in a circle, then you can use the with_labels=True argument to bring some degree of informativeness to the drawing: G . is_directed () True nx . draw ( G , with_labels = True ) The downside to drawing graphs this way is that large graphs end up looking like hairballs. Can you imagine a graph with more than the 28 nodes that we have? As you probably can imagine, the default nx.draw(G) is probably not suitable for generating visual insights.","title":"Hairballs"},{"location":"01-introduction/03-viz/#matrix-plot","text":"A different way that we can visualize a graph is by visualizing it in its matrix form. The nodes are on the x- and y- axes, and a filled square represent an edge between the nodes. We can draw a graph's matrix form conveniently by using nxviz.MatrixPlot : import nxviz as nv from nxviz import annotate nv . matrix ( G , group_by = \"gender\" , node_color_by = \"gender\" ) annotate . matrix_group ( G , group_by = \"gender\" ) What can you tell from the graph visualization? A few things are immediately obvious: The diagonal is empty: no student voted for themselves as their favourite. The matrix is asymmetric about the diagonal: this is a directed graph! (An undirected graph would be symmetric about the diagonal.) You might go on to suggest that there is some clustering happening, but without applying a proper clustering algorithm on the adjacency matrix, we would be hard-pressed to know for sure. After all, we can simply re-order the node ordering along the axes to produce a seemingly-random matrix.","title":"Matrix Plot"},{"location":"01-introduction/03-viz/#arc-plot","text":"The Arc Plot is another rational graph visualization. Here, we line up the nodes along a horizontal axis, and draw arcs between nodes if they are connected by an edge. We can also optionally group and colour them by some metadata. In the case of this student graph, we group and colour them by \"gender\". # a = ArcPlot(G, node_color='gender', node_grouping='gender') nv . arc ( G , node_color_by = \"gender\" , group_by = \"gender\" ) annotate . arc_group ( G , group_by = \"gender\" ) The Arc Plot forms the basis of the next visualization, the highly popular Circos plot.","title":"Arc Plot"},{"location":"01-introduction/03-viz/#circos-plot","text":"The Circos Plot was developed by Martin Krzywinski at the BC Cancer Research Center. The nxviz.CircosPlot takes inspiration from the original by joining the two ends of the Arc Plot into a circle. Likewise, we can colour and order nodes by node metadata: nv . circos ( G , group_by = \"gender\" , node_color_by = \"gender\" ) annotate . circos_group ( G , group_by = \"gender\" ) Generally speaking, you can think of a Circos Plot as being a more compact and aesthetically pleasing version of Arc Plots.","title":"Circos Plot"},{"location":"01-introduction/03-viz/#hive-plot","text":"The final plot we'll show is, Hive Plots. from nxviz import plots import matplotlib.pyplot as plt nv . hive ( G , group_by = \"gender\" , node_color_by = \"gender\" ) annotate . hive_group ( G , group_by = \"gender\" ) As you can see, with Hive Plots, we first group nodes along two or three radial axes. In this case, we have the boys along one radial axis and the girls along the other. We can also order the nodes along each axis if we so choose to. In this case, no particular ordering is chosen. Next, we draw edges. We start first with edges between groups. That is shown on the left side of the figure, joining nodes in the \"yellow\" and \"green\" (boys/girls) groups. We then proceed to edges within groups. This is done by cloning the node radial axis before drawing edges.","title":"Hive Plot"},{"location":"01-introduction/03-viz/#principles-of-rational-graph-viz","text":"While I was implementing these visualizations in nxviz , I learned an important lesson in implementing graph visualizations in general: To be most informative and communicative, a graph visualization should first prioritize node placement in a fashion that makes sense. In some ways, this makes a ton of sense. The nodes are the \"entities\" in a graph, corresponding to people, proteins, and ports. For \"entities\", we have natural ways to group, order and summarize (reduce). (An example of a \"reduction\" is counting the number of things.) Prioritizing node placement allows us to appeal to our audience's natural sense of grouping, ordering and reduction. So the next time you see a hairball, I hope you're able to critique it for what it doesn't communicate, and possibly use the same principle to design a better visualization!","title":"Principles of Rational Graph Viz"},{"location":"02-algorithms/01-hubs/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"-oimHbVDdDA\" , width = 560 , height = 315 ) Because of the relational structure in a graph, we can begin to think about \"importance\" of a node that is induced because of its relationships to the rest of the nodes in the graph. Before we go on, let's think about a pertinent and contemporary example. An example: contact tracing At the time of writing (April 2020), finding important nodes in a graph has actually taken on a measure of importance that we might not have appreciated before. With the COVID-19 virus spreading, contact tracing has become quite important. In an infectious disease contact network, where individuals are nodes and contact between individuals of some kind are the edges, an \"important\" node in this contact network would be an individual who was infected who also was in contact with many people during the time that they were infected. Our dataset: \"Sociopatterns\" The dataset that we will use in this chapter is the \" sociopatterns network \" dataset. Incidentally, it's also about infectious diseases. Note to readers: We originally obtained the dataset in 2014 from the Konect website. It is unfortunately no longer available. The sociopatterns.org website hosts an edge list of a slightly different format, so it will look different from what we have here. From the original description on Konect, here is the description of the dataset: This network describes the face-to-face behavior of people during the exhibition INFECTIOUS: STAY AWAY in 2009 at the Science Gallery in Dublin. Nodes represent exhibition visitors; edges represent face-to-face contacts that were active for at least 20 seconds. Multiple edges between two nodes are possible and denote multiple contacts. The network contains the data from the day with the most interactions. To simplify the network, we have represented only the last contact between individuals. from nams import load_data as cf G = cf . load_sociopatterns_network () It is loaded as an undirected graph object: type ( G ) networkx.classes.graph.Graph As usual, before proceeding with any analysis, we should know basic graph statistics. len ( G . nodes ()), len ( G . edges ()) (410, 2765) A Measure of Importance: \"Number of Neighbors\" One measure of importance of a node is the number of neighbors that the node has. What is a neighbor ? We will work with the following definition: The neighbor of a node is connected to that node by an edge. Let's explore this concept, using the NetworkX API. Every NetworkX graph provides a G.neighbors(node) class method, which lets us query a graph for the number of neighbors of a given node: G . neighbors ( 7 ) <dict_keyiterator at 0x7f1854052b80> It returns a generator that doesn't immediately return the exact neighbors list. This means we cannot know its exact length, as it is a generator. If you tried to do: len ( G . neighbors ( 7 )) you would get the following error: --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 13 - 72 c56971d077 > in < module > ----> 1 len ( G . neighbors ( 7 )) TypeError : object of type 'dict_keyiterator' has no len () Hence, we will need to cast it as a list in order to know both its length and its members: list ( G . neighbors ( 7 )) [5, 6, 21, 22, 37, 48, 51] In the event that some nodes have an extensive list of neighbors, then using the dict_keyiterator is potentially a good memory-saving technique, as it lazily yields the neighbors. Exercise: Rank-ordering the number of neighbors a node has Since we know how to get the list of nodes that are neighbors of a given node, try this following exercise: Can you create a ranked list of the importance of each individual, based on the number of neighbors they have? Here are a few hints to help: You could consider using a pandas Series . This would be a modern and idiomatic way of approaching the problem. You could also consider using Python's sorted function. from nams.solutions.hubs import rank_ordered_neighbors #### REPLACE THE NEXT FEW LINES WITH YOUR ANSWER # answer = rank_ordered_neighbors(G) # answer The original implementation looked like the following from nams.solutions.hubs import rank_ordered_neighbors_original # rank_ordered_neighbors_original?? And another implementation that uses generators: from nams.solutions.hubs import rank_ordered_neighbors_generator # rank_ordered_neighbors_generator?? Generalizing \"neighbors\" to arbitrarily-sized graphs The concept of neighbors is simple and appealing, but it leaves us with a slight point of dissatisfaction: it is difficult to compare graphs of different sizes. Is a node more important solely because it has more neighbors? What if it were situated in an extremely large graph? Would we not expect it to have more neighbors? As such, we need a normalization factor. One reasonable one, in fact, is the number of nodes that a given node could possibly be connected to. By taking the ratio of the number of neighbors a node has to the number of neighbors it could possibly have, we get the degree centrality metric. Formally defined, the degree centrality of a node (let's call it d d ) is the number of neighbors that a node has (let's call it n n ) divided by the number of neighbors it could possibly have (let's call it N N ): d = \\frac{n}{N} d = \\frac{n}{N} NetworkX provides a function for us to calculate degree centrality conveniently: import networkx as nx import pandas as pd dcs = pd . Series ( nx . degree_centrality ( G )) dcs 100 0.070905 101 0.031785 102 0.039120 103 0.063570 104 0.041565 ... 89 0.009780 91 0.051345 96 0.036675 99 0.034230 98 0.002445 Length: 410, dtype: float64 nx.degree_centrality(G) returns to us a dictionary of key-value pairs, where the keys are node IDs and values are the degree centrality score. To save on output length, I took the liberty of casting it as a pandas Series to make it easier to display. Incidentally, we can also sort the series to find the nodes with the highest degree centralities: dcs . sort_values ( ascending = False ) 51 0.122249 272 0.114914 235 0.105134 195 0.105134 265 0.083130 ... 390 0.002445 135 0.002445 398 0.002445 186 0.002445 98 0.002445 Length: 410, dtype: float64 Does the list order look familiar? It should, since the numerator of the degree centrality metric is identical to the number of neighbors, and the denominator is a constant. Distribution of graph metrics One important concept that you should come to know is that the distribution of node-centric values can characterize classes of graphs. What do we mean by \"distribution of node-centric values\"? One would be the degree distribution, that is, the collection of node degree values in a graph. Generally, you might be familiar with plotting a histogram to visualize distributions of values, but in this book, we are going to avoid histograms like the plague. I detail a lot of reasons in a blog post I wrote in 2018, but the main points are that: It's easier to lie with histograms. You get informative statistical information (median, IQR, extremes/outliers) more easily. Exercise: Degree distribution In this next exercise, we are going to get practice visualizing these values using empirical cumulative distribution function plots. I have written for you an ECDF function that you can use already. Its API looks like the following: x , y = ecdf ( list_of_values ) giving you x and y values that you can directly plot. The exercise prompt is this: Plot the ECDF of the degree centrality and degree distributions. First do it for degree centrality : from nams.functions import ecdf from nams.solutions.hubs import ecdf_degree_centrality #### REPLACE THE FUNCTION CALL WITH YOUR ANSWER ecdf_degree_centrality ( G ) Now do it for degree : from nams.solutions.hubs import ecdf_degree #### REPLACE THE FUNCTION CALL WITH YOUR ANSWER ecdf_degree ( G ) The fact that they are identically-shaped should not surprise you! Exercise: What about that denominator? The denominator N N in the degree centrality definition is \"the number of nodes that a node could possibly be connected to\". Can you think of two ways N N be defined? from nams.solutions.hubs import num_possible_neighbors #### UNCOMMENT TO SEE MY ANSWER # print(num_possible_neighbors()) Exercise: Circos Plotting Let's get some practice with the nxviz API. Visualize the graph G , while ordering and colouring them by the 'order' node attribute. from nams.solutions.hubs import circos_plot #### REPLACE THE NEXT LINE WITH YOUR ANSWER circos_plot ( G ) And here's an alternative view using an arc plot. import nxviz as nv nv . arc ( G , sort_by = \"order\" , node_color_by = \"order\" ) <AxesSubplot:> Exercise: Visual insights Since we know that node colour and order are by the \"order\" in which the person entered into the exhibit, what does this visualization tell you? from nams.solutions.hubs import visual_insights #### UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # print(visual_insights()) Exercise: Investigating degree centrality and node order One of the insights that we might have gleaned from visualizing the graph is that the nodes that have a high degree centrality might also be responsible for the edges that criss-cross the Circos plot. To test this, plot the following: x-axis: node degree centrality y-axis: maximum difference between the neighbors' order s (a node attribute) and the node's order . from nams.solutions.hubs import dc_node_order dc_node_order ( G ) The somewhat positive correlation between the degree centrality might tell us that this trend holds true. A further applied question would be to ask what behaviour of these nodes would give rise to this pattern. Are these nodes actually exhibit staff? Or is there some other reason why they are staying so long? This, of course, would require joining in further information that we would overlay on top of the graph (by adding them as node or edge attributes) before we might make further statements. Reflections In this chapter, we defined a metric of node importance: the degree centrality metric. In the example we looked at, it could help us identify potential infectious agent superspreaders in a disease contact network. In other settings, it might help us spot: message amplifiers/influencers in a social network, and potentially crowded airports that have lots of connections into and out of it (still relevant to infectious disease spread!) and many more! What other settings can you think of in which the number of neighbors that a node has can become a metric of importance for the node? Solutions Here are the solutions to the exercises above. from nams.solutions import hubs import inspect print ( inspect . getsource ( hubs )) \"\"\"Solutions to Hubs chapter.\"\"\" import matplotlib.pyplot as plt import networkx as nx import pandas as pd import nxviz as nv from nxviz import annotate from nams import ecdf def rank_ordered_neighbors(G): \"\"\" Uses a pandas Series to help with sorting. \"\"\" s = pd.Series({n: len(list(G.neighbors(n))) for n in G.nodes()}) return s.sort_values(ascending=False) def rank_ordered_neighbors_original(G): \"\"\"Original implementation of rank-ordered number of neighbors.\"\"\" return sorted(G.nodes(), key=lambda x: len(list(G.neighbors(x))), reverse=True) def rank_ordered_neighbors_generator(G): \"\"\" Rank-ordered generator of neighbors. Contributed by @dgerlanc. Ref: https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/75 \"\"\" gen = ((len(list(G.neighbors(x))), x) for x in G.nodes()) return sorted(gen, reverse=True) def ecdf_degree_centrality(G): \"\"\"ECDF of degree centrality.\"\"\" x, y = ecdf(list(nx.degree_centrality(G).values())) plt.scatter(x, y) plt.xlabel(\"degree centrality\") plt.ylabel(\"cumulative fraction\") def ecdf_degree(G): \"\"\"ECDF of degree.\"\"\" num_neighbors = [len(list(G.neighbors(n))) for n in G.nodes()] x, y = ecdf(num_neighbors) plt.scatter(x, y) plt.xlabel(\"degree\") plt.ylabel(\"cumulative fraction\") def num_possible_neighbors(): \"\"\"Answer to the number of possible neighbors for a node.\"\"\" return r\"\"\" The number of possible neighbors can either be defined as: 1. All other nodes but myself 2. All other nodes and myself If $K$ is the number of nodes in the graph, then if defined as (1), $N$ (the denominator) is $K - 1$. If defined as (2), $N$ is equal to $K$. \"\"\" def circos_plot(G): \"\"\"Draw a Circos Plot of the graph.\"\"\" # c = CircosPlot(G, node_order=\"order\", node_color=\"order\") # c.draw() nv.circos(G, sort_by=\"order\", node_color_by=\"order\") annotate.node_colormapping(G, color_by=\"order\") def visual_insights(): \"\"\"Visual insights from the Circos Plot.\"\"\" return \"\"\" We see that most edges are \"local\" with nodes that are proximal in order. The nodes that are weird are the ones that have connections with individuals much later than itself, crossing larger jumps in order/time. Additionally, if you recall the ranked list of degree centralities, it appears that these nodes that have the highest degree centrality scores are also the ones that have edges that cross the circos plot. \"\"\" def dc_node_order(G): \"\"\"Comparison of degree centrality by maximum difference in node order.\"\"\" import matplotlib.pyplot as plt import pandas as pd import networkx as nx # Degree centralities dcs = pd.Series(nx.degree_centrality(G)) # Maximum node order difference maxdiffs = dict() for n, d in G.nodes(data=True): diffs = [] for nbr in G.neighbors(n): diffs.append(abs(G.nodes[nbr][\"order\"] - d[\"order\"])) maxdiffs[n] = max(diffs) maxdiffs = pd.Series(maxdiffs) ax = pd.DataFrame(dict(degree_centrality=dcs, max_diff=maxdiffs)).plot( x=\"degree_centrality\", y=\"max_diff\", kind=\"scatter\" )","title":"Chapter 4: Hubs"},{"location":"02-algorithms/01-hubs/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"-oimHbVDdDA\" , width = 560 , height = 315 ) Because of the relational structure in a graph, we can begin to think about \"importance\" of a node that is induced because of its relationships to the rest of the nodes in the graph. Before we go on, let's think about a pertinent and contemporary example.","title":"Introduction"},{"location":"02-algorithms/01-hubs/#an-example-contact-tracing","text":"At the time of writing (April 2020), finding important nodes in a graph has actually taken on a measure of importance that we might not have appreciated before. With the COVID-19 virus spreading, contact tracing has become quite important. In an infectious disease contact network, where individuals are nodes and contact between individuals of some kind are the edges, an \"important\" node in this contact network would be an individual who was infected who also was in contact with many people during the time that they were infected.","title":"An example: contact tracing"},{"location":"02-algorithms/01-hubs/#our-dataset-sociopatterns","text":"The dataset that we will use in this chapter is the \" sociopatterns network \" dataset. Incidentally, it's also about infectious diseases. Note to readers: We originally obtained the dataset in 2014 from the Konect website. It is unfortunately no longer available. The sociopatterns.org website hosts an edge list of a slightly different format, so it will look different from what we have here. From the original description on Konect, here is the description of the dataset: This network describes the face-to-face behavior of people during the exhibition INFECTIOUS: STAY AWAY in 2009 at the Science Gallery in Dublin. Nodes represent exhibition visitors; edges represent face-to-face contacts that were active for at least 20 seconds. Multiple edges between two nodes are possible and denote multiple contacts. The network contains the data from the day with the most interactions. To simplify the network, we have represented only the last contact between individuals. from nams import load_data as cf G = cf . load_sociopatterns_network () It is loaded as an undirected graph object: type ( G ) networkx.classes.graph.Graph As usual, before proceeding with any analysis, we should know basic graph statistics. len ( G . nodes ()), len ( G . edges ()) (410, 2765)","title":"Our dataset: \"Sociopatterns\""},{"location":"02-algorithms/01-hubs/#a-measure-of-importance-number-of-neighbors","text":"One measure of importance of a node is the number of neighbors that the node has. What is a neighbor ? We will work with the following definition: The neighbor of a node is connected to that node by an edge. Let's explore this concept, using the NetworkX API. Every NetworkX graph provides a G.neighbors(node) class method, which lets us query a graph for the number of neighbors of a given node: G . neighbors ( 7 ) <dict_keyiterator at 0x7f1854052b80> It returns a generator that doesn't immediately return the exact neighbors list. This means we cannot know its exact length, as it is a generator. If you tried to do: len ( G . neighbors ( 7 )) you would get the following error: --------------------------------------------------------------------------- TypeError Traceback ( most recent call last ) < ipython - input - 13 - 72 c56971d077 > in < module > ----> 1 len ( G . neighbors ( 7 )) TypeError : object of type 'dict_keyiterator' has no len () Hence, we will need to cast it as a list in order to know both its length and its members: list ( G . neighbors ( 7 )) [5, 6, 21, 22, 37, 48, 51] In the event that some nodes have an extensive list of neighbors, then using the dict_keyiterator is potentially a good memory-saving technique, as it lazily yields the neighbors.","title":"A Measure of Importance: \"Number of Neighbors\""},{"location":"02-algorithms/01-hubs/#exercise-rank-ordering-the-number-of-neighbors-a-node-has","text":"Since we know how to get the list of nodes that are neighbors of a given node, try this following exercise: Can you create a ranked list of the importance of each individual, based on the number of neighbors they have? Here are a few hints to help: You could consider using a pandas Series . This would be a modern and idiomatic way of approaching the problem. You could also consider using Python's sorted function. from nams.solutions.hubs import rank_ordered_neighbors #### REPLACE THE NEXT FEW LINES WITH YOUR ANSWER # answer = rank_ordered_neighbors(G) # answer The original implementation looked like the following from nams.solutions.hubs import rank_ordered_neighbors_original # rank_ordered_neighbors_original?? And another implementation that uses generators: from nams.solutions.hubs import rank_ordered_neighbors_generator # rank_ordered_neighbors_generator??","title":"Exercise: Rank-ordering the number of neighbors a node has"},{"location":"02-algorithms/01-hubs/#generalizing-neighbors-to-arbitrarily-sized-graphs","text":"The concept of neighbors is simple and appealing, but it leaves us with a slight point of dissatisfaction: it is difficult to compare graphs of different sizes. Is a node more important solely because it has more neighbors? What if it were situated in an extremely large graph? Would we not expect it to have more neighbors? As such, we need a normalization factor. One reasonable one, in fact, is the number of nodes that a given node could possibly be connected to. By taking the ratio of the number of neighbors a node has to the number of neighbors it could possibly have, we get the degree centrality metric. Formally defined, the degree centrality of a node (let's call it d d ) is the number of neighbors that a node has (let's call it n n ) divided by the number of neighbors it could possibly have (let's call it N N ): d = \\frac{n}{N} d = \\frac{n}{N} NetworkX provides a function for us to calculate degree centrality conveniently: import networkx as nx import pandas as pd dcs = pd . Series ( nx . degree_centrality ( G )) dcs 100 0.070905 101 0.031785 102 0.039120 103 0.063570 104 0.041565 ... 89 0.009780 91 0.051345 96 0.036675 99 0.034230 98 0.002445 Length: 410, dtype: float64 nx.degree_centrality(G) returns to us a dictionary of key-value pairs, where the keys are node IDs and values are the degree centrality score. To save on output length, I took the liberty of casting it as a pandas Series to make it easier to display. Incidentally, we can also sort the series to find the nodes with the highest degree centralities: dcs . sort_values ( ascending = False ) 51 0.122249 272 0.114914 235 0.105134 195 0.105134 265 0.083130 ... 390 0.002445 135 0.002445 398 0.002445 186 0.002445 98 0.002445 Length: 410, dtype: float64 Does the list order look familiar? It should, since the numerator of the degree centrality metric is identical to the number of neighbors, and the denominator is a constant.","title":"Generalizing \"neighbors\" to arbitrarily-sized graphs"},{"location":"02-algorithms/01-hubs/#distribution-of-graph-metrics","text":"One important concept that you should come to know is that the distribution of node-centric values can characterize classes of graphs. What do we mean by \"distribution of node-centric values\"? One would be the degree distribution, that is, the collection of node degree values in a graph. Generally, you might be familiar with plotting a histogram to visualize distributions of values, but in this book, we are going to avoid histograms like the plague. I detail a lot of reasons in a blog post I wrote in 2018, but the main points are that: It's easier to lie with histograms. You get informative statistical information (median, IQR, extremes/outliers) more easily.","title":"Distribution of graph metrics"},{"location":"02-algorithms/01-hubs/#exercise-degree-distribution","text":"In this next exercise, we are going to get practice visualizing these values using empirical cumulative distribution function plots. I have written for you an ECDF function that you can use already. Its API looks like the following: x , y = ecdf ( list_of_values ) giving you x and y values that you can directly plot. The exercise prompt is this: Plot the ECDF of the degree centrality and degree distributions. First do it for degree centrality : from nams.functions import ecdf from nams.solutions.hubs import ecdf_degree_centrality #### REPLACE THE FUNCTION CALL WITH YOUR ANSWER ecdf_degree_centrality ( G ) Now do it for degree : from nams.solutions.hubs import ecdf_degree #### REPLACE THE FUNCTION CALL WITH YOUR ANSWER ecdf_degree ( G ) The fact that they are identically-shaped should not surprise you!","title":"Exercise: Degree distribution"},{"location":"02-algorithms/01-hubs/#exercise-what-about-that-denominator","text":"The denominator N N in the degree centrality definition is \"the number of nodes that a node could possibly be connected to\". Can you think of two ways N N be defined? from nams.solutions.hubs import num_possible_neighbors #### UNCOMMENT TO SEE MY ANSWER # print(num_possible_neighbors())","title":"Exercise: What about that denominator?"},{"location":"02-algorithms/01-hubs/#exercise-circos-plotting","text":"Let's get some practice with the nxviz API. Visualize the graph G , while ordering and colouring them by the 'order' node attribute. from nams.solutions.hubs import circos_plot #### REPLACE THE NEXT LINE WITH YOUR ANSWER circos_plot ( G ) And here's an alternative view using an arc plot. import nxviz as nv nv . arc ( G , sort_by = \"order\" , node_color_by = \"order\" ) <AxesSubplot:>","title":"Exercise: Circos Plotting"},{"location":"02-algorithms/01-hubs/#exercise-visual-insights","text":"Since we know that node colour and order are by the \"order\" in which the person entered into the exhibit, what does this visualization tell you? from nams.solutions.hubs import visual_insights #### UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # print(visual_insights())","title":"Exercise: Visual insights"},{"location":"02-algorithms/01-hubs/#exercise-investigating-degree-centrality-and-node-order","text":"One of the insights that we might have gleaned from visualizing the graph is that the nodes that have a high degree centrality might also be responsible for the edges that criss-cross the Circos plot. To test this, plot the following: x-axis: node degree centrality y-axis: maximum difference between the neighbors' order s (a node attribute) and the node's order . from nams.solutions.hubs import dc_node_order dc_node_order ( G ) The somewhat positive correlation between the degree centrality might tell us that this trend holds true. A further applied question would be to ask what behaviour of these nodes would give rise to this pattern. Are these nodes actually exhibit staff? Or is there some other reason why they are staying so long? This, of course, would require joining in further information that we would overlay on top of the graph (by adding them as node or edge attributes) before we might make further statements.","title":"Exercise: Investigating degree centrality and node order"},{"location":"02-algorithms/01-hubs/#reflections","text":"In this chapter, we defined a metric of node importance: the degree centrality metric. In the example we looked at, it could help us identify potential infectious agent superspreaders in a disease contact network. In other settings, it might help us spot: message amplifiers/influencers in a social network, and potentially crowded airports that have lots of connections into and out of it (still relevant to infectious disease spread!) and many more! What other settings can you think of in which the number of neighbors that a node has can become a metric of importance for the node?","title":"Reflections"},{"location":"02-algorithms/01-hubs/#solutions","text":"Here are the solutions to the exercises above. from nams.solutions import hubs import inspect print ( inspect . getsource ( hubs )) \"\"\"Solutions to Hubs chapter.\"\"\" import matplotlib.pyplot as plt import networkx as nx import pandas as pd import nxviz as nv from nxviz import annotate from nams import ecdf def rank_ordered_neighbors(G): \"\"\" Uses a pandas Series to help with sorting. \"\"\" s = pd.Series({n: len(list(G.neighbors(n))) for n in G.nodes()}) return s.sort_values(ascending=False) def rank_ordered_neighbors_original(G): \"\"\"Original implementation of rank-ordered number of neighbors.\"\"\" return sorted(G.nodes(), key=lambda x: len(list(G.neighbors(x))), reverse=True) def rank_ordered_neighbors_generator(G): \"\"\" Rank-ordered generator of neighbors. Contributed by @dgerlanc. Ref: https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/75 \"\"\" gen = ((len(list(G.neighbors(x))), x) for x in G.nodes()) return sorted(gen, reverse=True) def ecdf_degree_centrality(G): \"\"\"ECDF of degree centrality.\"\"\" x, y = ecdf(list(nx.degree_centrality(G).values())) plt.scatter(x, y) plt.xlabel(\"degree centrality\") plt.ylabel(\"cumulative fraction\") def ecdf_degree(G): \"\"\"ECDF of degree.\"\"\" num_neighbors = [len(list(G.neighbors(n))) for n in G.nodes()] x, y = ecdf(num_neighbors) plt.scatter(x, y) plt.xlabel(\"degree\") plt.ylabel(\"cumulative fraction\") def num_possible_neighbors(): \"\"\"Answer to the number of possible neighbors for a node.\"\"\" return r\"\"\" The number of possible neighbors can either be defined as: 1. All other nodes but myself 2. All other nodes and myself If $K$ is the number of nodes in the graph, then if defined as (1), $N$ (the denominator) is $K - 1$. If defined as (2), $N$ is equal to $K$. \"\"\" def circos_plot(G): \"\"\"Draw a Circos Plot of the graph.\"\"\" # c = CircosPlot(G, node_order=\"order\", node_color=\"order\") # c.draw() nv.circos(G, sort_by=\"order\", node_color_by=\"order\") annotate.node_colormapping(G, color_by=\"order\") def visual_insights(): \"\"\"Visual insights from the Circos Plot.\"\"\" return \"\"\" We see that most edges are \"local\" with nodes that are proximal in order. The nodes that are weird are the ones that have connections with individuals much later than itself, crossing larger jumps in order/time. Additionally, if you recall the ranked list of degree centralities, it appears that these nodes that have the highest degree centrality scores are also the ones that have edges that cross the circos plot. \"\"\" def dc_node_order(G): \"\"\"Comparison of degree centrality by maximum difference in node order.\"\"\" import matplotlib.pyplot as plt import pandas as pd import networkx as nx # Degree centralities dcs = pd.Series(nx.degree_centrality(G)) # Maximum node order difference maxdiffs = dict() for n, d in G.nodes(data=True): diffs = [] for nbr in G.neighbors(n): diffs.append(abs(G.nodes[nbr][\"order\"] - d[\"order\"])) maxdiffs[n] = max(diffs) maxdiffs = pd.Series(maxdiffs) ax = pd.DataFrame(dict(degree_centrality=dcs, max_diff=maxdiffs)).plot( x=\"degree_centrality\", y=\"max_diff\", kind=\"scatter\" )","title":"Solutions"},{"location":"02-algorithms/02-paths/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"JjpbztqP9_0\" , width = \"100%\" ) Graph traversal is akin to walking along the graph, node by node, constrained by the edges that connect the nodes. Graph traversal is particularly useful for understanding the local structure of certain portions of the graph and for finding paths that connect two nodes in the network. In this chapter, we are going to learn how to perform pathfinding in a graph, specifically by looking for shortest paths via the breadth-first search algorithm. Breadth-First Search The BFS algorithm is a staple of computer science curricula, and for good reason: it teaches learners how to \"think on\" a graph, putting one in the position of \"the dumb computer\" that can't use a visual cortex to \" just know \" how to trace a path from one node to another. As a topic, learning how to do BFS additionally imparts algorithmic thinking to the learner. Exercise: Design the algorithm Try out this exercise to get some practice with algorithmic thinking. On a piece of paper, conjure up a graph that has 15-20 nodes. Connect them any way you like. Pick two nodes. Pretend that you're standing on one of the nodes, but you can't see any further beyond one neighbor away. Work out how you can find a path from the node you're standing on to the other node, given that you can only see nodes that are one neighbor away but have an infinitely good memory. If you are successful at designing the algorithm, you should get the answer below. from nams import load_data as cf G = cf . load_sociopatterns_network () from nams.solutions.paths import bfs_algorithm # UNCOMMENT NEXT LINE TO GET THE ANSWER. # bfs_algorithm() Exercise: Implement the algorithm Now that you've seen how the algorithm works, try implementing it! # FILL IN THE BLANKS BELOW def path_exists ( node1 , node2 , G ): \"\"\" This function checks whether a path exists between two nodes (node1, node2) in graph G. \"\"\" visited_nodes = _____ queue = [ _____ ] while len ( queue ) > 0 : node = ___________ neighbors = list ( _________________ ) if _____ in _________ : # print('Path exists between nodes {0} and {1}'.format(node1, node2)) return True else : visited_nodes . ___ ( ____ ) nbrs = [ _ for _ in _________ if _ not in _____________ ] queue = ____ + _____ # print('Path does not exist between nodes {0} and {1}'.format(node1, node2)) return False # UNCOMMENT THE FOLLOWING TWO LINES TO SEE THE ANSWER from nams.solutions.paths import path_exists # path_exists?? # CHECK YOUR ANSWER AGAINST THE TEST FUNCTION BELOW from random import sample import networkx as nx def test_path_exists ( N ): \"\"\" N: The number of times to spot-check. \"\"\" for i in range ( N ): n1 , n2 = sample ( G . nodes (), 2 ) assert path_exists ( n1 , n2 , G ) == bool ( nx . shortest_path ( G , n1 , n2 )) return True assert test_path_exists ( 10 ) Visualizing Paths One of the objectives of that exercise before was to help you \"think on graphs\". Now that you've learned how to do so, you might be wondering, \"How do I visualize that path through the graph?\" Well first off, if you inspect the test_path_exists function above, you'll notice that NetworkX provides a shortest_path() function that you can use. Here's what using nx.shortest_path() looks like. path = nx . shortest_path ( G , 7 , 400 ) path [7, 51, 188, 230, 335, 400] As you can see, it returns the nodes along the shortest path, incidentally in the exact order that you would traverse. One thing to note, though! If there are multiple shortest paths from one node to another, NetworkX will only return one of them. So how do you draw those nodes only ? You can use the G.subgraph(nodes) to return a new graph that only has nodes in nodes and only the edges that exist between them. After that, you can use any plotting library you like. We will show an example here that uses nxviz's matrix plot. Let's see it in action: import nxviz as nv g = G . subgraph ( path ) nv . matrix ( g , sort_by = \"order\" ) <AxesSubplot:> Voila! Now we have the subgraph (1) extracted and (2) drawn to screen! In this case, the matrix plot is a suitable visualization for its compactness. The off-diagonals also show that each node is a neighbor to the next one. You'll also notice that if you try to modify the graph g , say by adding a node: g . add_node ( 2048 ) you will get an error: --------------------------------------------------------------------------- NetworkXError Traceback ( most recent call last ) < ipython - input - 10 - ca6aa4c26819 > in < module > ----> 1 g . add_node ( 2048 ) ~/ anaconda / envs / nams / lib / python3 .7 / site - packages / networkx / classes / function . py in frozen ( * args , ** kwargs ) 156 def frozen ( * args , ** kwargs ): 157 \"\"\"Dummy method for raising errors when trying to modify frozen graphs\"\"\" --> 158 raise nx . NetworkXError ( \"Frozen graph can't be modified\" ) 159 160 NetworkXError : Frozen graph can 't be modified From the perspective of semantics, this makes a ton of sense: the subgraph g is a perfect subset of the larger graph G , and should not be allowed to be modified unless the larger container graph is modified. Exercise: Draw path with neighbors one degree out Try out this next exercise: Extend graph drawing with the neighbors of each of those nodes. Use any of the nxviz plots ( nv.matrix , nv.arc , nv.circos ); try to see which one helps you tell the best story. from nams.solutions.paths import plot_path_with_neighbors ### YOUR SOLUTION BELOW plot_path_with_neighbors ( G , 7 , 400 ) In this case, we opted for an Arc plot because we only have one grouping of nodes but have a logical way to order them. Because the path follows the order, the edges being highlighted automatically look like hops through the graph. Bottleneck nodes We're now going to revisit the concept of an \"important node\", this time now leveraging what we know about paths. In the \"hubs\" chapter, we saw how a node that is \"important\" could be so because it is connected to many other nodes. Paths give us an alternative definition. If we imagine that we have to pass a message on a graph from one node to another, then there may be \"bottleneck\" nodes for which if they are removed, then messages have a harder time flowing through the graph. One metric that measures this form of importance is the \"betweenness centrality\" metric. On a graph through which a generic \"message\" is flowing, a node with a high betweenness centrality is one that has a high proportion of shortest paths flowing through it. In other words, it behaves like a bottleneck . Betweenness centrality in NetworkX NetworkX provides a \"betweenness centrality\" function that behaves consistently with the \"degree centrality\" function, in that it returns a mapping from node to metric: import pandas as pd pd . Series ( nx . betweenness_centrality ( G )) 100 0.014809 101 0.001398 102 0.000748 103 0.006735 104 0.001198 ... 89 0.000004 91 0.006415 96 0.000323 99 0.000322 98 0.000000 Length: 410, dtype: float64 Exercise: compare degree and betweenness centrality Make a scatterplot of degree centrality on the x-axis and betweenness centrality on the y-axis. Do they correlate with one another? import matplotlib.pyplot as plt import seaborn as sns # YOUR ANSWER HERE: from nams.solutions.paths import plot_degree_betweenness plot_degree_betweenness ( G ) Think about it... ...does it make sense that degree centrality and betweenness centrality are not well-correlated? Can you think of a scenario where a node has a \"high\" betweenness centrality but a \"low\" degree centrality? Before peeking at the graph below, think about your answer for a moment. nx . draw ( nx . barbell_graph ( 5 , 1 )) Recap In this chapter, you learned the following things: You figured out how to implement the breadth-first-search algorithm to find shortest paths. You learned how to extract subgraphs from a larger graph. You implemented visualizations of subgraphs, which should help you as you communicate with colleagues. You calculated betweenness centrality metrics for a graph, and visualized how they correlated with degree centrality. Solutions Here are the solutions to the exercises above. from nams.solutions import paths import inspect print ( inspect . getsource ( paths )) \"\"\"Solutions to Paths chapter.\"\"\" import matplotlib.pyplot as plt import networkx as nx import pandas as pd import seaborn as sns from nams.functions import render_html def bfs_algorithm(): \"\"\" How to design a BFS algorithm. \"\"\" ans = \"\"\" How does the breadth-first search work? It essentially is as follows: 1. Begin with a queue that has only one element in it: the starting node. 2. Add the neighbors of that node to the queue. 1. If destination node is present in the queue, end. 2. If destination node is not present, proceed. 3. For each node in the queue: 1. Remove node from the queue. 2. Add neighbors of the node to the queue. Check if destination node is present or not. 3. If destination node is present, end. <!--Credit: @cavaunpeu for finding bug in pseudocode.--> 4. If destination node is not present, continue. \"\"\" return render_html(ans) def path_exists(node1, node2, G): \"\"\" This function checks whether a path exists between two nodes (node1, node2) in graph G. \"\"\" visited_nodes = set() queue = [node1] while len(queue) > 0: node = queue.pop() neighbors = list(G.neighbors(node)) if node2 in neighbors: return True else: visited_nodes.add(node) nbrs = [n for n in neighbors if n not in visited_nodes] queue = nbrs + queue return False def path_exists_for_loop(node1, node2, G): \"\"\" This function checks whether a path exists between two nodes (node1, node2) in graph G. Special thanks to @ghirlekar for suggesting that we keep track of the \"visited nodes\" to prevent infinite loops from happening. This also removes the need to remove nodes from queue. Reference: https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/3 With thanks to @joshporter1 for the second bug fix. Originally there was an extraneous \"if\" statement that guaranteed that the \"False\" case would never be returned - because queue never changes in shape. Discovered at PyCon 2017. With thanks to @chendaniely for pointing out the extraneous \"break\". If you would like to see @dgerlanc's implementation, see https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/76 \"\"\" visited_nodes = set() queue = [node1] for node in queue: neighbors = list(G.neighbors(node)) if node2 in neighbors: return True else: visited_nodes.add(node) queue.extend([n for n in neighbors if n not in visited_nodes]) return False def path_exists_deque(node1, node2, G): \"\"\"An alternative implementation.\"\"\" from collections import deque visited_nodes = set() queue = deque([node1]) while len(queue) > 0: node = queue.popleft() neighbors = list(G.neighbors(node)) if node2 in neighbors: return True else: visited_nodes.add(node) queue.extend([n for n in neighbors if n not in visited_nodes]) return False import nxviz as nv from nxviz import annotate, highlights def plot_path_with_neighbors(G, n1, n2): \"\"\"Plot a path with the heighbors of of the nodes along that path.\"\"\" path = nx.shortest_path(G, n1, n2) nodes = [*path] for node in path: nodes.extend(list(G.neighbors(node))) nodes = list(set(nodes)) g = G.subgraph(nodes) nv.arc( g, sort_by=\"order\", node_color_by=\"order\", edge_enc_kwargs={\"alpha_scale\": 0.5} ) for n in path: highlights.arc_node(g, n, sort_by=\"order\") for n1, n2 in zip(path[:-1], path[1:]): highlights.arc_edge(g, n1, n2, sort_by=\"order\") def plot_degree_betweenness(G): \"\"\"Plot scatterplot between degree and betweenness centrality.\"\"\" bc = pd.Series(nx.betweenness_centrality(G)) dc = pd.Series(nx.degree_centrality(G)) df = pd.DataFrame(dict(bc=bc, dc=dc)) ax = df.plot(x=\"dc\", y=\"bc\", kind=\"scatter\") ax.set_ylabel(\"Betweenness\\nCentrality\") ax.set_xlabel(\"Degree Centrality\") sns.despine()","title":"Chapter 5: Paths"},{"location":"02-algorithms/02-paths/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"JjpbztqP9_0\" , width = \"100%\" ) Graph traversal is akin to walking along the graph, node by node, constrained by the edges that connect the nodes. Graph traversal is particularly useful for understanding the local structure of certain portions of the graph and for finding paths that connect two nodes in the network. In this chapter, we are going to learn how to perform pathfinding in a graph, specifically by looking for shortest paths via the breadth-first search algorithm.","title":"Introduction"},{"location":"02-algorithms/02-paths/#breadth-first-search","text":"The BFS algorithm is a staple of computer science curricula, and for good reason: it teaches learners how to \"think on\" a graph, putting one in the position of \"the dumb computer\" that can't use a visual cortex to \" just know \" how to trace a path from one node to another. As a topic, learning how to do BFS additionally imparts algorithmic thinking to the learner.","title":"Breadth-First Search"},{"location":"02-algorithms/02-paths/#exercise-design-the-algorithm","text":"Try out this exercise to get some practice with algorithmic thinking. On a piece of paper, conjure up a graph that has 15-20 nodes. Connect them any way you like. Pick two nodes. Pretend that you're standing on one of the nodes, but you can't see any further beyond one neighbor away. Work out how you can find a path from the node you're standing on to the other node, given that you can only see nodes that are one neighbor away but have an infinitely good memory. If you are successful at designing the algorithm, you should get the answer below. from nams import load_data as cf G = cf . load_sociopatterns_network () from nams.solutions.paths import bfs_algorithm # UNCOMMENT NEXT LINE TO GET THE ANSWER. # bfs_algorithm()","title":"Exercise: Design the algorithm"},{"location":"02-algorithms/02-paths/#exercise-implement-the-algorithm","text":"Now that you've seen how the algorithm works, try implementing it! # FILL IN THE BLANKS BELOW def path_exists ( node1 , node2 , G ): \"\"\" This function checks whether a path exists between two nodes (node1, node2) in graph G. \"\"\" visited_nodes = _____ queue = [ _____ ] while len ( queue ) > 0 : node = ___________ neighbors = list ( _________________ ) if _____ in _________ : # print('Path exists between nodes {0} and {1}'.format(node1, node2)) return True else : visited_nodes . ___ ( ____ ) nbrs = [ _ for _ in _________ if _ not in _____________ ] queue = ____ + _____ # print('Path does not exist between nodes {0} and {1}'.format(node1, node2)) return False # UNCOMMENT THE FOLLOWING TWO LINES TO SEE THE ANSWER from nams.solutions.paths import path_exists # path_exists?? # CHECK YOUR ANSWER AGAINST THE TEST FUNCTION BELOW from random import sample import networkx as nx def test_path_exists ( N ): \"\"\" N: The number of times to spot-check. \"\"\" for i in range ( N ): n1 , n2 = sample ( G . nodes (), 2 ) assert path_exists ( n1 , n2 , G ) == bool ( nx . shortest_path ( G , n1 , n2 )) return True assert test_path_exists ( 10 )","title":"Exercise: Implement the algorithm"},{"location":"02-algorithms/02-paths/#visualizing-paths","text":"One of the objectives of that exercise before was to help you \"think on graphs\". Now that you've learned how to do so, you might be wondering, \"How do I visualize that path through the graph?\" Well first off, if you inspect the test_path_exists function above, you'll notice that NetworkX provides a shortest_path() function that you can use. Here's what using nx.shortest_path() looks like. path = nx . shortest_path ( G , 7 , 400 ) path [7, 51, 188, 230, 335, 400] As you can see, it returns the nodes along the shortest path, incidentally in the exact order that you would traverse. One thing to note, though! If there are multiple shortest paths from one node to another, NetworkX will only return one of them. So how do you draw those nodes only ? You can use the G.subgraph(nodes) to return a new graph that only has nodes in nodes and only the edges that exist between them. After that, you can use any plotting library you like. We will show an example here that uses nxviz's matrix plot. Let's see it in action: import nxviz as nv g = G . subgraph ( path ) nv . matrix ( g , sort_by = \"order\" ) <AxesSubplot:> Voila! Now we have the subgraph (1) extracted and (2) drawn to screen! In this case, the matrix plot is a suitable visualization for its compactness. The off-diagonals also show that each node is a neighbor to the next one. You'll also notice that if you try to modify the graph g , say by adding a node: g . add_node ( 2048 ) you will get an error: --------------------------------------------------------------------------- NetworkXError Traceback ( most recent call last ) < ipython - input - 10 - ca6aa4c26819 > in < module > ----> 1 g . add_node ( 2048 ) ~/ anaconda / envs / nams / lib / python3 .7 / site - packages / networkx / classes / function . py in frozen ( * args , ** kwargs ) 156 def frozen ( * args , ** kwargs ): 157 \"\"\"Dummy method for raising errors when trying to modify frozen graphs\"\"\" --> 158 raise nx . NetworkXError ( \"Frozen graph can't be modified\" ) 159 160 NetworkXError : Frozen graph can 't be modified From the perspective of semantics, this makes a ton of sense: the subgraph g is a perfect subset of the larger graph G , and should not be allowed to be modified unless the larger container graph is modified.","title":"Visualizing Paths"},{"location":"02-algorithms/02-paths/#exercise-draw-path-with-neighbors-one-degree-out","text":"Try out this next exercise: Extend graph drawing with the neighbors of each of those nodes. Use any of the nxviz plots ( nv.matrix , nv.arc , nv.circos ); try to see which one helps you tell the best story. from nams.solutions.paths import plot_path_with_neighbors ### YOUR SOLUTION BELOW plot_path_with_neighbors ( G , 7 , 400 ) In this case, we opted for an Arc plot because we only have one grouping of nodes but have a logical way to order them. Because the path follows the order, the edges being highlighted automatically look like hops through the graph.","title":"Exercise: Draw path with neighbors one degree out"},{"location":"02-algorithms/02-paths/#bottleneck-nodes","text":"We're now going to revisit the concept of an \"important node\", this time now leveraging what we know about paths. In the \"hubs\" chapter, we saw how a node that is \"important\" could be so because it is connected to many other nodes. Paths give us an alternative definition. If we imagine that we have to pass a message on a graph from one node to another, then there may be \"bottleneck\" nodes for which if they are removed, then messages have a harder time flowing through the graph. One metric that measures this form of importance is the \"betweenness centrality\" metric. On a graph through which a generic \"message\" is flowing, a node with a high betweenness centrality is one that has a high proportion of shortest paths flowing through it. In other words, it behaves like a bottleneck .","title":"Bottleneck nodes"},{"location":"02-algorithms/02-paths/#betweenness-centrality-in-networkx","text":"NetworkX provides a \"betweenness centrality\" function that behaves consistently with the \"degree centrality\" function, in that it returns a mapping from node to metric: import pandas as pd pd . Series ( nx . betweenness_centrality ( G )) 100 0.014809 101 0.001398 102 0.000748 103 0.006735 104 0.001198 ... 89 0.000004 91 0.006415 96 0.000323 99 0.000322 98 0.000000 Length: 410, dtype: float64","title":"Betweenness centrality in NetworkX"},{"location":"02-algorithms/02-paths/#exercise-compare-degree-and-betweenness-centrality","text":"Make a scatterplot of degree centrality on the x-axis and betweenness centrality on the y-axis. Do they correlate with one another? import matplotlib.pyplot as plt import seaborn as sns # YOUR ANSWER HERE: from nams.solutions.paths import plot_degree_betweenness plot_degree_betweenness ( G )","title":"Exercise: compare degree and betweenness centrality"},{"location":"02-algorithms/02-paths/#think-about-it","text":"...does it make sense that degree centrality and betweenness centrality are not well-correlated? Can you think of a scenario where a node has a \"high\" betweenness centrality but a \"low\" degree centrality? Before peeking at the graph below, think about your answer for a moment. nx . draw ( nx . barbell_graph ( 5 , 1 ))","title":"Think about it..."},{"location":"02-algorithms/02-paths/#recap","text":"In this chapter, you learned the following things: You figured out how to implement the breadth-first-search algorithm to find shortest paths. You learned how to extract subgraphs from a larger graph. You implemented visualizations of subgraphs, which should help you as you communicate with colleagues. You calculated betweenness centrality metrics for a graph, and visualized how they correlated with degree centrality.","title":"Recap"},{"location":"02-algorithms/02-paths/#solutions","text":"Here are the solutions to the exercises above. from nams.solutions import paths import inspect print ( inspect . getsource ( paths )) \"\"\"Solutions to Paths chapter.\"\"\" import matplotlib.pyplot as plt import networkx as nx import pandas as pd import seaborn as sns from nams.functions import render_html def bfs_algorithm(): \"\"\" How to design a BFS algorithm. \"\"\" ans = \"\"\" How does the breadth-first search work? It essentially is as follows: 1. Begin with a queue that has only one element in it: the starting node. 2. Add the neighbors of that node to the queue. 1. If destination node is present in the queue, end. 2. If destination node is not present, proceed. 3. For each node in the queue: 1. Remove node from the queue. 2. Add neighbors of the node to the queue. Check if destination node is present or not. 3. If destination node is present, end. <!--Credit: @cavaunpeu for finding bug in pseudocode.--> 4. If destination node is not present, continue. \"\"\" return render_html(ans) def path_exists(node1, node2, G): \"\"\" This function checks whether a path exists between two nodes (node1, node2) in graph G. \"\"\" visited_nodes = set() queue = [node1] while len(queue) > 0: node = queue.pop() neighbors = list(G.neighbors(node)) if node2 in neighbors: return True else: visited_nodes.add(node) nbrs = [n for n in neighbors if n not in visited_nodes] queue = nbrs + queue return False def path_exists_for_loop(node1, node2, G): \"\"\" This function checks whether a path exists between two nodes (node1, node2) in graph G. Special thanks to @ghirlekar for suggesting that we keep track of the \"visited nodes\" to prevent infinite loops from happening. This also removes the need to remove nodes from queue. Reference: https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/3 With thanks to @joshporter1 for the second bug fix. Originally there was an extraneous \"if\" statement that guaranteed that the \"False\" case would never be returned - because queue never changes in shape. Discovered at PyCon 2017. With thanks to @chendaniely for pointing out the extraneous \"break\". If you would like to see @dgerlanc's implementation, see https://github.com/ericmjl/Network-Analysis-Made-Simple/issues/76 \"\"\" visited_nodes = set() queue = [node1] for node in queue: neighbors = list(G.neighbors(node)) if node2 in neighbors: return True else: visited_nodes.add(node) queue.extend([n for n in neighbors if n not in visited_nodes]) return False def path_exists_deque(node1, node2, G): \"\"\"An alternative implementation.\"\"\" from collections import deque visited_nodes = set() queue = deque([node1]) while len(queue) > 0: node = queue.popleft() neighbors = list(G.neighbors(node)) if node2 in neighbors: return True else: visited_nodes.add(node) queue.extend([n for n in neighbors if n not in visited_nodes]) return False import nxviz as nv from nxviz import annotate, highlights def plot_path_with_neighbors(G, n1, n2): \"\"\"Plot a path with the heighbors of of the nodes along that path.\"\"\" path = nx.shortest_path(G, n1, n2) nodes = [*path] for node in path: nodes.extend(list(G.neighbors(node))) nodes = list(set(nodes)) g = G.subgraph(nodes) nv.arc( g, sort_by=\"order\", node_color_by=\"order\", edge_enc_kwargs={\"alpha_scale\": 0.5} ) for n in path: highlights.arc_node(g, n, sort_by=\"order\") for n1, n2 in zip(path[:-1], path[1:]): highlights.arc_edge(g, n1, n2, sort_by=\"order\") def plot_degree_betweenness(G): \"\"\"Plot scatterplot between degree and betweenness centrality.\"\"\" bc = pd.Series(nx.betweenness_centrality(G)) dc = pd.Series(nx.degree_centrality(G)) df = pd.DataFrame(dict(bc=bc, dc=dc)) ax = df.plot(x=\"dc\", y=\"bc\", kind=\"scatter\") ax.set_ylabel(\"Betweenness\\nCentrality\") ax.set_xlabel(\"Degree Centrality\") sns.despine()","title":"Solutions"},{"location":"02-algorithms/03-structures/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"3DWSRCbPPJs\" , width = \"100%\" ) If you remember, at the beginning of this book, we saw a quote from John Quackenbush that essentially said that the reason a graph is interesting is because of its edges. In this chapter, we'll see this in action once again, as we are going to figure out how to leverage the edges to find special structures in a graph. Triangles The first structure that we are going to learn about is triangles . Triangles are super interesting! They are what one might consider to be \"the simplest complex structure\" in a graph. Triangles can also have semantically-rich meaning depending on the application. To borrow a bad example, love triangles in social networks are generally frowned upon, while on the other hand, when we connect two people that we know together, we instead complete a triangle. Load Data To learn about triangles, we are going to leverage a physician trust network. Here's the data description: This directed network captures innovation spread among 246 physicians for towns in Illinois, Peoria, Bloomington, Quincy and Galesburg. The data was collected in 1966. A node represents a physician and an edge between two physicians shows that the left physician told that the right physician is his friend or that he turns to the right physician if he needs advice or is interested in a discussion. There always only exists one edge between two nodes even if more than one of the listed conditions are true. from nams import load_data as cf G = cf . load_physicians_network () Exercise: Finding triangles in a graph This exercise is going to flex your ability to \"think on a graph\", just as you did in the previous chapters. Leveraging what you know, can you think of a few strategies to find triangles in a graph? from nams.solutions.structures import triangle_finding_strategies # triangle_finding_strategies() Exercise: Identify whether a node is in a triangle relationship or not Let's now get down to implementing this next piece of code. Write a function that identifies whether a node is or is not in a triangle relationship. It should take in a graph G and a node n , and return a boolean True if the node n is in any triangle relationship and boolean False if the node n is not in any triangle relationship. A hint that may help you: Every graph object G has a G.has_edge(n1, n2) method that you can use to identify whether a graph has an edge between n1 and n2 . Also: itertools.combinations lets you iterate over every K-combination of items in an iterable. def in_triangle ( G , node ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import in_triangle # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # in_triangle?? Now, test your implementation below! The code cell will not error out if your answer is correct. from random import sample import networkx as nx def test_in_triangle (): nodes = sample ( G . nodes (), 10 ) for node in nodes : assert in_triangle ( G , 3 ) == bool ( nx . triangles ( G , 3 )) test_in_triangle () As you can see from the test function above, NetworkX provides an nx.triangles(G, node) function. It returns the number of triangles that a node is involved in. We convert it to boolean as a hack to check whether or not a node is involved in a triangle relationship because 0 is equivalent to boolean False , while any non-zero number is equivalent to boolean True . Exercise: Extract triangles for plotting We're going to leverage another piece of knowledge that you already have: the ability to extract subgraphs. We'll be plotting all of the triangles that a node is involved in. Given a node, write a function that extracts out all of the neighbors that it is in a triangle relationship with. Then, in a new function, implement code that plots only the subgraph that contains those nodes. def get_triangle_neighbors ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import get_triangle_neighbors # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # get_triangle_neighbors?? def plot_triangle_relations ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import plot_triangle_relations plot_triangle_relations ( G , 3 ) Triadic Closure In professional circles, making connections between two people is one of the most valuable things you can do professionally. What you do in that moment is what we would call triadic closure . Algorithmically, we can do the same thing if we maintain a graph of connections! Essentially, what we are looking for are \"open\" or \"unfinished\" triangles\". In this section, we'll try our hand at implementing a rudimentary triadic closure system. Exercise: Design the algorithm What graph logic would you use to identify triadic closure opportunities? Try writing out your general strategy, or discuss it with someone. from nams.solutions.structures import triadic_closure_algorithm # UNCOMMENT FOR MY ANSWER # triadic_closure_algorithm() Exercise: Implement triadic closure. Now, try your hand at implementing triadic closure. Write a function that takes in a graph G and a node n , and returns all of the neighbors that are potential triadic closures with n being the center node. def get_open_triangles_neighbors ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import get_open_triangles_neighbors # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # get_open_triangles_neighbors?? Exercise: Plot the open triangles Now, write a function that takes in a graph G and a node n , and plots out that node n and all of the neighbors that it could help close triangles with. def plot_open_triangle_relations ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import plot_open_triangle_relations plot_open_triangle_relations ( G , 3 ) Cliques Triangles are interesting in a graph theoretic setting because triangles are the simplest complex clique that exist. But wait! What is the definition of a \"clique\"? A \"clique\" is a set of nodes in a graph that are fully connected with one another by edges between them. Exercise: Simplest cliques Given this definition, what is the simplest \"clique\" possible? from nams.solutions.structures import simplest_clique # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # simplest_clique() k k -Cliques Cliques are identified by their size k k , which is the number of nodes that are present in the clique. A triangle is what we would consider to be a k k -clique where k=3 k=3 . A square with cross-diagonal connections is what we would consider to be a k k -clique where k=4 k=4 . By now, you should get the gist of the idea. Maximal Cliques Related to this idea of a k k -clique is another idea called \"maximal cliques\". Maximal cliques are defined as follows: A maximal clique is a subgraph of nodes in a graph to which no other node can be added to it and still remain a clique. NetworkX provides a way to find all maximal cliques: # I have truncated the output to the first 5 maximal cliques. list ( nx . find_cliques ( G ))[ 0 : 5 ] [[1, 2], [1, 3], [1, 4, 5, 6], [1, 7], [1, 72]] Exercise: finding sized- k k maximal cliques Write a generator function that yields all maximal cliques of size k k . I'm requesting a generator as a matter of good practice; you never know when the list you return might explode in memory consumption, so generators are a cheap and easy way to reduce memory usage. def size_k_maximal_cliques ( G , k ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import size_k_maximal_cliques Now, test your implementation against the test function below. def test_size_k_maximal_cliques ( G , k ): clique_generator = size_k_maximal_cliques ( G , k ) for clique in clique_generator : assert len ( clique ) == k test_size_k_maximal_cliques ( G , 5 ) Clique Decomposition One super neat property of cliques is that every clique of size k k can be decomposed to the set of cliques of size k-1 k-1 . Does this make sense to you? If not, think about triangles (3-cliques). They can be decomposed to three edges (2-cliques). Think again about 4-cliques. Housed within 4-cliques are four 3-cliques. Draw it out if you're still not convinced! Exercise: finding all k k -cliques in a graph Knowing this property of k k -cliques, write a generator function that yields all k k -cliques in a graph, leveraging the nx.find_cliques(G) function. Some hints to help you along: If a k k -clique can be decomposed to its k-1 k-1 cliques, it follows that the k-1 k-1 cliques can be decomposed into k-2 k-2 cliques, and so on until you hit 2-cliques. This implies that all cliques of size k k house cliques of size n < k n < k , where n >= 2 n >= 2 . def find_k_cliques ( G , k ): # your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import find_k_cliques def test_find_k_cliques ( G , k ): for clique in find_k_cliques ( G , k ): assert len ( clique ) == k test_find_k_cliques ( G , 3 ) Connected Components Now that we've explored a lot around cliques, we're now going to explore this idea of \"connected components\". To do so, I am going to have you draw the graph that we are working with. import nxviz as nv nv . circos ( G ) <AxesSubplot:> Exercise: Visual insights From this rendering of the CircosPlot, what visual insights do you have about the structure of the graph? from nams.solutions.structures import visual_insights # UNCOMMENT TO SEE MY ANSWER # visual_insights() Defining connected components From Wikipedia : In graph theory, a connected component (or just component) of an undirected graph is a subgraph in which any two vertices are connected to each other by paths, and which is connected to no additional vertices in the supergraph. NetworkX provides a function to let us find all of the connected components: ccsubgraph_nodes = list ( nx . connected_components ( G )) Let's see how many connected component subgraphs are present: len ( ccsubgraph_nodes ) 4 Exercise: visualizing connected component subgraphs In this exercise, we're going to draw a circos plot of the graph, but colour and order the nodes by their connected component subgraph. Recall Circos API: c = CircosPlot ( G , node_order = 'node_attribute' , node_color = 'node_attribute' ) c . draw () plt . show () # or plt.savefig(...) Follow the steps along here to accomplish this. Firstly, label the nodes with a unique identifier for connected component subgraph that it resides in. Use subgraph to store this piece of metadata. def label_connected_component_subgraphs ( G ): # Your answer here return G # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import label_connected_component_subgraphs G_labelled = label_connected_component_subgraphs ( G ) # UNCOMMENT TO SEE THE ANSWER # label_connected_component_subgraphs?? Now, draw a CircosPlot with the node order and colouring dictated by the subgraph key. def plot_cc_subgraph ( G ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import plot_cc_subgraph from nxviz import annotate plot_cc_subgraph ( G_labelled ) annotate . circos_group ( G_labelled , group_by = \"subgraph\" ) Using an arc plot will also clearly illuminate for us that there are no inter-group connections. nv . arc ( G_labelled , group_by = \"subgraph\" , node_color_by = \"subgraph\" ) annotate . arc_group ( G_labelled , group_by = \"subgraph\" , rotation = 0 ) Voila! It looks quite clear that there are indeed four disjoint group of physicians. Solutions from nams.solutions import structures import inspect print ( inspect . getsource ( structures )) \"\"\"Solutions to Structures chapter.\"\"\" from itertools import combinations import networkx as nx from nxviz import circos from nams.functions import render_html def triangle_finding_strategies(): \"\"\" How to find triangles. \"\"\" ans = \"\"\" One way would be to take one node, and look at its neighbors. If its neighbors are also connected to one another, then we have found a triangle. Another way would be to start at a given node, and walk out two nodes. If the starting node is the neighbor of the node two hops away, then the path we traced traces out the nodes in a triangle. \"\"\" return render_html(ans) def in_triangle(G, node): \"\"\" Return whether a given node is present in a triangle relationship. \"\"\" for nbr1, nbr2 in combinations(G.neighbors(node), 2): if G.has_edge(nbr1, nbr2): return True return False def get_triangle_neighbors(G, node) -> set: \"\"\" Return neighbors involved in triangle relationship with node. \"\"\" neighbors1 = set(G.neighbors(node)) triangle_nodes = set() for nbr1, nbr2 in combinations(neighbors1, 2): if G.has_edge(nbr1, nbr2): triangle_nodes.add(nbr1) triangle_nodes.add(nbr2) return triangle_nodes def plot_triangle_relations(G, node): \"\"\" Plot all triangle relationships for a given node. \"\"\" triangle_nbrs = get_triangle_neighbors(G, node) triangle_nbrs.add(node) nx.draw(G.subgraph(triangle_nbrs), with_labels=True) def triadic_closure_algorithm(): \"\"\" How to do triadic closure. \"\"\" ans = \"\"\" I would suggest the following strategy: 1. Pick a node 1. For every pair of neighbors: 1. If neighbors are not connected, then this is a potential triangle to close. This strategy gives you potential triadic closures given a \"center\" node `n`. The other way is to trace out a path two degrees out and ask whether the terminal node is a neighbor of the starting node. If not, then we have another triadic closure to make. \"\"\" return render_html(ans) def get_open_triangles_neighbors(G, node) -> set: \"\"\" Return neighbors involved in open triangle relationships with a node. \"\"\" open_triangle_nodes = set() neighbors = list(G.neighbors(node)) for n1, n2 in combinations(neighbors, 2): if not G.has_edge(n1, n2): open_triangle_nodes.add(n1) open_triangle_nodes.add(n2) return open_triangle_nodes def plot_open_triangle_relations(G, node): \"\"\" Plot open triangle relationships for a given node. \"\"\" open_triangle_nbrs = get_open_triangles_neighbors(G, node) open_triangle_nbrs.add(node) nx.draw(G.subgraph(open_triangle_nbrs), with_labels=True) def simplest_clique(): \"\"\" Answer to \"what is the simplest clique\". \"\"\" return render_html(\"The simplest clique is an edge.\") def size_k_maximal_cliques(G, k): \"\"\" Return all size-k maximal cliques. \"\"\" for clique in nx.find_cliques(G): if len(clique) == k: yield clique def find_k_cliques(G, k): \"\"\" Find all cliques of size k. \"\"\" for clique in nx.find_cliques(G): if len(clique) >= k: for nodeset in combinations(clique, k): yield nodeset def visual_insights(): \"\"\" Answer to visual insights exercise. \"\"\" ans = \"\"\" We might hypothesize that there are 3, maybe 4 different \"communities\" of nodes that are completely disjoint with one another, i.e. there is no path between them. \"\"\" print(ans) def label_connected_component_subgraphs(G): \"\"\"Label all connected component subgraphs.\"\"\" G = G.copy() for i, nodeset in enumerate(nx.connected_components(G)): for n in nodeset: G.nodes[n][\"subgraph\"] = i return G def plot_cc_subgraph(G): \"\"\"Plot all connected component subgraphs.\"\"\" c = circos(G, node_color_by=\"subgraph\", group_by=\"subgraph\")","title":"Chapter 6: Structures"},{"location":"02-algorithms/03-structures/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"3DWSRCbPPJs\" , width = \"100%\" ) If you remember, at the beginning of this book, we saw a quote from John Quackenbush that essentially said that the reason a graph is interesting is because of its edges. In this chapter, we'll see this in action once again, as we are going to figure out how to leverage the edges to find special structures in a graph.","title":"Introduction"},{"location":"02-algorithms/03-structures/#triangles","text":"The first structure that we are going to learn about is triangles . Triangles are super interesting! They are what one might consider to be \"the simplest complex structure\" in a graph. Triangles can also have semantically-rich meaning depending on the application. To borrow a bad example, love triangles in social networks are generally frowned upon, while on the other hand, when we connect two people that we know together, we instead complete a triangle.","title":"Triangles"},{"location":"02-algorithms/03-structures/#load-data","text":"To learn about triangles, we are going to leverage a physician trust network. Here's the data description: This directed network captures innovation spread among 246 physicians for towns in Illinois, Peoria, Bloomington, Quincy and Galesburg. The data was collected in 1966. A node represents a physician and an edge between two physicians shows that the left physician told that the right physician is his friend or that he turns to the right physician if he needs advice or is interested in a discussion. There always only exists one edge between two nodes even if more than one of the listed conditions are true. from nams import load_data as cf G = cf . load_physicians_network ()","title":"Load Data"},{"location":"02-algorithms/03-structures/#exercise-finding-triangles-in-a-graph","text":"This exercise is going to flex your ability to \"think on a graph\", just as you did in the previous chapters. Leveraging what you know, can you think of a few strategies to find triangles in a graph? from nams.solutions.structures import triangle_finding_strategies # triangle_finding_strategies()","title":"Exercise: Finding triangles in a graph"},{"location":"02-algorithms/03-structures/#exercise-identify-whether-a-node-is-in-a-triangle-relationship-or-not","text":"Let's now get down to implementing this next piece of code. Write a function that identifies whether a node is or is not in a triangle relationship. It should take in a graph G and a node n , and return a boolean True if the node n is in any triangle relationship and boolean False if the node n is not in any triangle relationship. A hint that may help you: Every graph object G has a G.has_edge(n1, n2) method that you can use to identify whether a graph has an edge between n1 and n2 . Also: itertools.combinations lets you iterate over every K-combination of items in an iterable. def in_triangle ( G , node ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import in_triangle # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # in_triangle?? Now, test your implementation below! The code cell will not error out if your answer is correct. from random import sample import networkx as nx def test_in_triangle (): nodes = sample ( G . nodes (), 10 ) for node in nodes : assert in_triangle ( G , 3 ) == bool ( nx . triangles ( G , 3 )) test_in_triangle () As you can see from the test function above, NetworkX provides an nx.triangles(G, node) function. It returns the number of triangles that a node is involved in. We convert it to boolean as a hack to check whether or not a node is involved in a triangle relationship because 0 is equivalent to boolean False , while any non-zero number is equivalent to boolean True .","title":"Exercise: Identify whether a node is in a triangle relationship or not"},{"location":"02-algorithms/03-structures/#exercise-extract-triangles-for-plotting","text":"We're going to leverage another piece of knowledge that you already have: the ability to extract subgraphs. We'll be plotting all of the triangles that a node is involved in. Given a node, write a function that extracts out all of the neighbors that it is in a triangle relationship with. Then, in a new function, implement code that plots only the subgraph that contains those nodes. def get_triangle_neighbors ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import get_triangle_neighbors # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # get_triangle_neighbors?? def plot_triangle_relations ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import plot_triangle_relations plot_triangle_relations ( G , 3 )","title":"Exercise: Extract triangles for plotting"},{"location":"02-algorithms/03-structures/#triadic-closure","text":"In professional circles, making connections between two people is one of the most valuable things you can do professionally. What you do in that moment is what we would call triadic closure . Algorithmically, we can do the same thing if we maintain a graph of connections! Essentially, what we are looking for are \"open\" or \"unfinished\" triangles\". In this section, we'll try our hand at implementing a rudimentary triadic closure system.","title":"Triadic Closure"},{"location":"02-algorithms/03-structures/#exercise-design-the-algorithm","text":"What graph logic would you use to identify triadic closure opportunities? Try writing out your general strategy, or discuss it with someone. from nams.solutions.structures import triadic_closure_algorithm # UNCOMMENT FOR MY ANSWER # triadic_closure_algorithm()","title":"Exercise: Design the algorithm"},{"location":"02-algorithms/03-structures/#exercise-implement-triadic-closure","text":"Now, try your hand at implementing triadic closure. Write a function that takes in a graph G and a node n , and returns all of the neighbors that are potential triadic closures with n being the center node. def get_open_triangles_neighbors ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import get_open_triangles_neighbors # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # get_open_triangles_neighbors??","title":"Exercise: Implement triadic closure."},{"location":"02-algorithms/03-structures/#exercise-plot-the-open-triangles","text":"Now, write a function that takes in a graph G and a node n , and plots out that node n and all of the neighbors that it could help close triangles with. def plot_open_triangle_relations ( G , n ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import plot_open_triangle_relations plot_open_triangle_relations ( G , 3 )","title":"Exercise: Plot the open triangles"},{"location":"02-algorithms/03-structures/#cliques","text":"Triangles are interesting in a graph theoretic setting because triangles are the simplest complex clique that exist. But wait! What is the definition of a \"clique\"? A \"clique\" is a set of nodes in a graph that are fully connected with one another by edges between them.","title":"Cliques"},{"location":"02-algorithms/03-structures/#exercise-simplest-cliques","text":"Given this definition, what is the simplest \"clique\" possible? from nams.solutions.structures import simplest_clique # UNCOMMENT THE NEXT LINE TO SEE MY ANSWER # simplest_clique()","title":"Exercise: Simplest cliques"},{"location":"02-algorithms/03-structures/#kk-cliques","text":"Cliques are identified by their size k k , which is the number of nodes that are present in the clique. A triangle is what we would consider to be a k k -clique where k=3 k=3 . A square with cross-diagonal connections is what we would consider to be a k k -clique where k=4 k=4 . By now, you should get the gist of the idea.","title":"kk-Cliques"},{"location":"02-algorithms/03-structures/#maximal-cliques","text":"Related to this idea of a k k -clique is another idea called \"maximal cliques\". Maximal cliques are defined as follows: A maximal clique is a subgraph of nodes in a graph to which no other node can be added to it and still remain a clique. NetworkX provides a way to find all maximal cliques: # I have truncated the output to the first 5 maximal cliques. list ( nx . find_cliques ( G ))[ 0 : 5 ] [[1, 2], [1, 3], [1, 4, 5, 6], [1, 7], [1, 72]]","title":"Maximal Cliques"},{"location":"02-algorithms/03-structures/#exercise-finding-sized-kk-maximal-cliques","text":"Write a generator function that yields all maximal cliques of size k k . I'm requesting a generator as a matter of good practice; you never know when the list you return might explode in memory consumption, so generators are a cheap and easy way to reduce memory usage. def size_k_maximal_cliques ( G , k ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import size_k_maximal_cliques Now, test your implementation against the test function below. def test_size_k_maximal_cliques ( G , k ): clique_generator = size_k_maximal_cliques ( G , k ) for clique in clique_generator : assert len ( clique ) == k test_size_k_maximal_cliques ( G , 5 )","title":"Exercise: finding sized-kk maximal cliques"},{"location":"02-algorithms/03-structures/#clique-decomposition","text":"One super neat property of cliques is that every clique of size k k can be decomposed to the set of cliques of size k-1 k-1 . Does this make sense to you? If not, think about triangles (3-cliques). They can be decomposed to three edges (2-cliques). Think again about 4-cliques. Housed within 4-cliques are four 3-cliques. Draw it out if you're still not convinced!","title":"Clique Decomposition"},{"location":"02-algorithms/03-structures/#exercise-finding-all-kk-cliques-in-a-graph","text":"Knowing this property of k k -cliques, write a generator function that yields all k k -cliques in a graph, leveraging the nx.find_cliques(G) function. Some hints to help you along: If a k k -clique can be decomposed to its k-1 k-1 cliques, it follows that the k-1 k-1 cliques can be decomposed into k-2 k-2 cliques, and so on until you hit 2-cliques. This implies that all cliques of size k k house cliques of size n < k n < k , where n >= 2 n >= 2 . def find_k_cliques ( G , k ): # your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import find_k_cliques def test_find_k_cliques ( G , k ): for clique in find_k_cliques ( G , k ): assert len ( clique ) == k test_find_k_cliques ( G , 3 )","title":"Exercise: finding all kk-cliques in a graph"},{"location":"02-algorithms/03-structures/#connected-components","text":"Now that we've explored a lot around cliques, we're now going to explore this idea of \"connected components\". To do so, I am going to have you draw the graph that we are working with. import nxviz as nv nv . circos ( G ) <AxesSubplot:>","title":"Connected Components"},{"location":"02-algorithms/03-structures/#exercise-visual-insights","text":"From this rendering of the CircosPlot, what visual insights do you have about the structure of the graph? from nams.solutions.structures import visual_insights # UNCOMMENT TO SEE MY ANSWER # visual_insights()","title":"Exercise: Visual insights"},{"location":"02-algorithms/03-structures/#defining-connected-components","text":"From Wikipedia : In graph theory, a connected component (or just component) of an undirected graph is a subgraph in which any two vertices are connected to each other by paths, and which is connected to no additional vertices in the supergraph. NetworkX provides a function to let us find all of the connected components: ccsubgraph_nodes = list ( nx . connected_components ( G )) Let's see how many connected component subgraphs are present: len ( ccsubgraph_nodes ) 4","title":"Defining connected components"},{"location":"02-algorithms/03-structures/#exercise-visualizing-connected-component-subgraphs","text":"In this exercise, we're going to draw a circos plot of the graph, but colour and order the nodes by their connected component subgraph. Recall Circos API: c = CircosPlot ( G , node_order = 'node_attribute' , node_color = 'node_attribute' ) c . draw () plt . show () # or plt.savefig(...) Follow the steps along here to accomplish this. Firstly, label the nodes with a unique identifier for connected component subgraph that it resides in. Use subgraph to store this piece of metadata. def label_connected_component_subgraphs ( G ): # Your answer here return G # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import label_connected_component_subgraphs G_labelled = label_connected_component_subgraphs ( G ) # UNCOMMENT TO SEE THE ANSWER # label_connected_component_subgraphs?? Now, draw a CircosPlot with the node order and colouring dictated by the subgraph key. def plot_cc_subgraph ( G ): # Your answer here pass # COMMENT OUT THE IMPORT LINE TO TEST YOUR ANSWER from nams.solutions.structures import plot_cc_subgraph from nxviz import annotate plot_cc_subgraph ( G_labelled ) annotate . circos_group ( G_labelled , group_by = \"subgraph\" ) Using an arc plot will also clearly illuminate for us that there are no inter-group connections. nv . arc ( G_labelled , group_by = \"subgraph\" , node_color_by = \"subgraph\" ) annotate . arc_group ( G_labelled , group_by = \"subgraph\" , rotation = 0 ) Voila! It looks quite clear that there are indeed four disjoint group of physicians.","title":"Exercise: visualizing connected component subgraphs"},{"location":"02-algorithms/03-structures/#solutions","text":"from nams.solutions import structures import inspect print ( inspect . getsource ( structures )) \"\"\"Solutions to Structures chapter.\"\"\" from itertools import combinations import networkx as nx from nxviz import circos from nams.functions import render_html def triangle_finding_strategies(): \"\"\" How to find triangles. \"\"\" ans = \"\"\" One way would be to take one node, and look at its neighbors. If its neighbors are also connected to one another, then we have found a triangle. Another way would be to start at a given node, and walk out two nodes. If the starting node is the neighbor of the node two hops away, then the path we traced traces out the nodes in a triangle. \"\"\" return render_html(ans) def in_triangle(G, node): \"\"\" Return whether a given node is present in a triangle relationship. \"\"\" for nbr1, nbr2 in combinations(G.neighbors(node), 2): if G.has_edge(nbr1, nbr2): return True return False def get_triangle_neighbors(G, node) -> set: \"\"\" Return neighbors involved in triangle relationship with node. \"\"\" neighbors1 = set(G.neighbors(node)) triangle_nodes = set() for nbr1, nbr2 in combinations(neighbors1, 2): if G.has_edge(nbr1, nbr2): triangle_nodes.add(nbr1) triangle_nodes.add(nbr2) return triangle_nodes def plot_triangle_relations(G, node): \"\"\" Plot all triangle relationships for a given node. \"\"\" triangle_nbrs = get_triangle_neighbors(G, node) triangle_nbrs.add(node) nx.draw(G.subgraph(triangle_nbrs), with_labels=True) def triadic_closure_algorithm(): \"\"\" How to do triadic closure. \"\"\" ans = \"\"\" I would suggest the following strategy: 1. Pick a node 1. For every pair of neighbors: 1. If neighbors are not connected, then this is a potential triangle to close. This strategy gives you potential triadic closures given a \"center\" node `n`. The other way is to trace out a path two degrees out and ask whether the terminal node is a neighbor of the starting node. If not, then we have another triadic closure to make. \"\"\" return render_html(ans) def get_open_triangles_neighbors(G, node) -> set: \"\"\" Return neighbors involved in open triangle relationships with a node. \"\"\" open_triangle_nodes = set() neighbors = list(G.neighbors(node)) for n1, n2 in combinations(neighbors, 2): if not G.has_edge(n1, n2): open_triangle_nodes.add(n1) open_triangle_nodes.add(n2) return open_triangle_nodes def plot_open_triangle_relations(G, node): \"\"\" Plot open triangle relationships for a given node. \"\"\" open_triangle_nbrs = get_open_triangles_neighbors(G, node) open_triangle_nbrs.add(node) nx.draw(G.subgraph(open_triangle_nbrs), with_labels=True) def simplest_clique(): \"\"\" Answer to \"what is the simplest clique\". \"\"\" return render_html(\"The simplest clique is an edge.\") def size_k_maximal_cliques(G, k): \"\"\" Return all size-k maximal cliques. \"\"\" for clique in nx.find_cliques(G): if len(clique) == k: yield clique def find_k_cliques(G, k): \"\"\" Find all cliques of size k. \"\"\" for clique in nx.find_cliques(G): if len(clique) >= k: for nodeset in combinations(clique, k): yield nodeset def visual_insights(): \"\"\" Answer to visual insights exercise. \"\"\" ans = \"\"\" We might hypothesize that there are 3, maybe 4 different \"communities\" of nodes that are completely disjoint with one another, i.e. there is no path between them. \"\"\" print(ans) def label_connected_component_subgraphs(G): \"\"\"Label all connected component subgraphs.\"\"\" G = G.copy() for i, nodeset in enumerate(nx.connected_components(G)): for n in nodeset: G.nodes[n][\"subgraph\"] = i return G def plot_cc_subgraph(G): \"\"\"Plot all connected component subgraphs.\"\"\" c = circos(G, node_color_by=\"subgraph\", group_by=\"subgraph\")","title":"Solutions"},{"location":"03-practical/01-io/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"3sJnTpeFXZ4\" , width = \"100%\" ) In order to get you familiar with graph ideas, I have deliberately chosen to steer away from the more pedantic matters of loading graph data to and from disk. That said, the following scenario will eventually happen, where a graph dataset lands on your lap, and you'll need to load it in memory and start analyzing it. Thus, we're going to go through graph I/O, specifically the APIs on how to convert graph data that comes to you into that magical NetworkX object G . Let's get going! Graph Data as Tables Let's recall what we've learned in the introductory chapters. Graphs can be represented using two sets : Node set Edge set Node set as tables Let's say we had a graph with 3 nodes in it: A, B, C . We could represent it in plain text, computer-readable format: A B C Suppose the nodes also had metadata. Then, we could tag on metadata as well: A, circle, 5 B, circle, 7 C, square, 9 Does this look familiar to you? Yes, node sets can be stored in CSV format, with one of the columns being node ID, and the rest of the columns being metadata. Edge set as tables If, between the nodes, we had 4 edges (this is a directed graph), we can also represent those edges in plain text, computer-readable format: A, C B, C A, B C, A And let's say we also had other metadata, we can represent it in the same CSV format: A, C, red B, C, orange A, B, yellow C, A, green If you've been in the data world for a while, this should not look foreign to you. Yes, edge sets can be stored in CSV format too! Two of the columns represent the nodes involved in an edge, and the rest of the columns represent the metadata. Combined Representation In fact, one might also choose to combine the node set and edge set tables together in a merged format: n1, n2, colour, shape1, num1, shape2, num2 A, C, red, circle, 5, square, 9 B, C, orange, circle, 7, square, 9 A, B, yellow, circle, 5, circle, 7 C, A, green, square, 9, circle, 5 In this chapter, the datasets that we will be looking at are going to be formatted in both ways. Let's get going. Dataset We will be working with the Divvy bike sharing dataset. Divvy is a bike sharing service in Chicago. Since 2013, Divvy has released their bike sharing dataset to the public. The 2013 dataset is comprised of two files: - Divvy_Stations_2013.csv , containing the stations in the system, and - DivvyTrips_2013.csv , containing the trips. Let's dig into the data! from pyprojroot import here Firstly, we need to unzip the dataset: import zipfile import os from nams.load_data import datasets # This block of code checks to make sure that a particular directory is present. if \"divvy_2013\" not in os . listdir ( datasets ): print ( 'Unzipping the divvy_2013.zip file in the datasets folder.' ) with zipfile . ZipFile ( datasets / \"divvy_2013.zip\" , \"r\" ) as zip_ref : zip_ref . extractall ( datasets ) Now, let's load in both tables. First is the stations table: import pandas as pd stations = pd . read_csv ( datasets / 'divvy_2013/Divvy_Stations_2013.csv' , parse_dates = [ 'online date' ], encoding = 'utf-8' ) stations . head () id name latitude longitude dpcapacity landmark online date 0 5 State St & Harrison St 41.874 -87.6277 19 30 2013-06-28 1 13 Wilton Ave & Diversey Pkwy 41.9325 -87.6527 19 66 2013-06-28 2 14 Morgan St & 18th St 41.8581 -87.6511 15 163 2013-06-28 3 15 Racine Ave & 18th St 41.8582 -87.6565 15 164 2013-06-28 4 16 Wood St & North Ave 41.9103 -87.6725 15 223 2013-08-12 stations . describe () id latitude longitude dpcapacity landmark count 300 300 300 300 300 mean 189.063 41.8963 -87.6482 16.8 192.013 std 99.4845 0.0409522 0.0230011 4.67399 120.535 min 5 41.7887 -87.7079 11 1 25% 108.75 41.8718 -87.6658 15 83.75 50% 196.5 41.8946 -87.6486 15 184.5 75% 276.25 41.9264 -87.6318 19 288.25 max 351 41.9784 -87.5807 47 440 Now, let's load in the trips table. trips = pd . read_csv ( datasets / 'divvy_2013/Divvy_Trips_2013.csv' , parse_dates = [ 'starttime' , 'stoptime' ]) trips . head () trip_id starttime stoptime bikeid tripduration from_station_id from_station_name to_station_id to_station_name usertype gender birthday 0 4118 2013-06-27 12:11:00 2013-06-27 12:16:00 480 316 85 Michigan Ave & Oak St 28 Larrabee St & Menomonee St Customer nan nan 1 4275 2013-06-27 14:44:00 2013-06-27 14:45:00 77 64 32 Racine Ave & Congress Pkwy 32 Racine Ave & Congress Pkwy Customer nan nan 2 4291 2013-06-27 14:58:00 2013-06-27 15:05:00 77 433 32 Racine Ave & Congress Pkwy 19 Loomis St & Taylor St Customer nan nan 3 4316 2013-06-27 15:06:00 2013-06-27 15:09:00 77 123 19 Loomis St & Taylor St 19 Loomis St & Taylor St Customer nan nan 4 4342 2013-06-27 15:13:00 2013-06-27 15:27:00 77 852 19 Loomis St & Taylor St 55 Halsted St & James M Rochford St Customer nan nan import janitor trips_summary = ( trips . groupby ([ \"from_station_id\" , \"to_station_id\" ]) . count () . reset_index () . select_columns ( [ \"from_station_id\" , \"to_station_id\" , \"trip_id\" ] ) . rename_column ( \"trip_id\" , \"num_trips\" ) ) trips_summary . head () from_station_id to_station_id num_trips 0 5 5 232 1 5 13 1 2 5 14 15 3 5 15 9 4 5 16 4 Graph Model Given the data, if we wished to use a graph as a data model for the number of trips between stations, then naturally, nodes would be the stations, and edges would be trips between them. This graph would be directed, as one could have more trips from station A to B and less in the reverse. With this definition, we can begin graph construction! Create NetworkX graph from pandas edgelist NetworkX provides an extremely convenient way to load data from a pandas DataFrame: import networkx as nx G = nx . from_pandas_edgelist ( df = trips_summary , source = \"from_station_id\" , target = \"to_station_id\" , edge_attr = [ \"num_trips\" ], create_using = nx . DiGraph ) Inspect the graph Once the graph is in memory, we can inspect it to get out summary graph statistics. print ( nx . info ( G )) Name: Type: DiGraph Number of nodes: 300 Number of edges: 44422 Average in degree: 148.0733 Average out degree: 148.0733 You'll notice that the edge metadata have been added correctly: we have recorded in there the number of trips between stations. list ( G . edges ( data = True ))[ 0 : 5 ] [(5, 5, {'num_trips': 232}), (5, 13, {'num_trips': 1}), (5, 14, {'num_trips': 15}), (5, 15, {'num_trips': 9}), (5, 16, {'num_trips': 4})] However, the node metadata is not present: list ( G . nodes ( data = True ))[ 0 : 5 ] [(5, {}), (13, {}), (14, {}), (15, {}), (16, {})] Annotate node metadata We have rich station data on hand, such as the longitude and latitude of each station, and it would be a pity to discard it, especially when we can potentially use it as part of the analysis or for visualization purposes. Let's see how we can add this information in. Firstly, recall what the stations dataframe looked like: stations . head () id name latitude longitude dpcapacity landmark online date 0 5 State St & Harrison St 41.874 -87.6277 19 30 2013-06-28 1 13 Wilton Ave & Diversey Pkwy 41.9325 -87.6527 19 66 2013-06-28 2 14 Morgan St & 18th St 41.8581 -87.6511 15 163 2013-06-28 3 15 Racine Ave & 18th St 41.8582 -87.6565 15 164 2013-06-28 4 16 Wood St & North Ave 41.9103 -87.6725 15 223 2013-08-12 The id column gives us the node ID in the graph, so if we set id to be the index, if we then also loop over each row, we can treat the rest of the columns as dictionary keys and values as dictionary values, and add the information into the graph. Let's see this in action. for node , metadata in stations . set_index ( \"id\" ) . iterrows (): for key , val in metadata . items (): G . nodes [ node ][ key ] = val Now, our node metadata should be populated. list ( G . nodes ( data = True ))[ 0 : 5 ] [(5, {'name': 'State St & Harrison St', 'latitude': 41.87395806, 'longitude': -87.62773949, 'dpcapacity': 19, 'landmark': 30, 'online date': Timestamp('2013-06-28 00:00:00')}), (13, {'name': 'Wilton Ave & Diversey Pkwy', 'latitude': 41.93250008, 'longitude': -87.65268082, 'dpcapacity': 19, 'landmark': 66, 'online date': Timestamp('2013-06-28 00:00:00')}), (14, {'name': 'Morgan St & 18th St', 'latitude': 41.858086, 'longitude': -87.651073, 'dpcapacity': 15, 'landmark': 163, 'online date': Timestamp('2013-06-28 00:00:00')}), (15, {'name': 'Racine Ave & 18th St', 'latitude': 41.85818061, 'longitude': -87.65648665, 'dpcapacity': 15, 'landmark': 164, 'online date': Timestamp('2013-06-28 00:00:00')}), (16, {'name': 'Wood St & North Ave', 'latitude': 41.910329, 'longitude': -87.672516, 'dpcapacity': 15, 'landmark': 223, 'online date': Timestamp('2013-08-12 00:00:00')})] In nxviz , a GeoPlot object is available that allows you to quickly visualize a graph that has geographic data. However, being matplotlib -based, it is going to be quickly overwhelmed by the sheer number of edges. As such, we are going to first filter the edges. Exercise: Filter graph edges Leveraging what you know about how to manipulate graphs, now try filtering edges. Hint: NetworkX graph objects can be deep-copied using G.copy() : G_copy = G . copy () Hint: NetworkX graph objects also let you remove edges: G . remove_edge ( node1 , node2 ) # does not return anything def filter_graph ( G , minimum_num_trips ): \"\"\" Filter the graph such that only edges that have minimum_num_trips or more are present. \"\"\" G_filtered = G . ____ () for _ , _ , _ in G . _____ ( data = ____ ): if d [ ___________ ] < ___ : G_________ . ___________ ( _ , _ ) return G_filtered from nams.solutions.io import filter_graph G_filtered = filter_graph ( G , 50 ) Visualize using GeoPlot nxviz provides a GeoPlot object that lets you quickly visualize geospatial graph data. A note on geospatial visualizations: As the creator of nxviz , I would recommend using proper geospatial packages to build custom geospatial graph viz, such as pysal .) That said, nxviz can probably do what you need for a quick-and-dirty view of the data. import nxviz as nv c = nv . geo ( G_filtered , node_color_by = \"dpcapacity\" ) Does that look familiar to you? Looks quite a bit like Chicago, I'd say :) Jesting aside, this visualization does help illustrate that the majority of trips occur between stations that are near the city center. Pickling Graphs Since NetworkX graphs are Python objects, the canonical way to save them is by pickling them. You can do this using: nx . write_gpickle ( G , file_path ) Here's an example in action: nx . write_gpickle ( G , \"/tmp/divvy.pkl\" ) And just to show that it can be loaded back into memory: G_loaded = nx . read_gpickle ( \"/tmp/divvy.pkl\" ) Exercise: checking graph integrity If you get a graph dataset as a pickle, you should always check it against reference properties to make sure of its data integrity. Write a function that tests that the graph has the correct number of nodes and edges inside it. def test_graph_integrity ( G ): \"\"\"Test integrity of raw Divvy graph.\"\"\" # Your solution here pass from nams.solutions.io import test_graph_integrity test_graph_integrity ( G ) Other text formats CSV files and pandas DataFrames give us a convenient way to store graph data, and if possible, do insist with your data collaborators that they provide you with graph data that are in this format. If they don't, however, no sweat! After all, Python is super versatile. In this ebook, we have loaded data in from non-CSV sources, sometimes by parsing text files raw, sometimes by treating special characters as delimiters in a CSV-like file, and sometimes by resorting to parsing JSON. You can see other examples of how we load data by browsing through the source file of load_data.py and studying how we construct graph objects. Solutions The solutions to this chapter's exercises are below from nams.solutions import io import inspect print ( inspect . getsource ( io )) \"\"\"Solutions to I/O chapter\"\"\" def filter_graph(G, minimum_num_trips): \"\"\" Filter the graph such that only edges that have minimum_num_trips or more are present. \"\"\" G_filtered = G.copy() for u, v, d in G.edges(data=True): if d[\"num_trips\"] < minimum_num_trips: G_filtered.remove_edge(u, v) return G_filtered def test_graph_integrity(G): \"\"\"Test integrity of raw Divvy graph.\"\"\" assert len(G.nodes()) == 300 assert len(G.edges()) == 44422","title":"Chapter 7: Graph I/O"},{"location":"03-practical/01-io/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"3sJnTpeFXZ4\" , width = \"100%\" ) In order to get you familiar with graph ideas, I have deliberately chosen to steer away from the more pedantic matters of loading graph data to and from disk. That said, the following scenario will eventually happen, where a graph dataset lands on your lap, and you'll need to load it in memory and start analyzing it. Thus, we're going to go through graph I/O, specifically the APIs on how to convert graph data that comes to you into that magical NetworkX object G . Let's get going!","title":"Introduction"},{"location":"03-practical/01-io/#graph-data-as-tables","text":"Let's recall what we've learned in the introductory chapters. Graphs can be represented using two sets : Node set Edge set","title":"Graph Data as Tables"},{"location":"03-practical/01-io/#node-set-as-tables","text":"Let's say we had a graph with 3 nodes in it: A, B, C . We could represent it in plain text, computer-readable format: A B C Suppose the nodes also had metadata. Then, we could tag on metadata as well: A, circle, 5 B, circle, 7 C, square, 9 Does this look familiar to you? Yes, node sets can be stored in CSV format, with one of the columns being node ID, and the rest of the columns being metadata.","title":"Node set as tables"},{"location":"03-practical/01-io/#edge-set-as-tables","text":"If, between the nodes, we had 4 edges (this is a directed graph), we can also represent those edges in plain text, computer-readable format: A, C B, C A, B C, A And let's say we also had other metadata, we can represent it in the same CSV format: A, C, red B, C, orange A, B, yellow C, A, green If you've been in the data world for a while, this should not look foreign to you. Yes, edge sets can be stored in CSV format too! Two of the columns represent the nodes involved in an edge, and the rest of the columns represent the metadata.","title":"Edge set as tables"},{"location":"03-practical/01-io/#combined-representation","text":"In fact, one might also choose to combine the node set and edge set tables together in a merged format: n1, n2, colour, shape1, num1, shape2, num2 A, C, red, circle, 5, square, 9 B, C, orange, circle, 7, square, 9 A, B, yellow, circle, 5, circle, 7 C, A, green, square, 9, circle, 5 In this chapter, the datasets that we will be looking at are going to be formatted in both ways. Let's get going.","title":"Combined Representation"},{"location":"03-practical/01-io/#dataset","text":"We will be working with the Divvy bike sharing dataset. Divvy is a bike sharing service in Chicago. Since 2013, Divvy has released their bike sharing dataset to the public. The 2013 dataset is comprised of two files: - Divvy_Stations_2013.csv , containing the stations in the system, and - DivvyTrips_2013.csv , containing the trips. Let's dig into the data! from pyprojroot import here Firstly, we need to unzip the dataset: import zipfile import os from nams.load_data import datasets # This block of code checks to make sure that a particular directory is present. if \"divvy_2013\" not in os . listdir ( datasets ): print ( 'Unzipping the divvy_2013.zip file in the datasets folder.' ) with zipfile . ZipFile ( datasets / \"divvy_2013.zip\" , \"r\" ) as zip_ref : zip_ref . extractall ( datasets ) Now, let's load in both tables. First is the stations table: import pandas as pd stations = pd . read_csv ( datasets / 'divvy_2013/Divvy_Stations_2013.csv' , parse_dates = [ 'online date' ], encoding = 'utf-8' ) stations . head () id name latitude longitude dpcapacity landmark online date 0 5 State St & Harrison St 41.874 -87.6277 19 30 2013-06-28 1 13 Wilton Ave & Diversey Pkwy 41.9325 -87.6527 19 66 2013-06-28 2 14 Morgan St & 18th St 41.8581 -87.6511 15 163 2013-06-28 3 15 Racine Ave & 18th St 41.8582 -87.6565 15 164 2013-06-28 4 16 Wood St & North Ave 41.9103 -87.6725 15 223 2013-08-12 stations . describe () id latitude longitude dpcapacity landmark count 300 300 300 300 300 mean 189.063 41.8963 -87.6482 16.8 192.013 std 99.4845 0.0409522 0.0230011 4.67399 120.535 min 5 41.7887 -87.7079 11 1 25% 108.75 41.8718 -87.6658 15 83.75 50% 196.5 41.8946 -87.6486 15 184.5 75% 276.25 41.9264 -87.6318 19 288.25 max 351 41.9784 -87.5807 47 440 Now, let's load in the trips table. trips = pd . read_csv ( datasets / 'divvy_2013/Divvy_Trips_2013.csv' , parse_dates = [ 'starttime' , 'stoptime' ]) trips . head () trip_id starttime stoptime bikeid tripduration from_station_id from_station_name to_station_id to_station_name usertype gender birthday 0 4118 2013-06-27 12:11:00 2013-06-27 12:16:00 480 316 85 Michigan Ave & Oak St 28 Larrabee St & Menomonee St Customer nan nan 1 4275 2013-06-27 14:44:00 2013-06-27 14:45:00 77 64 32 Racine Ave & Congress Pkwy 32 Racine Ave & Congress Pkwy Customer nan nan 2 4291 2013-06-27 14:58:00 2013-06-27 15:05:00 77 433 32 Racine Ave & Congress Pkwy 19 Loomis St & Taylor St Customer nan nan 3 4316 2013-06-27 15:06:00 2013-06-27 15:09:00 77 123 19 Loomis St & Taylor St 19 Loomis St & Taylor St Customer nan nan 4 4342 2013-06-27 15:13:00 2013-06-27 15:27:00 77 852 19 Loomis St & Taylor St 55 Halsted St & James M Rochford St Customer nan nan import janitor trips_summary = ( trips . groupby ([ \"from_station_id\" , \"to_station_id\" ]) . count () . reset_index () . select_columns ( [ \"from_station_id\" , \"to_station_id\" , \"trip_id\" ] ) . rename_column ( \"trip_id\" , \"num_trips\" ) ) trips_summary . head () from_station_id to_station_id num_trips 0 5 5 232 1 5 13 1 2 5 14 15 3 5 15 9 4 5 16 4","title":"Dataset"},{"location":"03-practical/01-io/#graph-model","text":"Given the data, if we wished to use a graph as a data model for the number of trips between stations, then naturally, nodes would be the stations, and edges would be trips between them. This graph would be directed, as one could have more trips from station A to B and less in the reverse. With this definition, we can begin graph construction!","title":"Graph Model"},{"location":"03-practical/01-io/#create-networkx-graph-from-pandas-edgelist","text":"NetworkX provides an extremely convenient way to load data from a pandas DataFrame: import networkx as nx G = nx . from_pandas_edgelist ( df = trips_summary , source = \"from_station_id\" , target = \"to_station_id\" , edge_attr = [ \"num_trips\" ], create_using = nx . DiGraph )","title":"Create NetworkX graph from pandas edgelist"},{"location":"03-practical/01-io/#inspect-the-graph","text":"Once the graph is in memory, we can inspect it to get out summary graph statistics. print ( nx . info ( G )) Name: Type: DiGraph Number of nodes: 300 Number of edges: 44422 Average in degree: 148.0733 Average out degree: 148.0733 You'll notice that the edge metadata have been added correctly: we have recorded in there the number of trips between stations. list ( G . edges ( data = True ))[ 0 : 5 ] [(5, 5, {'num_trips': 232}), (5, 13, {'num_trips': 1}), (5, 14, {'num_trips': 15}), (5, 15, {'num_trips': 9}), (5, 16, {'num_trips': 4})] However, the node metadata is not present: list ( G . nodes ( data = True ))[ 0 : 5 ] [(5, {}), (13, {}), (14, {}), (15, {}), (16, {})]","title":"Inspect the graph"},{"location":"03-practical/01-io/#annotate-node-metadata","text":"We have rich station data on hand, such as the longitude and latitude of each station, and it would be a pity to discard it, especially when we can potentially use it as part of the analysis or for visualization purposes. Let's see how we can add this information in. Firstly, recall what the stations dataframe looked like: stations . head () id name latitude longitude dpcapacity landmark online date 0 5 State St & Harrison St 41.874 -87.6277 19 30 2013-06-28 1 13 Wilton Ave & Diversey Pkwy 41.9325 -87.6527 19 66 2013-06-28 2 14 Morgan St & 18th St 41.8581 -87.6511 15 163 2013-06-28 3 15 Racine Ave & 18th St 41.8582 -87.6565 15 164 2013-06-28 4 16 Wood St & North Ave 41.9103 -87.6725 15 223 2013-08-12 The id column gives us the node ID in the graph, so if we set id to be the index, if we then also loop over each row, we can treat the rest of the columns as dictionary keys and values as dictionary values, and add the information into the graph. Let's see this in action. for node , metadata in stations . set_index ( \"id\" ) . iterrows (): for key , val in metadata . items (): G . nodes [ node ][ key ] = val Now, our node metadata should be populated. list ( G . nodes ( data = True ))[ 0 : 5 ] [(5, {'name': 'State St & Harrison St', 'latitude': 41.87395806, 'longitude': -87.62773949, 'dpcapacity': 19, 'landmark': 30, 'online date': Timestamp('2013-06-28 00:00:00')}), (13, {'name': 'Wilton Ave & Diversey Pkwy', 'latitude': 41.93250008, 'longitude': -87.65268082, 'dpcapacity': 19, 'landmark': 66, 'online date': Timestamp('2013-06-28 00:00:00')}), (14, {'name': 'Morgan St & 18th St', 'latitude': 41.858086, 'longitude': -87.651073, 'dpcapacity': 15, 'landmark': 163, 'online date': Timestamp('2013-06-28 00:00:00')}), (15, {'name': 'Racine Ave & 18th St', 'latitude': 41.85818061, 'longitude': -87.65648665, 'dpcapacity': 15, 'landmark': 164, 'online date': Timestamp('2013-06-28 00:00:00')}), (16, {'name': 'Wood St & North Ave', 'latitude': 41.910329, 'longitude': -87.672516, 'dpcapacity': 15, 'landmark': 223, 'online date': Timestamp('2013-08-12 00:00:00')})] In nxviz , a GeoPlot object is available that allows you to quickly visualize a graph that has geographic data. However, being matplotlib -based, it is going to be quickly overwhelmed by the sheer number of edges. As such, we are going to first filter the edges.","title":"Annotate node metadata"},{"location":"03-practical/01-io/#exercise-filter-graph-edges","text":"Leveraging what you know about how to manipulate graphs, now try filtering edges. Hint: NetworkX graph objects can be deep-copied using G.copy() : G_copy = G . copy () Hint: NetworkX graph objects also let you remove edges: G . remove_edge ( node1 , node2 ) # does not return anything def filter_graph ( G , minimum_num_trips ): \"\"\" Filter the graph such that only edges that have minimum_num_trips or more are present. \"\"\" G_filtered = G . ____ () for _ , _ , _ in G . _____ ( data = ____ ): if d [ ___________ ] < ___ : G_________ . ___________ ( _ , _ ) return G_filtered from nams.solutions.io import filter_graph G_filtered = filter_graph ( G , 50 )","title":"Exercise: Filter graph edges"},{"location":"03-practical/01-io/#visualize-using-geoplot","text":"nxviz provides a GeoPlot object that lets you quickly visualize geospatial graph data. A note on geospatial visualizations: As the creator of nxviz , I would recommend using proper geospatial packages to build custom geospatial graph viz, such as pysal .) That said, nxviz can probably do what you need for a quick-and-dirty view of the data. import nxviz as nv c = nv . geo ( G_filtered , node_color_by = \"dpcapacity\" ) Does that look familiar to you? Looks quite a bit like Chicago, I'd say :) Jesting aside, this visualization does help illustrate that the majority of trips occur between stations that are near the city center.","title":"Visualize using GeoPlot"},{"location":"03-practical/01-io/#pickling-graphs","text":"Since NetworkX graphs are Python objects, the canonical way to save them is by pickling them. You can do this using: nx . write_gpickle ( G , file_path ) Here's an example in action: nx . write_gpickle ( G , \"/tmp/divvy.pkl\" ) And just to show that it can be loaded back into memory: G_loaded = nx . read_gpickle ( \"/tmp/divvy.pkl\" )","title":"Pickling Graphs"},{"location":"03-practical/01-io/#exercise-checking-graph-integrity","text":"If you get a graph dataset as a pickle, you should always check it against reference properties to make sure of its data integrity. Write a function that tests that the graph has the correct number of nodes and edges inside it. def test_graph_integrity ( G ): \"\"\"Test integrity of raw Divvy graph.\"\"\" # Your solution here pass from nams.solutions.io import test_graph_integrity test_graph_integrity ( G )","title":"Exercise: checking graph integrity"},{"location":"03-practical/01-io/#other-text-formats","text":"CSV files and pandas DataFrames give us a convenient way to store graph data, and if possible, do insist with your data collaborators that they provide you with graph data that are in this format. If they don't, however, no sweat! After all, Python is super versatile. In this ebook, we have loaded data in from non-CSV sources, sometimes by parsing text files raw, sometimes by treating special characters as delimiters in a CSV-like file, and sometimes by resorting to parsing JSON. You can see other examples of how we load data by browsing through the source file of load_data.py and studying how we construct graph objects.","title":"Other text formats"},{"location":"03-practical/01-io/#solutions","text":"The solutions to this chapter's exercises are below from nams.solutions import io import inspect print ( inspect . getsource ( io )) \"\"\"Solutions to I/O chapter\"\"\" def filter_graph(G, minimum_num_trips): \"\"\" Filter the graph such that only edges that have minimum_num_trips or more are present. \"\"\" G_filtered = G.copy() for u, v, d in G.edges(data=True): if d[\"num_trips\"] < minimum_num_trips: G_filtered.remove_edge(u, v) return G_filtered def test_graph_integrity(G): \"\"\"Test integrity of raw Divvy graph.\"\"\" assert len(G.nodes()) == 300 assert len(G.edges()) == 44422","title":"Solutions"},{"location":"03-practical/02-testing/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"SdbKs-crm-g\" , width = \"100%\" ) By this point in the book, you should have observed that we have written a number of tests for our data. Why test? If you like it, put a ring on it... ...and if you rely on it, test it. I am personally a proponent of writing tests for our data because as data scientists, the fields of our data, and their correct values, form the \"data programming interface\" (DPI) much like function signatures form the \"application programming interface\" (API). Since we test the APIs that we rely on, we probably should test the DPIs that we rely on too. What to test When thinking about what part of the data to test, it can be confusing. After all, data are seemingly generated from random processes (my Bayesian foxtail has been revealed), and it seems difficult to test random processes. That said, from my experience handling data, I can suggest a few principles. Test invariants Firstly, we test invariant properties of the data. Put in plain language, things we know ought to be true. Using the Divvy bike dataset example, we know that every node ought to have a station name. Thus, the minimum that we can test is that the station_name attribute is present on every node. As an example: def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): assert \"station_name\" in d . keys () Test nullity Secondly, we can test that values that ought not to be null should not be null. Using the Divvy bike dataset example again, if we also know that the station name cannot be null or an empty string, then we can bake that into the test. def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): assert \"station_name\" in d . keys () assert bool ( d [ \"station_name\" ]) Test boundaries We can also test boundary values. For example, within the city of Chicago, we know that latitude and longitude values ought to be within the vicinity of 41.85003, -87.65005 . If we get data values that are, say, outside the range of [41, 42]; [-88, -87] , then we know that we have data issues as well. Here's an example: def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): # Test for station names. assert \"station_name\" in d . keys () assert bool ( d [ \"station_name\" ]) # Test for longitude/latitude assert d [ \"latitude\" ] >= 41 and d [ \"latitude\" ] <= 42 assert d [ \"longitude\" ] >= - 88 and d [ \"longitude\" ] <= - 87 An apology to geospatial experts: I genuinely don't know the bounding box lat/lon coordinates of Chicago, so if you know those coordinates, please reach out so I can update the test. Continuous data testing The key idea with testing is to have tests that continuously run all the time in the background without you ever needing to intervene to kickstart it off. It's like having a bot in the background always running checks for you so you don't have to kickstart them. To do so, you should be equipped with a few tools. I won't go into them in-depth here, as I will be writing a \"continuous data testing\" essay in the near future. That said, here is the gist. Firstly, use pytest to get set up with testing. You essentially write a test_something.py file in which you write your test suite, and your test functions are all nothinng more than simple functions. # test_data.py def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): # Test for station names. assert \"station_name\" in d . keys () assert bool ( d [ \"station_name\" ]) # Test for longitude/latitude assert d [ \"latitude\" ] >= 41 and d [ \"latitude\" ] <= 42 assert d [ \"longitude\" ] >= - 88 and d [ \"longitude\" ] <= - 87 At the command line, if you ran pytest , it will automatically discover all functions prefixed with test_ in all .py files underneath the current working directory. Secondly, set up a continuous pipelining system to continuously run data tests. For example, you can set up Jenkins , Travis , Azure Pipelines , Prefect , and more, depending on what your organization has bought into. Sometimes data tests take longer than software tests, especially if you are pulling dumps from a database, so you might want to run this portion of tests in a separate pipeline instead. Further reading In my essays collection, I wrote about testing data . Itamar Turner-Trauring has written about keeping tests quick and speedy , which is extremely crucial to keeping yourself motivated to write tests.","title":"Chapter 8: Testing"},{"location":"03-practical/02-testing/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"SdbKs-crm-g\" , width = \"100%\" ) By this point in the book, you should have observed that we have written a number of tests for our data.","title":"Introduction"},{"location":"03-practical/02-testing/#why-test","text":"","title":"Why test?"},{"location":"03-practical/02-testing/#if-you-like-it-put-a-ring-on-it","text":"...and if you rely on it, test it. I am personally a proponent of writing tests for our data because as data scientists, the fields of our data, and their correct values, form the \"data programming interface\" (DPI) much like function signatures form the \"application programming interface\" (API). Since we test the APIs that we rely on, we probably should test the DPIs that we rely on too.","title":"If you like it, put a ring on it..."},{"location":"03-practical/02-testing/#what-to-test","text":"When thinking about what part of the data to test, it can be confusing. After all, data are seemingly generated from random processes (my Bayesian foxtail has been revealed), and it seems difficult to test random processes. That said, from my experience handling data, I can suggest a few principles.","title":"What to test"},{"location":"03-practical/02-testing/#test-invariants","text":"Firstly, we test invariant properties of the data. Put in plain language, things we know ought to be true. Using the Divvy bike dataset example, we know that every node ought to have a station name. Thus, the minimum that we can test is that the station_name attribute is present on every node. As an example: def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): assert \"station_name\" in d . keys ()","title":"Test invariants"},{"location":"03-practical/02-testing/#test-nullity","text":"Secondly, we can test that values that ought not to be null should not be null. Using the Divvy bike dataset example again, if we also know that the station name cannot be null or an empty string, then we can bake that into the test. def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): assert \"station_name\" in d . keys () assert bool ( d [ \"station_name\" ])","title":"Test nullity"},{"location":"03-practical/02-testing/#test-boundaries","text":"We can also test boundary values. For example, within the city of Chicago, we know that latitude and longitude values ought to be within the vicinity of 41.85003, -87.65005 . If we get data values that are, say, outside the range of [41, 42]; [-88, -87] , then we know that we have data issues as well. Here's an example: def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): # Test for station names. assert \"station_name\" in d . keys () assert bool ( d [ \"station_name\" ]) # Test for longitude/latitude assert d [ \"latitude\" ] >= 41 and d [ \"latitude\" ] <= 42 assert d [ \"longitude\" ] >= - 88 and d [ \"longitude\" ] <= - 87 An apology to geospatial experts: I genuinely don't know the bounding box lat/lon coordinates of Chicago, so if you know those coordinates, please reach out so I can update the test.","title":"Test boundaries"},{"location":"03-practical/02-testing/#continuous-data-testing","text":"The key idea with testing is to have tests that continuously run all the time in the background without you ever needing to intervene to kickstart it off. It's like having a bot in the background always running checks for you so you don't have to kickstart them. To do so, you should be equipped with a few tools. I won't go into them in-depth here, as I will be writing a \"continuous data testing\" essay in the near future. That said, here is the gist. Firstly, use pytest to get set up with testing. You essentially write a test_something.py file in which you write your test suite, and your test functions are all nothinng more than simple functions. # test_data.py def test_divvy_nodes ( G ): \"\"\"Test node metadata on Divvy dataset.\"\"\" for n , d in G . nodes ( data = True ): # Test for station names. assert \"station_name\" in d . keys () assert bool ( d [ \"station_name\" ]) # Test for longitude/latitude assert d [ \"latitude\" ] >= 41 and d [ \"latitude\" ] <= 42 assert d [ \"longitude\" ] >= - 88 and d [ \"longitude\" ] <= - 87 At the command line, if you ran pytest , it will automatically discover all functions prefixed with test_ in all .py files underneath the current working directory. Secondly, set up a continuous pipelining system to continuously run data tests. For example, you can set up Jenkins , Travis , Azure Pipelines , Prefect , and more, depending on what your organization has bought into. Sometimes data tests take longer than software tests, especially if you are pulling dumps from a database, so you might want to run this portion of tests in a separate pipeline instead.","title":"Continuous data testing"},{"location":"03-practical/02-testing/#further-reading","text":"In my essays collection, I wrote about testing data . Itamar Turner-Trauring has written about keeping tests quick and speedy , which is extremely crucial to keeping yourself motivated to write tests.","title":"Further reading"},{"location":"04-advanced/01-bipartite/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"BYOK12I9vgI\" , width = \"100%\" ) In this chapter, we will look at bipartite graphs and their applications. What are bipartite graphs? As the name suggests, bipartite have two (bi) node partitions (partite). In other words, we can assign nodes to one of the two partitions. (By contrast, all of the graphs that we have seen before are unipartite : they only have a single partition.) Rules for bipartite graphs With unipartite graphs, you might remember a few rules that apply. Firstly, nodes and edges belong to a set . This means the node set contains only unique members, i.e. no node can be duplicated. The same applies for the edge set. On top of those two basic rules, bipartite graphs add an additional rule: Edges can only occur between nodes of different partitions. In other words, nodes within the same partition are not allowed to be connected to one another. Applications of bipartite graphs Where do we see bipartite graphs being used? Here's one that is very relevant to e-commerce, which touches our daily lives: We can model customer purchases of products using a bipartite graph. Here, the two node sets are customer nodes and product nodes, and edges indicate that a customer C C purchased a product P P . On the basis of this graph, we can do interesting analyses, such as finding customers that are similar to one another on the basis of their shared product purchases. Can you think of other situations where a bipartite graph model can be useful? Dataset Here's another application in crime analysis, which is relevant to the example that we will use in this chapter: This bipartite network contains persons who appeared in at least one crime case as either a suspect, a victim, a witness or both a suspect and victim at the same time. A left node represents a person and a right node represents a crime. An edge between two nodes shows that the left node was involved in the crime represented by the right node. This crime dataset was also sourced from Konect. from nams import load_data as cf G = cf . load_crime_network () for n , d in G . nodes ( data = True ): G . nodes [ n ][ \"degree\" ] = G . degree ( n ) If you inspect the nodes, you will see that they contain a special metadata keyword: bipartite . This is a special keyword that NetworkX can use to identify nodes of a given partition. Visualize the crime network To help us get our bearings right, let's visualize the crime network. import nxviz as nv import matplotlib.pyplot as plt fig , ax = plt . subplots ( figsize = ( 7 , 7 )) nv . circos ( G , sort_by = \"degree\" , group_by = \"bipartite\" , node_color_by = \"bipartite\" , node_enc_kwargs = { \"size_scale\" : 3 }) <AxesSubplot:> Exercise: Extract each node set A useful thing to be able to do is to extract each partition's node set. This will become handy when interacting with NetworkX's bipartite algorithms later on. Write a function that extracts all of the nodes from specified node partition. It should also raise a plain Exception if no nodes exist in that specified partition. (as a precuation against users putting in invalid partition names). import networkx as nx def extract_partition_nodes ( G : nx . Graph , partition : str ): nodeset = [ _ for _ , _ in _______ if ____________ ] if _____________ : raise Exception ( f \"No nodes exist in the partition { partition } !\" ) return nodeset from nams.solutions.bipartite import extract_partition_nodes # Uncomment the next line to see the answer. # extract_partition_nodes?? Bipartite Graph Projections In a bipartite graph, one task that can be useful to do is to calculate the projection of a graph onto one of its nodes. What do we mean by the \"projection of a graph\"? It is best visualized using this figure: from nams.solutions.bipartite import draw_bipartite_graph_example , bipartite_example_graph from nxviz import annotate import matplotlib.pyplot as plt bG = bipartite_example_graph () pG = nx . bipartite . projection . projected_graph ( bG , \"abcd\" ) ax = draw_bipartite_graph_example () plt . sca ( ax [ 0 ]) annotate . parallel_labels ( bG , group_by = \"bipartite\" ) plt . sca ( ax [ 1 ]) annotate . arc_labels ( pG ) As shown in the figure above, we start first with a bipartite graph with two node sets, the \"alphabet\" set and the \"numeric\" set. The projection of this bipartite graph onto the \"alphabet\" node set is a graph that is constructed such that it only contains the \"alphabet\" nodes, and edges join the \"alphabet\" nodes because they share a connection to a \"numeric\" node. The red edge on the right is basically the red path traced on the left. Computing graph projections How does one compute graph projections using NetworkX? Turns out, NetworkX has a bipartite submodule, which gives us all of the facilities that we need to interact with bipartite algorithms. First of all, we need to check that the graph is indeed a bipartite graph. NetworkX provides a function for us to do so: from networkx.algorithms import bipartite bipartite . is_bipartite ( G ) True Now that we've confirmed that the graph is indeed bipartite, we can use the NetworkX bipartite submodule functions to generate the bipartite projection onto one of the node partitions. First off, we need to extract nodes from a particular partition. person_nodes = extract_partition_nodes ( G , \"person\" ) crime_nodes = extract_partition_nodes ( G , \"crime\" ) Next, we can compute the projection: person_graph = bipartite . projected_graph ( G , person_nodes ) crime_graph = bipartite . projected_graph ( G , crime_nodes ) And with that, we have our projected graphs! Go ahead and inspect them: list ( person_graph . edges ( data = True ))[ 0 : 5 ] [('p1', 'p336', {}), ('p1', 'p756', {}), ('p1', 'p93', {}), ('p1', 'p694', {}), ('p2', 'p300', {})] list ( crime_graph . edges ( data = True ))[ 0 : 5 ] [('c1', 'c4', {}), ('c1', 'c2', {}), ('c1', 'c3', {}), ('c2', 'c4', {}), ('c2', 'c3', {})] Now, what is the interpretation of these projected graphs? For person_graph , we have found individuals who are linked by shared participation (whether witness or suspect) in a crime. For crime_graph , we have found crimes that are linked by shared involvement by people. Just by this graph, we already can find out pretty useful information. Let's use an exercise that leverages what you already know to extract useful information from the projected graph. Exercise: find the crime(s) that have the most shared connections with other crimes Find crimes that are most similar to one another on the basis of the number of shared connections to individuals. Hint: This is a degree centrality problem! import pandas as pd def find_most_similar_crimes ( cG : nx . Graph ): \"\"\" Find the crimes that are most similar to other crimes. \"\"\" dcs = ______________ return ___________________ from nams.solutions.bipartite import find_most_similar_crimes find_most_similar_crimes ( crime_graph ) c110 0.136364 c47 0.070909 c23 0.070909 c95 0.063636 c14 0.061818 c352 0.060000 c432 0.060000 c160 0.058182 c417 0.058182 c525 0.058182 dtype: float64 Exercise: find the individual(s) that have the most shared connections with other individuals Now do the analogous thing for individuals! def find_most_similar_people ( pG : nx . Graph ): \"\"\" Find the persons that are most similar to other persons. \"\"\" dcs = ______________ return ___________________ from nams.solutions.bipartite import find_most_similar_people find_most_similar_people ( person_graph ) p425 0.061594 p2 0.057971 p356 0.053140 p56 0.039855 p695 0.039855 p497 0.036232 p715 0.035024 p10 0.033816 p815 0.032609 p74 0.030193 dtype: float64 Weighted Projection Though we were able to find out which graphs were connected with one another, we did not record in the resulting projected graph the strength by which the two nodes were connected. To preserve this information, we need another function: weighted_person_graph = bipartite . weighted_projected_graph ( G , person_nodes ) list ( weighted_person_graph . edges ( data = True ))[ 0 : 5 ] [('p1', 'p336', {'weight': 1}), ('p1', 'p756', {'weight': 1}), ('p1', 'p93', {'weight': 1}), ('p1', 'p694', {'weight': 1}), ('p2', 'p300', {'weight': 1})] Exercise: Find the people that can help with investigating a crime 's person . Let's pretend that we are a detective trying to solve a crime, and that we right now need to find other individuals who were not implicated in the same exact crime as an individual was, but who might be able to give us information about that individual because they were implicated in other crimes with that individual. Implement a function that takes in a bipartite graph G , a string person and a string crime , and returns a list of other person s that were not implicated in the crime , but were connected to the person via other crimes. It should return a ranked list , based on the number of shared crimes (from highest to lowest) because the ranking will help with triage. list ( G . neighbors ( 'p1' )) ['c1', 'c2', 'c3', 'c4'] def find_connected_persons ( G , person , crime ): # Step 0: Check that the given \"person\" and \"crime\" are connected. if _____________________________ : raise ValueError ( f \"Graph does not have a connection between { person } and { crime } !\" ) # Step 1: calculate weighted projection for person nodes. person_nodes = ____________________________________ person_graph = bipartite . ________________________ ( _ , ____________ ) # Step 2: Find neighbors of the given `person` node in projected graph. candidate_neighbors = ___________________________________ # Step 3: Remove candidate neighbors from the set if they are implicated in the given crime. for p in G . neighbors ( crime ): if ________________________ : _____________________________ # Step 4: Rank-order the candidate neighbors by number of shared connections. _________ = [] ## You might need a for-loop here return pd . DataFrame ( __________ ) . sort_values ( \"________\" , ascending = False ) from nams.solutions.bipartite import find_connected_persons find_connected_persons ( G , 'p2' , 'c10' ) node weight 38 p67 4 43 p356 2 5 p361 2 18 p338 2 0 p300 1 34 p90 1 27 p603 1 28 p499 1 29 p528 1 30 p710 1 31 p820 1 32 p665 1 33 p186 1 36 p782 1 35 p48 1 25 p587 1 37 p401 1 39 p495 1 40 p578 1 41 p439 1 42 p806 1 44 p498 1 45 p309 1 26 p5 1 23 p449 1 24 p471 1 1 p716 1 2 p4 1 3 p620 1 4 p223 1 6 p320 1 7 p39 1 8 p768 1 9 p475 1 10 p287 1 11 p304 1 12 p286 1 13 p660 1 14 p690 1 15 p661 1 16 p211 1 17 p608 1 19 p360 1 20 p773 1 21 p305 1 22 p563 1 46 p781 1 Degree Centrality The degree centrality metric is something we can calculate for bipartite graphs. Recall that the degree centrality metric is the number of neighbors of a node divided by the total number of possible neighbors. In a unipartite graph, the denominator can be the total number of nodes less one (if self-loops are not allowed) or simply the total number of nodes (if self loops are allowed). Exercise: What is the denominator for bipartite graphs? Think about it for a moment, then write down your answer. from nams.solutions.bipartite import bipartite_degree_centrality_denominator from nams.functions import render_html render_html ( bipartite_degree_centrality_denominator ()) The total number of neighbors that a node can possibly have is the number of nodes in the other partition. This comes naturally from the definition of a bipartite graph, where nodes can only be connected to nodes in the other partition. Exercise: Which persons are implicated in the most number of crimes? Find the persons (singular or plural) who are connected to the most number of crimes. To do so, you will need to use nx.bipartite.degree_centrality , rather than the regular nx.degree_centrality function. nx.bipartite.degree_centrality requires that you pass in a node set from one of the partitions so that it can correctly partition nodes on the other set. What is returned, though, is the degree centrality for nodes in both sets. Here is an example to show you how the function is used: dcs = nx . bipartite . degree_centrality ( my_graph , nodes_from_one_partition ) def find_most_crime_person ( G , person_nodes ): dcs = __________________________ return ___________________________ from nams.solutions.bipartite import find_most_crime_person find_most_crime_person ( G , person_nodes ) 'p815' Solutions Here are the solutions to the exercises above. from nams.solutions import bipartite import inspect print ( inspect . getsource ( bipartite )) import networkx as nx import pandas as pd from nams.functions import render_html def extract_partition_nodes(G: nx.Graph, partition: str): nodeset = [n for n, d in G.nodes(data=True) if d[\"bipartite\"] == partition] if len(nodeset) == 0: raise Exception(f\"No nodes exist in the partition {partition}!\") return nodeset def bipartite_example_graph(): bG = nx.Graph() bG.add_nodes_from(\"abcd\", bipartite=\"letters\") bG.add_nodes_from(range(1, 4), bipartite=\"numbers\") bG.add_edges_from([(\"a\", 1), (\"b\", 1), (\"b\", 3), (\"c\", 2), (\"c\", 3), (\"d\", 1)]) return bG def draw_bipartite_graph_example(): \"\"\"Draw an example bipartite graph and its corresponding projection.\"\"\" import matplotlib.pyplot as plt import nxviz as nv from nxviz import annotate, plots, highlights fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4)) plt.sca(ax[0]) bG = bipartite_example_graph() nv.parallel(bG, group_by=\"bipartite\", node_color_by=\"bipartite\") annotate.parallel_group(bG, group_by=\"bipartite\", y_offset=-0.5) highlights.parallel_edge(bG, \"a\", 1, group_by=\"bipartite\") highlights.parallel_edge(bG, \"b\", 1, group_by=\"bipartite\") pG = nx.bipartite.projected_graph(bG, nodes=list(\"abcd\")) plt.sca(ax[1]) nv.arc(pG) highlights.arc_edge(pG, \"a\", \"b\") return ax def find_most_similar_crimes(cG: nx.Graph): \"\"\" Find the crimes that are most similar to other crimes. \"\"\" dcs = pd.Series(nx.degree_centrality(cG)) return dcs.sort_values(ascending=False).head(10) def find_most_similar_people(pG: nx.Graph): \"\"\" Find the persons that are most similar to other persons. \"\"\" dcs = pd.Series(nx.degree_centrality(pG)) return dcs.sort_values(ascending=False).head(10) def find_connected_persons(G, person, crime): \"\"\"Answer to exercise on people implicated in crimes\"\"\" # Step 0: Check that the given \"person\" and \"crime\" are connected. if not G.has_edge(person, crime): raise ValueError( f\"Graph does not have a connection between {person} and {crime}!\" ) # Step 1: calculate weighted projection for person nodes. person_nodes = extract_partition_nodes(G, \"person\") person_graph = nx.bipartite.weighted_projected_graph(G, person_nodes) # Step 2: Find neighbors of the given `person` node in projected graph. candidate_neighbors = set(person_graph.neighbors(person)) # Step 3: Remove candidate neighbors from the set if they are implicated in the given crime. for p in G.neighbors(crime): if p in candidate_neighbors: candidate_neighbors.remove(p) # Step 4: Rank-order the candidate neighbors by number of shared connections. data = [] for nbr in candidate_neighbors: data.append(dict(node=nbr, weight=person_graph.edges[person, nbr][\"weight\"])) return pd.DataFrame(data).sort_values(\"weight\", ascending=False) def bipartite_degree_centrality_denominator(): \"\"\"Answer to bipartite graph denominator for degree centrality.\"\"\" ans = \"\"\" The total number of neighbors that a node can _possibly_ have is the number of nodes in the other partition. This comes naturally from the definition of a bipartite graph, where nodes can _only_ be connected to nodes in the other partition. \"\"\" return ans def find_most_crime_person(G, person_nodes): dcs = ( pd.Series(nx.bipartite.degree_centrality(G, person_nodes)) .sort_values(ascending=False) .to_frame() ) return dcs.reset_index().query(\"index.str.contains('p')\").iloc[0][\"index\"]","title":"Chapter 9: Bipartite Graphs"},{"location":"04-advanced/01-bipartite/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"BYOK12I9vgI\" , width = \"100%\" ) In this chapter, we will look at bipartite graphs and their applications.","title":"Introduction"},{"location":"04-advanced/01-bipartite/#what-are-bipartite-graphs","text":"As the name suggests, bipartite have two (bi) node partitions (partite). In other words, we can assign nodes to one of the two partitions. (By contrast, all of the graphs that we have seen before are unipartite : they only have a single partition.)","title":"What are bipartite graphs?"},{"location":"04-advanced/01-bipartite/#rules-for-bipartite-graphs","text":"With unipartite graphs, you might remember a few rules that apply. Firstly, nodes and edges belong to a set . This means the node set contains only unique members, i.e. no node can be duplicated. The same applies for the edge set. On top of those two basic rules, bipartite graphs add an additional rule: Edges can only occur between nodes of different partitions. In other words, nodes within the same partition are not allowed to be connected to one another.","title":"Rules for bipartite graphs"},{"location":"04-advanced/01-bipartite/#applications-of-bipartite-graphs","text":"Where do we see bipartite graphs being used? Here's one that is very relevant to e-commerce, which touches our daily lives: We can model customer purchases of products using a bipartite graph. Here, the two node sets are customer nodes and product nodes, and edges indicate that a customer C C purchased a product P P . On the basis of this graph, we can do interesting analyses, such as finding customers that are similar to one another on the basis of their shared product purchases. Can you think of other situations where a bipartite graph model can be useful?","title":"Applications of bipartite graphs"},{"location":"04-advanced/01-bipartite/#dataset","text":"Here's another application in crime analysis, which is relevant to the example that we will use in this chapter: This bipartite network contains persons who appeared in at least one crime case as either a suspect, a victim, a witness or both a suspect and victim at the same time. A left node represents a person and a right node represents a crime. An edge between two nodes shows that the left node was involved in the crime represented by the right node. This crime dataset was also sourced from Konect. from nams import load_data as cf G = cf . load_crime_network () for n , d in G . nodes ( data = True ): G . nodes [ n ][ \"degree\" ] = G . degree ( n ) If you inspect the nodes, you will see that they contain a special metadata keyword: bipartite . This is a special keyword that NetworkX can use to identify nodes of a given partition.","title":"Dataset"},{"location":"04-advanced/01-bipartite/#visualize-the-crime-network","text":"To help us get our bearings right, let's visualize the crime network. import nxviz as nv import matplotlib.pyplot as plt fig , ax = plt . subplots ( figsize = ( 7 , 7 )) nv . circos ( G , sort_by = \"degree\" , group_by = \"bipartite\" , node_color_by = \"bipartite\" , node_enc_kwargs = { \"size_scale\" : 3 }) <AxesSubplot:>","title":"Visualize the crime network"},{"location":"04-advanced/01-bipartite/#exercise-extract-each-node-set","text":"A useful thing to be able to do is to extract each partition's node set. This will become handy when interacting with NetworkX's bipartite algorithms later on. Write a function that extracts all of the nodes from specified node partition. It should also raise a plain Exception if no nodes exist in that specified partition. (as a precuation against users putting in invalid partition names). import networkx as nx def extract_partition_nodes ( G : nx . Graph , partition : str ): nodeset = [ _ for _ , _ in _______ if ____________ ] if _____________ : raise Exception ( f \"No nodes exist in the partition { partition } !\" ) return nodeset from nams.solutions.bipartite import extract_partition_nodes # Uncomment the next line to see the answer. # extract_partition_nodes??","title":"Exercise: Extract each node set"},{"location":"04-advanced/01-bipartite/#bipartite-graph-projections","text":"In a bipartite graph, one task that can be useful to do is to calculate the projection of a graph onto one of its nodes. What do we mean by the \"projection of a graph\"? It is best visualized using this figure: from nams.solutions.bipartite import draw_bipartite_graph_example , bipartite_example_graph from nxviz import annotate import matplotlib.pyplot as plt bG = bipartite_example_graph () pG = nx . bipartite . projection . projected_graph ( bG , \"abcd\" ) ax = draw_bipartite_graph_example () plt . sca ( ax [ 0 ]) annotate . parallel_labels ( bG , group_by = \"bipartite\" ) plt . sca ( ax [ 1 ]) annotate . arc_labels ( pG ) As shown in the figure above, we start first with a bipartite graph with two node sets, the \"alphabet\" set and the \"numeric\" set. The projection of this bipartite graph onto the \"alphabet\" node set is a graph that is constructed such that it only contains the \"alphabet\" nodes, and edges join the \"alphabet\" nodes because they share a connection to a \"numeric\" node. The red edge on the right is basically the red path traced on the left.","title":"Bipartite Graph Projections"},{"location":"04-advanced/01-bipartite/#computing-graph-projections","text":"How does one compute graph projections using NetworkX? Turns out, NetworkX has a bipartite submodule, which gives us all of the facilities that we need to interact with bipartite algorithms. First of all, we need to check that the graph is indeed a bipartite graph. NetworkX provides a function for us to do so: from networkx.algorithms import bipartite bipartite . is_bipartite ( G ) True Now that we've confirmed that the graph is indeed bipartite, we can use the NetworkX bipartite submodule functions to generate the bipartite projection onto one of the node partitions. First off, we need to extract nodes from a particular partition. person_nodes = extract_partition_nodes ( G , \"person\" ) crime_nodes = extract_partition_nodes ( G , \"crime\" ) Next, we can compute the projection: person_graph = bipartite . projected_graph ( G , person_nodes ) crime_graph = bipartite . projected_graph ( G , crime_nodes ) And with that, we have our projected graphs! Go ahead and inspect them: list ( person_graph . edges ( data = True ))[ 0 : 5 ] [('p1', 'p336', {}), ('p1', 'p756', {}), ('p1', 'p93', {}), ('p1', 'p694', {}), ('p2', 'p300', {})] list ( crime_graph . edges ( data = True ))[ 0 : 5 ] [('c1', 'c4', {}), ('c1', 'c2', {}), ('c1', 'c3', {}), ('c2', 'c4', {}), ('c2', 'c3', {})] Now, what is the interpretation of these projected graphs? For person_graph , we have found individuals who are linked by shared participation (whether witness or suspect) in a crime. For crime_graph , we have found crimes that are linked by shared involvement by people. Just by this graph, we already can find out pretty useful information. Let's use an exercise that leverages what you already know to extract useful information from the projected graph.","title":"Computing graph projections"},{"location":"04-advanced/01-bipartite/#exercise-find-the-crimes-that-have-the-most-shared-connections-with-other-crimes","text":"Find crimes that are most similar to one another on the basis of the number of shared connections to individuals. Hint: This is a degree centrality problem! import pandas as pd def find_most_similar_crimes ( cG : nx . Graph ): \"\"\" Find the crimes that are most similar to other crimes. \"\"\" dcs = ______________ return ___________________ from nams.solutions.bipartite import find_most_similar_crimes find_most_similar_crimes ( crime_graph ) c110 0.136364 c47 0.070909 c23 0.070909 c95 0.063636 c14 0.061818 c352 0.060000 c432 0.060000 c160 0.058182 c417 0.058182 c525 0.058182 dtype: float64","title":"Exercise: find the crime(s) that have the most shared connections with other crimes"},{"location":"04-advanced/01-bipartite/#exercise-find-the-individuals-that-have-the-most-shared-connections-with-other-individuals","text":"Now do the analogous thing for individuals! def find_most_similar_people ( pG : nx . Graph ): \"\"\" Find the persons that are most similar to other persons. \"\"\" dcs = ______________ return ___________________ from nams.solutions.bipartite import find_most_similar_people find_most_similar_people ( person_graph ) p425 0.061594 p2 0.057971 p356 0.053140 p56 0.039855 p695 0.039855 p497 0.036232 p715 0.035024 p10 0.033816 p815 0.032609 p74 0.030193 dtype: float64","title":"Exercise: find the individual(s) that have the most shared connections with other individuals"},{"location":"04-advanced/01-bipartite/#weighted-projection","text":"Though we were able to find out which graphs were connected with one another, we did not record in the resulting projected graph the strength by which the two nodes were connected. To preserve this information, we need another function: weighted_person_graph = bipartite . weighted_projected_graph ( G , person_nodes ) list ( weighted_person_graph . edges ( data = True ))[ 0 : 5 ] [('p1', 'p336', {'weight': 1}), ('p1', 'p756', {'weight': 1}), ('p1', 'p93', {'weight': 1}), ('p1', 'p694', {'weight': 1}), ('p2', 'p300', {'weight': 1})]","title":"Weighted Projection"},{"location":"04-advanced/01-bipartite/#exercise-find-the-people-that-can-help-with-investigating-a-crimes-person","text":"Let's pretend that we are a detective trying to solve a crime, and that we right now need to find other individuals who were not implicated in the same exact crime as an individual was, but who might be able to give us information about that individual because they were implicated in other crimes with that individual. Implement a function that takes in a bipartite graph G , a string person and a string crime , and returns a list of other person s that were not implicated in the crime , but were connected to the person via other crimes. It should return a ranked list , based on the number of shared crimes (from highest to lowest) because the ranking will help with triage. list ( G . neighbors ( 'p1' )) ['c1', 'c2', 'c3', 'c4'] def find_connected_persons ( G , person , crime ): # Step 0: Check that the given \"person\" and \"crime\" are connected. if _____________________________ : raise ValueError ( f \"Graph does not have a connection between { person } and { crime } !\" ) # Step 1: calculate weighted projection for person nodes. person_nodes = ____________________________________ person_graph = bipartite . ________________________ ( _ , ____________ ) # Step 2: Find neighbors of the given `person` node in projected graph. candidate_neighbors = ___________________________________ # Step 3: Remove candidate neighbors from the set if they are implicated in the given crime. for p in G . neighbors ( crime ): if ________________________ : _____________________________ # Step 4: Rank-order the candidate neighbors by number of shared connections. _________ = [] ## You might need a for-loop here return pd . DataFrame ( __________ ) . sort_values ( \"________\" , ascending = False ) from nams.solutions.bipartite import find_connected_persons find_connected_persons ( G , 'p2' , 'c10' ) node weight 38 p67 4 43 p356 2 5 p361 2 18 p338 2 0 p300 1 34 p90 1 27 p603 1 28 p499 1 29 p528 1 30 p710 1 31 p820 1 32 p665 1 33 p186 1 36 p782 1 35 p48 1 25 p587 1 37 p401 1 39 p495 1 40 p578 1 41 p439 1 42 p806 1 44 p498 1 45 p309 1 26 p5 1 23 p449 1 24 p471 1 1 p716 1 2 p4 1 3 p620 1 4 p223 1 6 p320 1 7 p39 1 8 p768 1 9 p475 1 10 p287 1 11 p304 1 12 p286 1 13 p660 1 14 p690 1 15 p661 1 16 p211 1 17 p608 1 19 p360 1 20 p773 1 21 p305 1 22 p563 1 46 p781 1","title":"Exercise: Find the people that can help with investigating a crime's person."},{"location":"04-advanced/01-bipartite/#degree-centrality","text":"The degree centrality metric is something we can calculate for bipartite graphs. Recall that the degree centrality metric is the number of neighbors of a node divided by the total number of possible neighbors. In a unipartite graph, the denominator can be the total number of nodes less one (if self-loops are not allowed) or simply the total number of nodes (if self loops are allowed).","title":"Degree Centrality"},{"location":"04-advanced/01-bipartite/#exercise-what-is-the-denominator-for-bipartite-graphs","text":"Think about it for a moment, then write down your answer. from nams.solutions.bipartite import bipartite_degree_centrality_denominator from nams.functions import render_html render_html ( bipartite_degree_centrality_denominator ()) The total number of neighbors that a node can possibly have is the number of nodes in the other partition. This comes naturally from the definition of a bipartite graph, where nodes can only be connected to nodes in the other partition.","title":"Exercise: What is the denominator for bipartite graphs?"},{"location":"04-advanced/01-bipartite/#exercise-which-persons-are-implicated-in-the-most-number-of-crimes","text":"Find the persons (singular or plural) who are connected to the most number of crimes. To do so, you will need to use nx.bipartite.degree_centrality , rather than the regular nx.degree_centrality function. nx.bipartite.degree_centrality requires that you pass in a node set from one of the partitions so that it can correctly partition nodes on the other set. What is returned, though, is the degree centrality for nodes in both sets. Here is an example to show you how the function is used: dcs = nx . bipartite . degree_centrality ( my_graph , nodes_from_one_partition ) def find_most_crime_person ( G , person_nodes ): dcs = __________________________ return ___________________________ from nams.solutions.bipartite import find_most_crime_person find_most_crime_person ( G , person_nodes ) 'p815'","title":"Exercise: Which persons are implicated in the most number of crimes?"},{"location":"04-advanced/01-bipartite/#solutions","text":"Here are the solutions to the exercises above. from nams.solutions import bipartite import inspect print ( inspect . getsource ( bipartite )) import networkx as nx import pandas as pd from nams.functions import render_html def extract_partition_nodes(G: nx.Graph, partition: str): nodeset = [n for n, d in G.nodes(data=True) if d[\"bipartite\"] == partition] if len(nodeset) == 0: raise Exception(f\"No nodes exist in the partition {partition}!\") return nodeset def bipartite_example_graph(): bG = nx.Graph() bG.add_nodes_from(\"abcd\", bipartite=\"letters\") bG.add_nodes_from(range(1, 4), bipartite=\"numbers\") bG.add_edges_from([(\"a\", 1), (\"b\", 1), (\"b\", 3), (\"c\", 2), (\"c\", 3), (\"d\", 1)]) return bG def draw_bipartite_graph_example(): \"\"\"Draw an example bipartite graph and its corresponding projection.\"\"\" import matplotlib.pyplot as plt import nxviz as nv from nxviz import annotate, plots, highlights fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4)) plt.sca(ax[0]) bG = bipartite_example_graph() nv.parallel(bG, group_by=\"bipartite\", node_color_by=\"bipartite\") annotate.parallel_group(bG, group_by=\"bipartite\", y_offset=-0.5) highlights.parallel_edge(bG, \"a\", 1, group_by=\"bipartite\") highlights.parallel_edge(bG, \"b\", 1, group_by=\"bipartite\") pG = nx.bipartite.projected_graph(bG, nodes=list(\"abcd\")) plt.sca(ax[1]) nv.arc(pG) highlights.arc_edge(pG, \"a\", \"b\") return ax def find_most_similar_crimes(cG: nx.Graph): \"\"\" Find the crimes that are most similar to other crimes. \"\"\" dcs = pd.Series(nx.degree_centrality(cG)) return dcs.sort_values(ascending=False).head(10) def find_most_similar_people(pG: nx.Graph): \"\"\" Find the persons that are most similar to other persons. \"\"\" dcs = pd.Series(nx.degree_centrality(pG)) return dcs.sort_values(ascending=False).head(10) def find_connected_persons(G, person, crime): \"\"\"Answer to exercise on people implicated in crimes\"\"\" # Step 0: Check that the given \"person\" and \"crime\" are connected. if not G.has_edge(person, crime): raise ValueError( f\"Graph does not have a connection between {person} and {crime}!\" ) # Step 1: calculate weighted projection for person nodes. person_nodes = extract_partition_nodes(G, \"person\") person_graph = nx.bipartite.weighted_projected_graph(G, person_nodes) # Step 2: Find neighbors of the given `person` node in projected graph. candidate_neighbors = set(person_graph.neighbors(person)) # Step 3: Remove candidate neighbors from the set if they are implicated in the given crime. for p in G.neighbors(crime): if p in candidate_neighbors: candidate_neighbors.remove(p) # Step 4: Rank-order the candidate neighbors by number of shared connections. data = [] for nbr in candidate_neighbors: data.append(dict(node=nbr, weight=person_graph.edges[person, nbr][\"weight\"])) return pd.DataFrame(data).sort_values(\"weight\", ascending=False) def bipartite_degree_centrality_denominator(): \"\"\"Answer to bipartite graph denominator for degree centrality.\"\"\" ans = \"\"\" The total number of neighbors that a node can _possibly_ have is the number of nodes in the other partition. This comes naturally from the definition of a bipartite graph, where nodes can _only_ be connected to nodes in the other partition. \"\"\" return ans def find_most_crime_person(G, person_nodes): dcs = ( pd.Series(nx.bipartite.degree_centrality(G, person_nodes)) .sort_values(ascending=False) .to_frame() ) return dcs.reset_index().query(\"index.str.contains('p')\").iloc[0][\"index\"]","title":"Solutions"},{"location":"04-advanced/02-linalg/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"uTHihJiRELc\" , width = \"100%\" ) In this chapter, we will look at the relationship between graphs and linear algebra. The deep connection between these two topics is super interesting, and I'd like to show it to you through an exploration of three topics: Path finding Message passing Bipartite projections Preliminaries Before we go deep into the linear algebra piece though, we have to first make sure some ideas are clear. The most important thing that we need when treating graphs in linear algebra form is the adjacency matrix . For example, for four nodes joined in a chain: import networkx as nx nodes = list ( range ( 4 )) G1 = nx . Graph () G1 . add_nodes_from ( nodes ) G1 . add_edges_from ( zip ( nodes , nodes [ 1 :])) we can visualize the graph: nx . draw ( G1 , with_labels = True ) and we can visualize its adjacency matrix: import nxviz as nv m = nv . matrix ( G1 ) and we can obtain the adjacency matrix as a NumPy array: A1 = nx . to_numpy_array ( G1 , nodelist = sorted ( G1 . nodes ())) A1 array([[0., 1., 0., 0.], [1., 0., 1., 0.], [0., 1., 0., 1.], [0., 0., 1., 0.]]) Symmetry Remember that for an undirected graph, the adjacency matrix will be symmetric about the diagonal, while for a directed graph, the adjacency matrix will be asymmetric . Path finding In the Paths chapter, we can use the breadth-first search algorithm to find a shortest path between any two nodes . As it turns out, using adjacency matrices, we can answer a related question, which is how many paths exist of length K between two nodes . To see how, we need to see the relationship between matrix powers and graph path lengths. Let's take the adjacency matrix above, raise it to the second power, and see what it tells us. import numpy as np np . linalg . matrix_power ( A1 , 2 ) array([[1., 0., 1., 0.], [0., 2., 0., 1.], [1., 0., 2., 0.], [0., 1., 0., 1.]]) Exercise: adjacency matrix power? What do you think the values in the adjacency matrix are related to? If studying in a group, discuss with your neighbors; if working on this alone, write down your thoughts. from nams.solutions.linalg import adjacency_matrix_power from nams.functions import render_html render_html ( adjacency_matrix_power ()) The diagonals equal to the degree of each node. The off-diagonals also contain values, which correspond to the number of paths that exist of length 2 between the node on the row axis and the node on the column axis. In fact, the diagonal also takes on the same meaning! For the terminal nodes, there is only 1 path from itself back to itself, while for the middle nodes, there are 2 paths from itself back to itself! Higher matrix powers The semantic meaning of adjacency matrix powers is preserved even if we go to higher powers. For example, if we go to the 3rd matrix power: np . linalg . matrix_power ( A1 , 3 ) array([[0., 2., 0., 1.], [2., 0., 3., 0.], [0., 3., 0., 2.], [1., 0., 2., 0.]]) You should be able to convince yourself that: There's no way to go from a node back to itself in 3 steps, thus explaining the diagonals, and The off-diagonals take on the correct values when you think about them in terms of \"ways to go from one node to another\". With directed graphs? Does the \"number of steps\" interpretation hold with directed graphs? Yes it does! Let's see it in action. G2 = nx . DiGraph () G2 . add_nodes_from ( nodes ) G2 . add_edges_from ( zip ( nodes , nodes [ 1 :])) nx . draw ( G2 , with_labels = True ) Exercise: directed graph matrix power Convince yourself that the resulting adjacency matrix power contains the same semantic meaning as that for an undirected graph, that is, the number of ways to go from \"row\" node to \"column\" node in K steps . (I have provided three different matrix powers for you.) A2 = nx . to_numpy_array ( G2 ) np . linalg . matrix_power ( A2 , 2 ) array([[0., 0., 1., 0.], [0., 0., 0., 1.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . linalg . matrix_power ( A2 , 3 ) array([[0., 0., 0., 1.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . linalg . matrix_power ( A2 , 4 ) array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) Message Passing Let's now dive into the second topic here, that of message passing. To show how message passing works on a graph, let's start with the directed linear chain, as this will make things easier to understand. \"Message\" representation in matrix form Our graph adjacency matrix contains nodes ordered in a particular fashion along the rows and columns. We can also create a \"message\" matrix M M , using the same ordering of nodes along the rows, with columns instead representing a \"message\" that is intended to be \"passed\" from one node to another: M = np . array ([ 1 , 0 , 0 , 0 ]) M array([1, 0, 0, 0]) Notice where the position of the value 1 is - at the first node. If we take M and matrix multiply it against A2, let's see what we get: M @ A2 array([0., 1., 0., 0.]) The message has been passed onto the next node! And if we pass the message one more time: M @ A2 @ A2 array([0., 0., 1., 0.]) Now, the message lies on the 3rd node! We can make an animation to visualize this more clearly. There are comments in the code to explain what's going on! def propagate ( G , msg , n_frames ): \"\"\" Computes the node values based on propagation. Intended to be used before or when being passed into the anim() function (defined below). :param G: A NetworkX Graph. :param msg: The initial state of the message. :returns: A list of 1/0 representing message status at each node. \"\"\" # Initialize a list to store message states at each timestep. msg_states = [] # Set a variable `new_msg` to be the initial message state. new_msg = msg # Get the adjacency matrix of the graph G. A = nx . to_numpy_array ( G ) # Perform message passing at each time step for i in range ( n_frames ): msg_states . append ( new_msg ) new_msg = new_msg @ A # Return the message states. return msg_states from IPython.display import HTML import matplotlib.pyplot as plt from matplotlib import animation def update_func ( step , nodes , colors ): \"\"\" The update function for each animation time step. :param step: Passed in from matplotlib's FuncAnimation. Must be present in the function signature. :param nodes: Returned from nx.draw_networkx_edges(). Is an array of colors. :param colors: A list of pre-computed colors. \"\"\" nodes . set_array ( colors [ step ] . ravel ()) return nodes def anim ( G , initial_state , n_frames = 4 ): \"\"\" Animation function! \"\"\" # First, pre-compute the message passing states over all frames. colors = propagate ( G , initial_state , n_frames ) # Instantiate a figure fig = plt . figure () # Precompute node positions so that they stay fixed over the entire animation pos = nx . kamada_kawai_layout ( G ) # Draw nodes to screen nodes = nx . draw_networkx_nodes ( G , pos = pos , node_color = colors [ 0 ] . ravel (), node_size = 20 ) # Draw edges to screen ax = nx . draw_networkx_edges ( G , pos ) # Finally, return the animation through matplotlib. return animation . FuncAnimation ( fig , update_func , frames = range ( n_frames ), fargs = ( nodes , colors )) # Initialize the message msg = np . zeros ( len ( G2 )) msg [ 0 ] = 1 # Animate the graph with message propagation. HTML ( anim ( G2 , msg , n_frames = 4 ) . to_html5_video ()) Your browser does not support the video tag. Bipartite Graphs & Matrices The section on message passing above assumed unipartite graphs, or at least graphs for which messages can be meaningfully passed between nodes. In this section, we will look at bipartite graphs. Recall from before the definition of a bipartite graph: Nodes are separated into two partitions (hence 'bi'-'partite'). Edges can only occur between nodes of different partitions. Bipartite graphs have a natural matrix representation, known as the biadjacency matrix . Nodes on one partition are the rows, and nodes on the other partition are the columns. NetworkX's bipartite module provides a function for computing the biadjacency matrix of a bipartite graph. Let's start by looking at a toy bipartite graph, a \"customer-product\" purchase record graph, with 4 products and 3 customers. The matrix representation might be as follows: # Rows = customers, columns = products, 1 = customer purchased product, 0 = customer did not purchase product. cp_mat = np . array ([[ 0 , 1 , 0 , 0 ], [ 1 , 0 , 1 , 0 ], [ 1 , 1 , 1 , 1 ]]) From this \"bi-adjacency\" matrix, one can compute the projection onto the customers, matrix multiplying the matrix with its transpose. c_mat = cp_mat @ cp_mat . T # c_mat means \"customer matrix\" c_mat array([[1, 0, 1], [0, 2, 2], [1, 2, 4]]) What we get is the connectivity matrix of the customers, based on shared purchases. The diagonals are the degree of the customers in the original graph, i.e. the number of purchases they originally made, and the off-diagonals are the connectivity matrix, based on shared products. To get the products matrix, we make the transposed matrix the left side of the matrix multiplication. p_mat = cp_mat . T @ cp_mat # p_mat means \"product matrix\" p_mat array([[2, 1, 2, 1], [1, 2, 1, 1], [2, 1, 2, 1], [1, 1, 1, 1]]) You may now try to convince yourself that the diagonals are the number of times a customer purchased that product, and the off-diagonals are the connectivity matrix of the products, weighted by how similar two customers are. Exercises In the following exercises, you will now play with a customer-product graph from Amazon. This dataset was downloaded from UCSD's Julian McAuley's website , and corresponds to the digital music dataset. This is a bipartite graph. The two partitions are: customers : The customers that were doing the reviews. products : The music that was being reviewed. In the original dataset (see the original JSON in the datasets/ directory), they are referred to as: customers : reviewerID products : asin from nams import load_data as cf G_amzn = cf . load_amazon_reviews () 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00<00:00, 71652.23it/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00<00:00, 396801.33it/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00<00:00, 393394.59it/s] Remember that with bipartite graphs, it is useful to obtain nodes from one of the partitions. from nams.solutions.bipartite import extract_partition_nodes customer_nodes = extract_partition_nodes ( G_amzn , \"customer\" ) mat = nx . bipartite . biadjacency_matrix ( G_amzn , row_order = customer_nodes ) You'll notice that this matrix is extremely large! There are 5541 customers and 3568 products, for a total matrix size of 5541 \\times 3568 = 19770288 5541 \\times 3568 = 19770288 , but it is stored in a sparse format because only 64706 elements are filled in. mat <5541x3568 sparse matrix of type '<class 'numpy.int64'>' with 64706 stored elements in Compressed Sparse Row format> Example: finding customers who reviewed the most number of music items. Let's find out which customers reviewed the most number of music items. To do so, you can break the problem into a few steps. First off, we compute the customer projection using matrix operations. customer_mat = mat @ mat . T Next, get the diagonals of the customer-customer matrix. Recall here that in customer_mat , the diagonals correspond to the degree of the customer nodes in the bipartite matrix. SciPy sparse matrices provide a .diagonal() method that returns the diagonal elements. # Get the diagonal. degrees = customer_mat . diagonal () Finally, find the index of the customer that has the highest degree. cust_idx = np . argmax ( degrees ) cust_idx 294 We can verify this independently by sorting the customer nodes by degree. import pandas as pd import janitor # There's some pandas-fu we need to use to get this correct. deg = ( pd . Series ( dict ( nx . degree ( G_amzn , customer_nodes ))) . to_frame () . reset_index () . rename_column ( \"index\" , \"customer\" ) . rename_column ( 0 , \"num_reviews\" ) . sort_values ( 'num_reviews' , ascending = False ) ) deg . head () customer num_reviews 294 A9Q28YTLYREO7 578 86 A3HU0B9XUEVHIM 375 77 A3KJ6JAZPH382D 301 307 A3C6ZCBUNXUT7V 261 218 A8IFUOL8S9BZC 256 Indeed, customer 294 was the one who had the most number of reviews! Example: finding similar customers Let's now also compute which two customers are similar, based on shared reviews. To do so involves the following steps: We construct a sparse matrix consisting of only the diagonals. scipy.sparse.diags(elements) will construct a sparse diagonal matrix based on the elements inside elements . Subtract the diagonals from the customer matrix projection. This yields the customer-customer similarity matrix, which should only consist of the off-diagonal elements of the customer matrix projection. Finally, get the indices where the weight (shared number of between the customers is highest. ( This code is provided for you. ) import scipy.sparse as sp # Construct diagonal elements. customer_diags = sp . diags ( degrees ) # Subtract off-diagonals. off_diagonals = customer_mat - customer_diags # Compute index of most similar individuals. np . unravel_index ( np . argmax ( off_diagonals ), customer_mat . shape ) (294, 86) Performance: Object vs. Matrices Finally, to motivate why you might want to use matrices rather than graph objects to compute some of these statistics, let's time the two ways of getting to the same answer. Objects Let's first use NetworkX's built-in machinery to find customers that are most similar. from time import time start = time () # Compute the projection G_cust = nx . bipartite . weighted_projected_graph ( G_amzn , customer_nodes ) # Identify the most similar customers most_similar_customers = sorted ( G_cust . edges ( data = True ), key = lambda x : x [ 2 ][ 'weight' ], reverse = True )[ 0 ] end = time () print ( f ' { end - start : .3f } seconds' ) print ( f 'Most similar customers: { most_similar_customers } ' ) 17.697 seconds Most similar customers: ('A3HU0B9XUEVHIM', 'A9Q28YTLYREO7', {'weight': 154}) Matrices Now, let's implement the same thing in matrix form. start = time () # Compute the projection using matrices mat = nx . bipartite . matrix . biadjacency_matrix ( G_amzn , customer_nodes ) cust_mat = mat @ mat . T # Identify the most similar customers degrees = customer_mat . diagonal () customer_diags = sp . diags ( degrees ) off_diagonals = customer_mat - customer_diags c1 , c2 = np . unravel_index ( np . argmax ( off_diagonals ), customer_mat . shape ) end = time () print ( f ' { end - start : .3f } seconds' ) print ( f 'Most similar customers: { customer_nodes [ c1 ] } , { customer_nodes [ c2 ] } , { cust_mat [ c1 , c2 ] } ' ) 0.493 seconds Most similar customers: A9Q28YTLYREO7, A3HU0B9XUEVHIM, 154 On a modern PC, the matrix computation should be about 10-50X faster using the matrix form compared to the object-oriented form. (The web server that is used to build the book might not necessarily have the software stack to do this though, so the time you see reported might not reflect the expected speedups.) I'd encourage you to fire up a Binder session or clone the book locally to test out the code yourself. You may notice that it's much easier to read the \"objects\" code, but the matrix code way outperforms the object code. This tradeoff is common in computing, and shouldn't surprise you. That said, the speed gain alone is a great reason to use matrices! Acceleration on a GPU If your appetite has been whipped up for even more acceleration and you have a GPU on your daily compute, then you're very much in luck! The RAPIDS.AI project has a package called cuGraph , which provides GPU-accelerated graph algorithms. As over release 0.16.0, all cuGraph algorithms will be able to accept NetworkX graph objects! This came about through online conversations on GitHub and Twitter, which for us, personally, speaks volumes to the power of open source projects! Because cuGraph does presume that you have access to a GPU, and because we assume most readers of this book might not have access to one easily, we'll delegate teaching how to install and use cuGraph to the cuGraph devs and their documentation . Nonetheless, if you do have the ability to install and use the RAPIDS stack, definitely check it out!","title":"Chapter 10: Linear Algebra"},{"location":"04-advanced/02-linalg/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"uTHihJiRELc\" , width = \"100%\" ) In this chapter, we will look at the relationship between graphs and linear algebra. The deep connection between these two topics is super interesting, and I'd like to show it to you through an exploration of three topics: Path finding Message passing Bipartite projections","title":"Introduction"},{"location":"04-advanced/02-linalg/#preliminaries","text":"Before we go deep into the linear algebra piece though, we have to first make sure some ideas are clear. The most important thing that we need when treating graphs in linear algebra form is the adjacency matrix . For example, for four nodes joined in a chain: import networkx as nx nodes = list ( range ( 4 )) G1 = nx . Graph () G1 . add_nodes_from ( nodes ) G1 . add_edges_from ( zip ( nodes , nodes [ 1 :])) we can visualize the graph: nx . draw ( G1 , with_labels = True ) and we can visualize its adjacency matrix: import nxviz as nv m = nv . matrix ( G1 ) and we can obtain the adjacency matrix as a NumPy array: A1 = nx . to_numpy_array ( G1 , nodelist = sorted ( G1 . nodes ())) A1 array([[0., 1., 0., 0.], [1., 0., 1., 0.], [0., 1., 0., 1.], [0., 0., 1., 0.]])","title":"Preliminaries"},{"location":"04-advanced/02-linalg/#symmetry","text":"Remember that for an undirected graph, the adjacency matrix will be symmetric about the diagonal, while for a directed graph, the adjacency matrix will be asymmetric .","title":"Symmetry"},{"location":"04-advanced/02-linalg/#path-finding","text":"In the Paths chapter, we can use the breadth-first search algorithm to find a shortest path between any two nodes . As it turns out, using adjacency matrices, we can answer a related question, which is how many paths exist of length K between two nodes . To see how, we need to see the relationship between matrix powers and graph path lengths. Let's take the adjacency matrix above, raise it to the second power, and see what it tells us. import numpy as np np . linalg . matrix_power ( A1 , 2 ) array([[1., 0., 1., 0.], [0., 2., 0., 1.], [1., 0., 2., 0.], [0., 1., 0., 1.]])","title":"Path finding"},{"location":"04-advanced/02-linalg/#exercise-adjacency-matrix-power","text":"What do you think the values in the adjacency matrix are related to? If studying in a group, discuss with your neighbors; if working on this alone, write down your thoughts. from nams.solutions.linalg import adjacency_matrix_power from nams.functions import render_html render_html ( adjacency_matrix_power ()) The diagonals equal to the degree of each node. The off-diagonals also contain values, which correspond to the number of paths that exist of length 2 between the node on the row axis and the node on the column axis. In fact, the diagonal also takes on the same meaning! For the terminal nodes, there is only 1 path from itself back to itself, while for the middle nodes, there are 2 paths from itself back to itself!","title":"Exercise: adjacency matrix power?"},{"location":"04-advanced/02-linalg/#higher-matrix-powers","text":"The semantic meaning of adjacency matrix powers is preserved even if we go to higher powers. For example, if we go to the 3rd matrix power: np . linalg . matrix_power ( A1 , 3 ) array([[0., 2., 0., 1.], [2., 0., 3., 0.], [0., 3., 0., 2.], [1., 0., 2., 0.]]) You should be able to convince yourself that: There's no way to go from a node back to itself in 3 steps, thus explaining the diagonals, and The off-diagonals take on the correct values when you think about them in terms of \"ways to go from one node to another\".","title":"Higher matrix powers"},{"location":"04-advanced/02-linalg/#with-directed-graphs","text":"Does the \"number of steps\" interpretation hold with directed graphs? Yes it does! Let's see it in action. G2 = nx . DiGraph () G2 . add_nodes_from ( nodes ) G2 . add_edges_from ( zip ( nodes , nodes [ 1 :])) nx . draw ( G2 , with_labels = True )","title":"With directed graphs?"},{"location":"04-advanced/02-linalg/#exercise-directed-graph-matrix-power","text":"Convince yourself that the resulting adjacency matrix power contains the same semantic meaning as that for an undirected graph, that is, the number of ways to go from \"row\" node to \"column\" node in K steps . (I have provided three different matrix powers for you.) A2 = nx . to_numpy_array ( G2 ) np . linalg . matrix_power ( A2 , 2 ) array([[0., 0., 1., 0.], [0., 0., 0., 1.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . linalg . matrix_power ( A2 , 3 ) array([[0., 0., 0., 1.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) np . linalg . matrix_power ( A2 , 4 ) array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]])","title":"Exercise: directed graph matrix power"},{"location":"04-advanced/02-linalg/#message-passing","text":"Let's now dive into the second topic here, that of message passing. To show how message passing works on a graph, let's start with the directed linear chain, as this will make things easier to understand.","title":"Message Passing"},{"location":"04-advanced/02-linalg/#message-representation-in-matrix-form","text":"Our graph adjacency matrix contains nodes ordered in a particular fashion along the rows and columns. We can also create a \"message\" matrix M M , using the same ordering of nodes along the rows, with columns instead representing a \"message\" that is intended to be \"passed\" from one node to another: M = np . array ([ 1 , 0 , 0 , 0 ]) M array([1, 0, 0, 0]) Notice where the position of the value 1 is - at the first node. If we take M and matrix multiply it against A2, let's see what we get: M @ A2 array([0., 1., 0., 0.]) The message has been passed onto the next node! And if we pass the message one more time: M @ A2 @ A2 array([0., 0., 1., 0.]) Now, the message lies on the 3rd node! We can make an animation to visualize this more clearly. There are comments in the code to explain what's going on! def propagate ( G , msg , n_frames ): \"\"\" Computes the node values based on propagation. Intended to be used before or when being passed into the anim() function (defined below). :param G: A NetworkX Graph. :param msg: The initial state of the message. :returns: A list of 1/0 representing message status at each node. \"\"\" # Initialize a list to store message states at each timestep. msg_states = [] # Set a variable `new_msg` to be the initial message state. new_msg = msg # Get the adjacency matrix of the graph G. A = nx . to_numpy_array ( G ) # Perform message passing at each time step for i in range ( n_frames ): msg_states . append ( new_msg ) new_msg = new_msg @ A # Return the message states. return msg_states from IPython.display import HTML import matplotlib.pyplot as plt from matplotlib import animation def update_func ( step , nodes , colors ): \"\"\" The update function for each animation time step. :param step: Passed in from matplotlib's FuncAnimation. Must be present in the function signature. :param nodes: Returned from nx.draw_networkx_edges(). Is an array of colors. :param colors: A list of pre-computed colors. \"\"\" nodes . set_array ( colors [ step ] . ravel ()) return nodes def anim ( G , initial_state , n_frames = 4 ): \"\"\" Animation function! \"\"\" # First, pre-compute the message passing states over all frames. colors = propagate ( G , initial_state , n_frames ) # Instantiate a figure fig = plt . figure () # Precompute node positions so that they stay fixed over the entire animation pos = nx . kamada_kawai_layout ( G ) # Draw nodes to screen nodes = nx . draw_networkx_nodes ( G , pos = pos , node_color = colors [ 0 ] . ravel (), node_size = 20 ) # Draw edges to screen ax = nx . draw_networkx_edges ( G , pos ) # Finally, return the animation through matplotlib. return animation . FuncAnimation ( fig , update_func , frames = range ( n_frames ), fargs = ( nodes , colors )) # Initialize the message msg = np . zeros ( len ( G2 )) msg [ 0 ] = 1 # Animate the graph with message propagation. HTML ( anim ( G2 , msg , n_frames = 4 ) . to_html5_video ()) Your browser does not support the video tag.","title":"\"Message\" representation in matrix form"},{"location":"04-advanced/02-linalg/#bipartite-graphs-matrices","text":"The section on message passing above assumed unipartite graphs, or at least graphs for which messages can be meaningfully passed between nodes. In this section, we will look at bipartite graphs. Recall from before the definition of a bipartite graph: Nodes are separated into two partitions (hence 'bi'-'partite'). Edges can only occur between nodes of different partitions. Bipartite graphs have a natural matrix representation, known as the biadjacency matrix . Nodes on one partition are the rows, and nodes on the other partition are the columns. NetworkX's bipartite module provides a function for computing the biadjacency matrix of a bipartite graph. Let's start by looking at a toy bipartite graph, a \"customer-product\" purchase record graph, with 4 products and 3 customers. The matrix representation might be as follows: # Rows = customers, columns = products, 1 = customer purchased product, 0 = customer did not purchase product. cp_mat = np . array ([[ 0 , 1 , 0 , 0 ], [ 1 , 0 , 1 , 0 ], [ 1 , 1 , 1 , 1 ]]) From this \"bi-adjacency\" matrix, one can compute the projection onto the customers, matrix multiplying the matrix with its transpose. c_mat = cp_mat @ cp_mat . T # c_mat means \"customer matrix\" c_mat array([[1, 0, 1], [0, 2, 2], [1, 2, 4]]) What we get is the connectivity matrix of the customers, based on shared purchases. The diagonals are the degree of the customers in the original graph, i.e. the number of purchases they originally made, and the off-diagonals are the connectivity matrix, based on shared products. To get the products matrix, we make the transposed matrix the left side of the matrix multiplication. p_mat = cp_mat . T @ cp_mat # p_mat means \"product matrix\" p_mat array([[2, 1, 2, 1], [1, 2, 1, 1], [2, 1, 2, 1], [1, 1, 1, 1]]) You may now try to convince yourself that the diagonals are the number of times a customer purchased that product, and the off-diagonals are the connectivity matrix of the products, weighted by how similar two customers are.","title":"Bipartite Graphs &amp; Matrices"},{"location":"04-advanced/02-linalg/#exercises","text":"In the following exercises, you will now play with a customer-product graph from Amazon. This dataset was downloaded from UCSD's Julian McAuley's website , and corresponds to the digital music dataset. This is a bipartite graph. The two partitions are: customers : The customers that were doing the reviews. products : The music that was being reviewed. In the original dataset (see the original JSON in the datasets/ directory), they are referred to as: customers : reviewerID products : asin from nams import load_data as cf G_amzn = cf . load_amazon_reviews () 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00<00:00, 71652.23it/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00<00:00, 396801.33it/s] 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64706/64706 [00:00<00:00, 393394.59it/s] Remember that with bipartite graphs, it is useful to obtain nodes from one of the partitions. from nams.solutions.bipartite import extract_partition_nodes customer_nodes = extract_partition_nodes ( G_amzn , \"customer\" ) mat = nx . bipartite . biadjacency_matrix ( G_amzn , row_order = customer_nodes ) You'll notice that this matrix is extremely large! There are 5541 customers and 3568 products, for a total matrix size of 5541 \\times 3568 = 19770288 5541 \\times 3568 = 19770288 , but it is stored in a sparse format because only 64706 elements are filled in. mat <5541x3568 sparse matrix of type '<class 'numpy.int64'>' with 64706 stored elements in Compressed Sparse Row format>","title":"Exercises"},{"location":"04-advanced/02-linalg/#example-finding-customers-who-reviewed-the-most-number-of-music-items","text":"Let's find out which customers reviewed the most number of music items. To do so, you can break the problem into a few steps. First off, we compute the customer projection using matrix operations. customer_mat = mat @ mat . T Next, get the diagonals of the customer-customer matrix. Recall here that in customer_mat , the diagonals correspond to the degree of the customer nodes in the bipartite matrix. SciPy sparse matrices provide a .diagonal() method that returns the diagonal elements. # Get the diagonal. degrees = customer_mat . diagonal () Finally, find the index of the customer that has the highest degree. cust_idx = np . argmax ( degrees ) cust_idx 294 We can verify this independently by sorting the customer nodes by degree. import pandas as pd import janitor # There's some pandas-fu we need to use to get this correct. deg = ( pd . Series ( dict ( nx . degree ( G_amzn , customer_nodes ))) . to_frame () . reset_index () . rename_column ( \"index\" , \"customer\" ) . rename_column ( 0 , \"num_reviews\" ) . sort_values ( 'num_reviews' , ascending = False ) ) deg . head () customer num_reviews 294 A9Q28YTLYREO7 578 86 A3HU0B9XUEVHIM 375 77 A3KJ6JAZPH382D 301 307 A3C6ZCBUNXUT7V 261 218 A8IFUOL8S9BZC 256 Indeed, customer 294 was the one who had the most number of reviews!","title":"Example: finding customers who reviewed the most number of music items."},{"location":"04-advanced/02-linalg/#example-finding-similar-customers","text":"Let's now also compute which two customers are similar, based on shared reviews. To do so involves the following steps: We construct a sparse matrix consisting of only the diagonals. scipy.sparse.diags(elements) will construct a sparse diagonal matrix based on the elements inside elements . Subtract the diagonals from the customer matrix projection. This yields the customer-customer similarity matrix, which should only consist of the off-diagonal elements of the customer matrix projection. Finally, get the indices where the weight (shared number of between the customers is highest. ( This code is provided for you. ) import scipy.sparse as sp # Construct diagonal elements. customer_diags = sp . diags ( degrees ) # Subtract off-diagonals. off_diagonals = customer_mat - customer_diags # Compute index of most similar individuals. np . unravel_index ( np . argmax ( off_diagonals ), customer_mat . shape ) (294, 86)","title":"Example: finding similar customers"},{"location":"04-advanced/02-linalg/#performance-object-vs-matrices","text":"Finally, to motivate why you might want to use matrices rather than graph objects to compute some of these statistics, let's time the two ways of getting to the same answer.","title":"Performance: Object vs. Matrices"},{"location":"04-advanced/02-linalg/#objects","text":"Let's first use NetworkX's built-in machinery to find customers that are most similar. from time import time start = time () # Compute the projection G_cust = nx . bipartite . weighted_projected_graph ( G_amzn , customer_nodes ) # Identify the most similar customers most_similar_customers = sorted ( G_cust . edges ( data = True ), key = lambda x : x [ 2 ][ 'weight' ], reverse = True )[ 0 ] end = time () print ( f ' { end - start : .3f } seconds' ) print ( f 'Most similar customers: { most_similar_customers } ' ) 17.697 seconds Most similar customers: ('A3HU0B9XUEVHIM', 'A9Q28YTLYREO7', {'weight': 154})","title":"Objects"},{"location":"04-advanced/02-linalg/#matrices","text":"Now, let's implement the same thing in matrix form. start = time () # Compute the projection using matrices mat = nx . bipartite . matrix . biadjacency_matrix ( G_amzn , customer_nodes ) cust_mat = mat @ mat . T # Identify the most similar customers degrees = customer_mat . diagonal () customer_diags = sp . diags ( degrees ) off_diagonals = customer_mat - customer_diags c1 , c2 = np . unravel_index ( np . argmax ( off_diagonals ), customer_mat . shape ) end = time () print ( f ' { end - start : .3f } seconds' ) print ( f 'Most similar customers: { customer_nodes [ c1 ] } , { customer_nodes [ c2 ] } , { cust_mat [ c1 , c2 ] } ' ) 0.493 seconds Most similar customers: A9Q28YTLYREO7, A3HU0B9XUEVHIM, 154 On a modern PC, the matrix computation should be about 10-50X faster using the matrix form compared to the object-oriented form. (The web server that is used to build the book might not necessarily have the software stack to do this though, so the time you see reported might not reflect the expected speedups.) I'd encourage you to fire up a Binder session or clone the book locally to test out the code yourself. You may notice that it's much easier to read the \"objects\" code, but the matrix code way outperforms the object code. This tradeoff is common in computing, and shouldn't surprise you. That said, the speed gain alone is a great reason to use matrices!","title":"Matrices"},{"location":"04-advanced/02-linalg/#acceleration-on-a-gpu","text":"If your appetite has been whipped up for even more acceleration and you have a GPU on your daily compute, then you're very much in luck! The RAPIDS.AI project has a package called cuGraph , which provides GPU-accelerated graph algorithms. As over release 0.16.0, all cuGraph algorithms will be able to accept NetworkX graph objects! This came about through online conversations on GitHub and Twitter, which for us, personally, speaks volumes to the power of open source projects! Because cuGraph does presume that you have access to a GPU, and because we assume most readers of this book might not have access to one easily, we'll delegate teaching how to install and use cuGraph to the cuGraph devs and their documentation . Nonetheless, if you do have the ability to install and use the RAPIDS stack, definitely check it out!","title":"Acceleration on a GPU"},{"location":"04-advanced/03-stats/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import warnings warnings . filterwarnings ( 'ignore' ) Introduction from IPython.display import YouTubeVideo YouTubeVideo ( id = \"P-0CJpO3spg\" , width = \"100%\" ) In this chapter, we are going to take a look at how to perform statistical inference on graphs. Statistics refresher Before we can proceed with statistical inference on graphs, we must first refresh ourselves with some ideas from the world of statistics. Otherwise, the methods that we will end up using may seem a tad weird , and hence difficult to follow along. To review statistical ideas, let's set up a few statements and explore what they mean. We are concerned with models of randomness As with all things statistics, we are concerned with models of randomness. Here, probability distributions give us a way to think about random events and how to assign credibility points to them. In an abstract fashion... The supremely abstract way of thinking about a probability distribution is that it is the space of all possibilities of \"stuff\" with different credibility points distributed amongst each possible \"thing\". More concretely: the coin flip A more concrete example is to consider the coin flip. Here, the space of all possibilities of \"stuff\" is the set of \"heads\" and \"tails\". If we have a fair coin, then we have 0.5 credibility points distributed to each of \"heads\" and \"tails\". Another example: dice rolls Another concrete example is to consider the six-sided dice. Here, the space of all possibilities of \"stuff\" is the set of numbers in the range [1, 6] [1, 6] . If we have a fair dice, then we have 1/6 credibility points assigned to each of the numbers. (Unfair dice will have an unequal distribution of credibility points across each face.) A graph-based example: social networks If we receive an undirected social network graph with 5 nodes and 6 edges, we have to keep in mind that this graph with 6 edges was merely one of 15 \\choose 6 15 \\choose 6 ways to construct 5 node, 6 edge graphs. (15 comes up because there are 15 edges that can be constructed in a 5-node undirected graph.) Hypothesis Testing A commonplace task in statistical inferences is calculating the probability of observing a value or something more extreme under an assumed \"null\" model of reality. This is what we commonly call \"hypothesis testing\", and where the oft-misunderstood term \"p-value\" shows up. Hypothesis testing in coin flips, by simulation As an example, hypothesis testing in coin flips follows this logic: I observe that 8 out of 10 coin tosses give me heads, giving me a probability of heads p=0.8 p=0.8 (a summary statistic). Under a \"null distribution\" of a fair coin, I simulate the distribution of probability of heads (the summary statistic) that I would get from 10 coin tosses. Finally, I use that distribution to calculate the probability of observing p=0.8 p=0.8 or more extreme. Hypothesis testing in graphs The same protocol applies when we perform hypothesis testing on graphs. Firstly, we calculate a summary statistic that describes our graph. Secondly, we propose a null graph model , and calculate our summary statistic under simulated versions of that null graph model. Thirdly, we look at the probability of observing the summary statistic value that we calculated in step 1 or more extreme, under the assumed graph null model distribution. Stochastic graph creation models Since we are going to be dealing with models of randomness in graphs, let's take a look at some examples. Erdos-Renyi (a.k.a. \"binomial\") graph On easy one to study is the Erdos-Renyi graph, also known as the \"binomial\" graph. The data generation story here is that we instantiate an undirected graph with n n nodes, giving \\frac{n^2 - n}{2} \\frac{n^2 - n}{2} possible edges. Each edge has a probability p p of being created. import networkx as nx G_er = nx . erdos_renyi_graph ( n = 30 , p = 0.2 ) nx . draw ( G_er ) You can verify that there's approximately 20% of \\frac{30^2 - 30}{2} = 435 \\frac{30^2 - 30}{2} = 435 . len ( G_er . edges ()) 87 len ( G_er . edges ()) / 435 0.2 We can also look at the degree distribution: import pandas as pd from nams.functions import ecdf import matplotlib.pyplot as plt x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_er )))) plt . scatter ( x , y ) <matplotlib.collections.PathCollection at 0x7febd4b61730> Barabasi-Albert Graph The data generating story of this graph generator is essentially that nodes that have lots of edges preferentially get new edges attached onto them. This is what we call a \"preferential attachment\" process. G_ba = nx . barabasi_albert_graph ( n = 30 , m = 3 ) nx . draw ( G_ba ) len ( G_ba . edges ()) 81 And the degree distribution: x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_ba )))) plt . scatter ( x , y ) <matplotlib.collections.PathCollection at 0x7febd4abf8e0> You can see that even though the number of edges between the two graphs are similar, their degree distribution is wildly different. Load Data For this notebook, we are going to look at a protein-protein interaction network, and test the hypothesis that this network was not generated by the data generating process described by an Erdos-Renyi graph. Let's load a protein-protein interaction network dataset . This undirected network contains protein interactions contained in yeast. Research showed that proteins with a high degree were more important for the surivial of the yeast than others. A node represents a protein and an edge represents a metabolic interaction between two proteins. The network contains loops. from nams import load_data as cf G = cf . load_propro_network () for n , d in G . nodes ( data = True ): G . nodes [ n ][ \"degree\" ] = G . degree ( n ) As is always the case, let's make sure we know some basic stats of the graph. len ( G . nodes ()) 1870 len ( G . edges ()) 2277 Let's also examine the degree distribution of the graph. x , y = ecdf ( pd . Series ( dict ( nx . degree ( G )))) plt . scatter ( x , y ) <matplotlib.collections.PathCollection at 0x7febd46067c0> Finally, we should visualize the graph to get a feel for it. import nxviz as nv from nxviz import annotate nv . circos ( G , sort_by = \"degree\" , node_color_by = \"degree\" , node_enc_kwargs = { \"size_scale\" : 10 }) annotate . node_colormapping ( G , color_by = \"degree\" ) One thing we might infer from this visualization is that the vast majority of nodes have a very small degree, while a very small number of nodes have a high degree. That would prompt us to think: what process could be responsible for generating this graph? Inferring Graph Generating Model Given a graph dataset, how do we identify which data generating model provides the best fit? One way to do this is to compare characteristics of a graph generating model against the characteristics of the graph. The logic here is that if we have a good graph generating model for the data, we should, in theory, observe the observed graph's characteristics in the graphs generated by the graph generating model. Comparison of degree distribution Let's compare the degree distribution between the data, a few Erdos-Renyi graphs, and a few Barabasi-Albert graphs. Comparison with Barabasi-Albert graphs from ipywidgets import interact , IntSlider m = IntSlider ( value = 2 , min = 1 , max = 10 ) @interact ( m = m ) def compare_barabasi_albert_graph ( m ): fig , ax = plt . subplots () G_ba = nx . barabasi_albert_graph ( n = len ( G . nodes ()), m = m ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_ba )))) ax . scatter ( x , y , label = \"Barabasi-Albert Graph\" ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G )))) ax . scatter ( x , y , label = \"Protein Interaction Network\" ) ax . legend () var element = $('#f60dacd8-f18a-4f8f-9008-309b2c8911b0'); Comparison with Erdos-Renyi graphs from ipywidgets import FloatSlider p = FloatSlider ( value = 0.6 , min = 0 , max = 0.1 , step = 0.001 ) @interact ( p = p ) def compare_erdos_renyi_graph ( p ): fig , ax = plt . subplots () G_er = nx . erdos_renyi_graph ( n = len ( G . nodes ()), p = p ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_er )))) ax . scatter ( x , y , label = \"Erdos-Renyi Graph\" ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G )))) ax . scatter ( x , y , label = \"Protein Interaction Network\" ) ax . legend () ax . set_title ( f \"p= { p } \" ) var element = $('#76794d69-d84f-45b3-ae04-bb9068f07d7f'); Given the degree distribution only, which model do you think better describes the generation of a protein-protein interaction network? Quantitative Model Comparison Each time we plug in a value of m m for the Barabasi-Albert graph model, we are using one of many possible Barabasi-Albert graph models, each with a different m m . Similarly, each time we choose a different p p for the Erdos-Renyi model, we are using one of many possible Erdos-Renyi graph models, each with a different p p . To quantitatively compare degree distributions, we can use the Wasserstein distance between the data. Let's see how to implement this. from scipy.stats import wasserstein_distance def erdos_renyi_degdist ( n , p ): \"\"\"Return a Pandas series of degree distribution of an Erdos-Renyi graph.\"\"\" G = nx . erdos_renyi_graph ( n = n , p = p ) return pd . Series ( dict ( nx . degree ( G ))) def barabasi_albert_degdist ( n , m ): \"\"\"Return a Pandas series of degree distribution of an Barabasi-Albert graph.\"\"\" G = nx . barabasi_albert_graph ( n = n , m = m ) return pd . Series ( dict ( nx . degree ( G ))) deg = pd . Series ( dict ( nx . degree ( G ))) er_deg = erdos_renyi_degdist ( n = len ( G . nodes ()), p = 0.001 ) ba_deg = barabasi_albert_degdist ( n = len ( G . nodes ()), m = 1 ) wasserstein_distance ( deg , er_deg ), wasserstein_distance ( deg , ba_deg ) (0.8021390374331547, 0.4909090909090922) Notice that because the graphs are instantiated in a non-deterministic fashion, re-running the cell above will give you different values for each new graph generated. Let's now plot the wasserstein distance to our graph data for the two particular Erdos-Renyi and Barabasi-Albert graph models shown above. import matplotlib.pyplot as plt from tqdm.autonotebook import tqdm er_dist = [] ba_dist = [] for _ in tqdm ( range ( 100 )): er_deg = erdos_renyi_degdist ( n = len ( G . nodes ()), p = 0.001 ) er_dist . append ( wasserstein_distance ( deg , er_deg )) ba_deg = barabasi_albert_degdist ( n = len ( G . nodes ()), m = 1 ) ba_dist . append ( wasserstein_distance ( deg , ba_deg )) # er_degs = [erdos_renyi_degdist(n=len(G.nodes()), p=0.001) for _ in range(100)] var element = $('#7914d23e-a2ea-43e1-8ad8-4d381e3f66a0'); import seaborn as sns import janitor data = ( pd . DataFrame ( { \"Erdos-Renyi\" : er_dist , \"Barabasi-Albert\" : ba_dist , } ) . melt ( value_vars = [ \"Erdos-Renyi\" , \"Barabasi-Albert\" ]) . rename_columns ({ \"variable\" : \"Graph Model\" , \"value\" : \"Wasserstein Distance\" }) ) sns . swarmplot ( data = data , x = \"Graph Model\" , y = \"Wasserstein Distance\" ) <AxesSubplot:xlabel='Graph Model', ylabel='Wasserstein Distance'> From this, we might conclude that the Barabasi-Albert graph with m=1 m=1 has the better fit to the protein-protein interaction network graph. Interpretation That statement, accurate as it might be, still does not connect the dots to biology . Let's think about the generative model for this graph. The Barabasi-Albert graph gives us a model for \"rich gets richer\". Given the current state of the graph, if we want to add a new edge, we first pick a node with probability proportional to the number of edges it already has. Then, we pick another node with probability proportional to the number of edges that it has too. Finally, we add an edge there. This has the effect of \"enriching\" nodes that have a large number of edges with more edges. How might this connect to biology? We can't necessarily provide a concrete answer, but this model might help raise new hypotheses. For example, if protein-protein interactions of the \"binding\" kind are driven by subdomains, then proteins that acquire a domain through recombination may end up being able to bind to everything else that the domain was able to. In this fashion, proteins with that particular binding domain gain new edges more readily. Testing these hypotheses would be a totally different matter, and at this point, I submit the above hypothesis with a large amount of salt thrown over my shoulder. In other words, the hypothesized mechanism could be completely wrong. However, I hope that this example illustrated that the usage of a \"graph generative model\" can help us narrow down hypotheses about the observed world. {\"state\": {\"ee1cfab561fb4e438a15ed5bef650627\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ab860990423b4ac99f0a6fa603955771\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"855502b204204fc4957f292184981bd0\": {\"model_name\": \"IntSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"IntSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"IntSliderView\", \"continuous_update\": true, \"description\": \"m\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_ee1cfab561fb4e438a15ed5bef650627\", \"max\": 10, \"min\": 1, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \"d\", \"step\": 1, \"style\": \"IPY_MODEL_ab860990423b4ac99f0a6fa603955771\", \"value\": 2}}, \"0d1561d6d3a04a7ea2217da812b9c9de\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"02e841e8daa74d5ab2bdf03104ecb99f\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_855502b204204fc4957f292184981bd0\", \"IPY_MODEL_ab07dff637eb4e729e54fab85711c44e\"], \"layout\": \"IPY_MODEL_0d1561d6d3a04a7ea2217da812b9c9de\"}}, \"137eb8107f7d40e78563e5b22f5d9234\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ab07dff637eb4e729e54fab85711c44e\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_137eb8107f7d40e78563e5b22f5d9234\", \"msg_id\": \"\", \"outputs\": []}}, \"590a16353cb44d4498e123b8bfeb3096\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"faaa484590d44e1baae0060e4593e995\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"d4605d7dba2d4b3c8760bcfd02edbf56\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"p\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_590a16353cb44d4498e123b8bfeb3096\", \"max\": 0.1, \"min\": 0.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.001, \"style\": \"IPY_MODEL_faaa484590d44e1baae0060e4593e995\", \"value\": 0.1}}, \"1fb6a510f23b4bb0a45bd7f69104d631\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"8b532b644fca4cb78a0f702c55aca4f0\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_d4605d7dba2d4b3c8760bcfd02edbf56\", \"IPY_MODEL_04fba99cb0d844dcb67633aafeb0dc2d\"], \"layout\": \"IPY_MODEL_1fb6a510f23b4bb0a45bd7f69104d631\"}}, \"36db5601876f4f019a8601eee07e81b9\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"04fba99cb0d844dcb67633aafeb0dc2d\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_36db5601876f4f019a8601eee07e81b9\", \"msg_id\": \"\", \"outputs\": []}}, \"5e862f457b9d4c81a8320c2ad1da5e46\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"b54fc17e2df94a6db87d19577f01eeea\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"c1248ecc78514bff9aaae21392cad9ee\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_5e862f457b9d4c81a8320c2ad1da5e46\", \"max\": 100.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_b54fc17e2df94a6db87d19577f01eeea\", \"value\": 100.0}}, \"b4431d4b6a9842d9b8ef0635d619f8e2\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c3278838a906486cb41ab63ad2c4b5d8\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"e007e373571d4281a75b1bf62d6ffb97\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_b4431d4b6a9842d9b8ef0635d619f8e2\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c3278838a906486cb41ab63ad2c4b5d8\", \"value\": \"100%\"}}, \"d2cb93fea30b4ba1af7ebddbf8d28753\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c218ce10b61749158fbb1e32074e196f\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"ae44c42bf6484c6cabb34aac0044dc7f\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_d2cb93fea30b4ba1af7ebddbf8d28753\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c218ce10b61749158fbb1e32074e196f\", \"value\": \" 100/100 [00:21&lt;00:00, 5.10it/s]\"}}, \"fc3eb1337da44d63b6824cf39b7a9dff\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"22d11bdf36f44d8d958892bcffd8b6ef\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_e007e373571d4281a75b1bf62d6ffb97\", \"IPY_MODEL_c1248ecc78514bff9aaae21392cad9ee\", \"IPY_MODEL_ae44c42bf6484c6cabb34aac0044dc7f\"], \"layout\": \"IPY_MODEL_fc3eb1337da44d63b6824cf39b7a9dff\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Chapter 11: Statistical Inference"},{"location":"04-advanced/03-stats/#introduction","text":"from IPython.display import YouTubeVideo YouTubeVideo ( id = \"P-0CJpO3spg\" , width = \"100%\" ) In this chapter, we are going to take a look at how to perform statistical inference on graphs.","title":"Introduction"},{"location":"04-advanced/03-stats/#statistics-refresher","text":"Before we can proceed with statistical inference on graphs, we must first refresh ourselves with some ideas from the world of statistics. Otherwise, the methods that we will end up using may seem a tad weird , and hence difficult to follow along. To review statistical ideas, let's set up a few statements and explore what they mean.","title":"Statistics refresher"},{"location":"04-advanced/03-stats/#we-are-concerned-with-models-of-randomness","text":"As with all things statistics, we are concerned with models of randomness. Here, probability distributions give us a way to think about random events and how to assign credibility points to them.","title":"We are concerned with models of randomness"},{"location":"04-advanced/03-stats/#in-an-abstract-fashion","text":"The supremely abstract way of thinking about a probability distribution is that it is the space of all possibilities of \"stuff\" with different credibility points distributed amongst each possible \"thing\".","title":"In an abstract fashion..."},{"location":"04-advanced/03-stats/#more-concretely-the-coin-flip","text":"A more concrete example is to consider the coin flip. Here, the space of all possibilities of \"stuff\" is the set of \"heads\" and \"tails\". If we have a fair coin, then we have 0.5 credibility points distributed to each of \"heads\" and \"tails\".","title":"More concretely: the coin flip"},{"location":"04-advanced/03-stats/#another-example-dice-rolls","text":"Another concrete example is to consider the six-sided dice. Here, the space of all possibilities of \"stuff\" is the set of numbers in the range [1, 6] [1, 6] . If we have a fair dice, then we have 1/6 credibility points assigned to each of the numbers. (Unfair dice will have an unequal distribution of credibility points across each face.)","title":"Another example: dice rolls"},{"location":"04-advanced/03-stats/#a-graph-based-example-social-networks","text":"If we receive an undirected social network graph with 5 nodes and 6 edges, we have to keep in mind that this graph with 6 edges was merely one of 15 \\choose 6 15 \\choose 6 ways to construct 5 node, 6 edge graphs. (15 comes up because there are 15 edges that can be constructed in a 5-node undirected graph.)","title":"A graph-based example: social networks"},{"location":"04-advanced/03-stats/#hypothesis-testing","text":"A commonplace task in statistical inferences is calculating the probability of observing a value or something more extreme under an assumed \"null\" model of reality. This is what we commonly call \"hypothesis testing\", and where the oft-misunderstood term \"p-value\" shows up.","title":"Hypothesis Testing"},{"location":"04-advanced/03-stats/#hypothesis-testing-in-coin-flips-by-simulation","text":"As an example, hypothesis testing in coin flips follows this logic: I observe that 8 out of 10 coin tosses give me heads, giving me a probability of heads p=0.8 p=0.8 (a summary statistic). Under a \"null distribution\" of a fair coin, I simulate the distribution of probability of heads (the summary statistic) that I would get from 10 coin tosses. Finally, I use that distribution to calculate the probability of observing p=0.8 p=0.8 or more extreme.","title":"Hypothesis testing in coin flips, by simulation"},{"location":"04-advanced/03-stats/#hypothesis-testing-in-graphs","text":"The same protocol applies when we perform hypothesis testing on graphs. Firstly, we calculate a summary statistic that describes our graph. Secondly, we propose a null graph model , and calculate our summary statistic under simulated versions of that null graph model. Thirdly, we look at the probability of observing the summary statistic value that we calculated in step 1 or more extreme, under the assumed graph null model distribution.","title":"Hypothesis testing in graphs"},{"location":"04-advanced/03-stats/#stochastic-graph-creation-models","text":"Since we are going to be dealing with models of randomness in graphs, let's take a look at some examples.","title":"Stochastic graph creation models"},{"location":"04-advanced/03-stats/#erdos-renyi-aka-binomial-graph","text":"On easy one to study is the Erdos-Renyi graph, also known as the \"binomial\" graph. The data generation story here is that we instantiate an undirected graph with n n nodes, giving \\frac{n^2 - n}{2} \\frac{n^2 - n}{2} possible edges. Each edge has a probability p p of being created. import networkx as nx G_er = nx . erdos_renyi_graph ( n = 30 , p = 0.2 ) nx . draw ( G_er ) You can verify that there's approximately 20% of \\frac{30^2 - 30}{2} = 435 \\frac{30^2 - 30}{2} = 435 . len ( G_er . edges ()) 87 len ( G_er . edges ()) / 435 0.2 We can also look at the degree distribution: import pandas as pd from nams.functions import ecdf import matplotlib.pyplot as plt x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_er )))) plt . scatter ( x , y ) <matplotlib.collections.PathCollection at 0x7febd4b61730>","title":"Erdos-Renyi (a.k.a. \"binomial\") graph"},{"location":"04-advanced/03-stats/#barabasi-albert-graph","text":"The data generating story of this graph generator is essentially that nodes that have lots of edges preferentially get new edges attached onto them. This is what we call a \"preferential attachment\" process. G_ba = nx . barabasi_albert_graph ( n = 30 , m = 3 ) nx . draw ( G_ba ) len ( G_ba . edges ()) 81 And the degree distribution: x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_ba )))) plt . scatter ( x , y ) <matplotlib.collections.PathCollection at 0x7febd4abf8e0> You can see that even though the number of edges between the two graphs are similar, their degree distribution is wildly different.","title":"Barabasi-Albert Graph"},{"location":"04-advanced/03-stats/#load-data","text":"For this notebook, we are going to look at a protein-protein interaction network, and test the hypothesis that this network was not generated by the data generating process described by an Erdos-Renyi graph. Let's load a protein-protein interaction network dataset . This undirected network contains protein interactions contained in yeast. Research showed that proteins with a high degree were more important for the surivial of the yeast than others. A node represents a protein and an edge represents a metabolic interaction between two proteins. The network contains loops. from nams import load_data as cf G = cf . load_propro_network () for n , d in G . nodes ( data = True ): G . nodes [ n ][ \"degree\" ] = G . degree ( n ) As is always the case, let's make sure we know some basic stats of the graph. len ( G . nodes ()) 1870 len ( G . edges ()) 2277 Let's also examine the degree distribution of the graph. x , y = ecdf ( pd . Series ( dict ( nx . degree ( G )))) plt . scatter ( x , y ) <matplotlib.collections.PathCollection at 0x7febd46067c0> Finally, we should visualize the graph to get a feel for it. import nxviz as nv from nxviz import annotate nv . circos ( G , sort_by = \"degree\" , node_color_by = \"degree\" , node_enc_kwargs = { \"size_scale\" : 10 }) annotate . node_colormapping ( G , color_by = \"degree\" ) One thing we might infer from this visualization is that the vast majority of nodes have a very small degree, while a very small number of nodes have a high degree. That would prompt us to think: what process could be responsible for generating this graph?","title":"Load Data"},{"location":"04-advanced/03-stats/#inferring-graph-generating-model","text":"Given a graph dataset, how do we identify which data generating model provides the best fit? One way to do this is to compare characteristics of a graph generating model against the characteristics of the graph. The logic here is that if we have a good graph generating model for the data, we should, in theory, observe the observed graph's characteristics in the graphs generated by the graph generating model.","title":"Inferring Graph Generating Model"},{"location":"04-advanced/03-stats/#comparison-of-degree-distribution","text":"Let's compare the degree distribution between the data, a few Erdos-Renyi graphs, and a few Barabasi-Albert graphs.","title":"Comparison of degree distribution"},{"location":"04-advanced/03-stats/#comparison-with-barabasi-albert-graphs","text":"from ipywidgets import interact , IntSlider m = IntSlider ( value = 2 , min = 1 , max = 10 ) @interact ( m = m ) def compare_barabasi_albert_graph ( m ): fig , ax = plt . subplots () G_ba = nx . barabasi_albert_graph ( n = len ( G . nodes ()), m = m ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_ba )))) ax . scatter ( x , y , label = \"Barabasi-Albert Graph\" ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G )))) ax . scatter ( x , y , label = \"Protein Interaction Network\" ) ax . legend () var element = $('#f60dacd8-f18a-4f8f-9008-309b2c8911b0');","title":"Comparison with Barabasi-Albert graphs"},{"location":"04-advanced/03-stats/#comparison-with-erdos-renyi-graphs","text":"from ipywidgets import FloatSlider p = FloatSlider ( value = 0.6 , min = 0 , max = 0.1 , step = 0.001 ) @interact ( p = p ) def compare_erdos_renyi_graph ( p ): fig , ax = plt . subplots () G_er = nx . erdos_renyi_graph ( n = len ( G . nodes ()), p = p ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G_er )))) ax . scatter ( x , y , label = \"Erdos-Renyi Graph\" ) x , y = ecdf ( pd . Series ( dict ( nx . degree ( G )))) ax . scatter ( x , y , label = \"Protein Interaction Network\" ) ax . legend () ax . set_title ( f \"p= { p } \" ) var element = $('#76794d69-d84f-45b3-ae04-bb9068f07d7f'); Given the degree distribution only, which model do you think better describes the generation of a protein-protein interaction network?","title":"Comparison with Erdos-Renyi graphs"},{"location":"04-advanced/03-stats/#quantitative-model-comparison","text":"Each time we plug in a value of m m for the Barabasi-Albert graph model, we are using one of many possible Barabasi-Albert graph models, each with a different m m . Similarly, each time we choose a different p p for the Erdos-Renyi model, we are using one of many possible Erdos-Renyi graph models, each with a different p p . To quantitatively compare degree distributions, we can use the Wasserstein distance between the data. Let's see how to implement this. from scipy.stats import wasserstein_distance def erdos_renyi_degdist ( n , p ): \"\"\"Return a Pandas series of degree distribution of an Erdos-Renyi graph.\"\"\" G = nx . erdos_renyi_graph ( n = n , p = p ) return pd . Series ( dict ( nx . degree ( G ))) def barabasi_albert_degdist ( n , m ): \"\"\"Return a Pandas series of degree distribution of an Barabasi-Albert graph.\"\"\" G = nx . barabasi_albert_graph ( n = n , m = m ) return pd . Series ( dict ( nx . degree ( G ))) deg = pd . Series ( dict ( nx . degree ( G ))) er_deg = erdos_renyi_degdist ( n = len ( G . nodes ()), p = 0.001 ) ba_deg = barabasi_albert_degdist ( n = len ( G . nodes ()), m = 1 ) wasserstein_distance ( deg , er_deg ), wasserstein_distance ( deg , ba_deg ) (0.8021390374331547, 0.4909090909090922) Notice that because the graphs are instantiated in a non-deterministic fashion, re-running the cell above will give you different values for each new graph generated. Let's now plot the wasserstein distance to our graph data for the two particular Erdos-Renyi and Barabasi-Albert graph models shown above. import matplotlib.pyplot as plt from tqdm.autonotebook import tqdm er_dist = [] ba_dist = [] for _ in tqdm ( range ( 100 )): er_deg = erdos_renyi_degdist ( n = len ( G . nodes ()), p = 0.001 ) er_dist . append ( wasserstein_distance ( deg , er_deg )) ba_deg = barabasi_albert_degdist ( n = len ( G . nodes ()), m = 1 ) ba_dist . append ( wasserstein_distance ( deg , ba_deg )) # er_degs = [erdos_renyi_degdist(n=len(G.nodes()), p=0.001) for _ in range(100)] var element = $('#7914d23e-a2ea-43e1-8ad8-4d381e3f66a0'); import seaborn as sns import janitor data = ( pd . DataFrame ( { \"Erdos-Renyi\" : er_dist , \"Barabasi-Albert\" : ba_dist , } ) . melt ( value_vars = [ \"Erdos-Renyi\" , \"Barabasi-Albert\" ]) . rename_columns ({ \"variable\" : \"Graph Model\" , \"value\" : \"Wasserstein Distance\" }) ) sns . swarmplot ( data = data , x = \"Graph Model\" , y = \"Wasserstein Distance\" ) <AxesSubplot:xlabel='Graph Model', ylabel='Wasserstein Distance'> From this, we might conclude that the Barabasi-Albert graph with m=1 m=1 has the better fit to the protein-protein interaction network graph.","title":"Quantitative Model Comparison"},{"location":"04-advanced/03-stats/#interpretation","text":"That statement, accurate as it might be, still does not connect the dots to biology . Let's think about the generative model for this graph. The Barabasi-Albert graph gives us a model for \"rich gets richer\". Given the current state of the graph, if we want to add a new edge, we first pick a node with probability proportional to the number of edges it already has. Then, we pick another node with probability proportional to the number of edges that it has too. Finally, we add an edge there. This has the effect of \"enriching\" nodes that have a large number of edges with more edges. How might this connect to biology? We can't necessarily provide a concrete answer, but this model might help raise new hypotheses. For example, if protein-protein interactions of the \"binding\" kind are driven by subdomains, then proteins that acquire a domain through recombination may end up being able to bind to everything else that the domain was able to. In this fashion, proteins with that particular binding domain gain new edges more readily. Testing these hypotheses would be a totally different matter, and at this point, I submit the above hypothesis with a large amount of salt thrown over my shoulder. In other words, the hypothesized mechanism could be completely wrong. However, I hope that this example illustrated that the usage of a \"graph generative model\" can help us narrow down hypotheses about the observed world. {\"state\": {\"ee1cfab561fb4e438a15ed5bef650627\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ab860990423b4ac99f0a6fa603955771\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"855502b204204fc4957f292184981bd0\": {\"model_name\": \"IntSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"IntSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"IntSliderView\", \"continuous_update\": true, \"description\": \"m\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_ee1cfab561fb4e438a15ed5bef650627\", \"max\": 10, \"min\": 1, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \"d\", \"step\": 1, \"style\": \"IPY_MODEL_ab860990423b4ac99f0a6fa603955771\", \"value\": 2}}, \"0d1561d6d3a04a7ea2217da812b9c9de\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"02e841e8daa74d5ab2bdf03104ecb99f\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_855502b204204fc4957f292184981bd0\", \"IPY_MODEL_ab07dff637eb4e729e54fab85711c44e\"], \"layout\": \"IPY_MODEL_0d1561d6d3a04a7ea2217da812b9c9de\"}}, \"137eb8107f7d40e78563e5b22f5d9234\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"ab07dff637eb4e729e54fab85711c44e\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_137eb8107f7d40e78563e5b22f5d9234\", \"msg_id\": \"\", \"outputs\": []}}, \"590a16353cb44d4498e123b8bfeb3096\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"faaa484590d44e1baae0060e4593e995\": {\"model_name\": \"SliderStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"SliderStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\", \"handle_color\": null}}, \"d4605d7dba2d4b3c8760bcfd02edbf56\": {\"model_name\": \"FloatSliderModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatSliderModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"FloatSliderView\", \"continuous_update\": true, \"description\": \"p\", \"description_tooltip\": null, \"disabled\": false, \"layout\": \"IPY_MODEL_590a16353cb44d4498e123b8bfeb3096\", \"max\": 0.1, \"min\": 0.0, \"orientation\": \"horizontal\", \"readout\": true, \"readout_format\": \".2f\", \"step\": 0.001, \"style\": \"IPY_MODEL_faaa484590d44e1baae0060e4593e995\", \"value\": 0.1}}, \"1fb6a510f23b4bb0a45bd7f69104d631\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"8b532b644fca4cb78a0f702c55aca4f0\": {\"model_name\": \"VBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [\"widget-interact\"], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"VBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"VBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_d4605d7dba2d4b3c8760bcfd02edbf56\", \"IPY_MODEL_04fba99cb0d844dcb67633aafeb0dc2d\"], \"layout\": \"IPY_MODEL_1fb6a510f23b4bb0a45bd7f69104d631\"}}, \"36db5601876f4f019a8601eee07e81b9\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"04fba99cb0d844dcb67633aafeb0dc2d\": {\"model_name\": \"OutputModel\", \"model_module\": \"@jupyter-widgets/output\", \"model_module_version\": \"1.0.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/output\", \"_model_module_version\": \"1.0.0\", \"_model_name\": \"OutputModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/output\", \"_view_module_version\": \"1.0.0\", \"_view_name\": \"OutputView\", \"layout\": \"IPY_MODEL_36db5601876f4f019a8601eee07e81b9\", \"msg_id\": \"\", \"outputs\": []}}, \"5e862f457b9d4c81a8320c2ad1da5e46\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"b54fc17e2df94a6db87d19577f01eeea\": {\"model_name\": \"ProgressStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"ProgressStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"bar_color\": null, \"description_width\": \"\"}}, \"c1248ecc78514bff9aaae21392cad9ee\": {\"model_name\": \"FloatProgressModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"FloatProgressModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"ProgressView\", \"bar_style\": \"success\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_5e862f457b9d4c81a8320c2ad1da5e46\", \"max\": 100.0, \"min\": 0.0, \"orientation\": \"horizontal\", \"style\": \"IPY_MODEL_b54fc17e2df94a6db87d19577f01eeea\", \"value\": 100.0}}, \"b4431d4b6a9842d9b8ef0635d619f8e2\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c3278838a906486cb41ab63ad2c4b5d8\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"e007e373571d4281a75b1bf62d6ffb97\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_b4431d4b6a9842d9b8ef0635d619f8e2\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c3278838a906486cb41ab63ad2c4b5d8\", \"value\": \"100%\"}}, \"d2cb93fea30b4ba1af7ebddbf8d28753\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"c218ce10b61749158fbb1e32074e196f\": {\"model_name\": \"DescriptionStyleModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"DescriptionStyleModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"StyleView\", \"description_width\": \"\"}}, \"ae44c42bf6484c6cabb34aac0044dc7f\": {\"model_name\": \"HTMLModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HTMLModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HTMLView\", \"description\": \"\", \"description_tooltip\": null, \"layout\": \"IPY_MODEL_d2cb93fea30b4ba1af7ebddbf8d28753\", \"placeholder\": \"\\u200b\", \"style\": \"IPY_MODEL_c218ce10b61749158fbb1e32074e196f\", \"value\": \" 100/100 [00:21&lt;00:00, 5.10it/s]\"}}, \"fc3eb1337da44d63b6824cf39b7a9dff\": {\"model_name\": \"LayoutModel\", \"model_module\": \"@jupyter-widgets/base\", \"model_module_version\": \"1.2.0\", \"state\": {\"_model_module\": \"@jupyter-widgets/base\", \"_model_module_version\": \"1.2.0\", \"_model_name\": \"LayoutModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/base\", \"_view_module_version\": \"1.2.0\", \"_view_name\": \"LayoutView\", \"align_content\": null, \"align_items\": null, \"align_self\": null, \"border\": null, \"bottom\": null, \"display\": null, \"flex\": null, \"flex_flow\": null, \"grid_area\": null, \"grid_auto_columns\": null, \"grid_auto_flow\": null, \"grid_auto_rows\": null, \"grid_column\": null, \"grid_gap\": null, \"grid_row\": null, \"grid_template_areas\": null, \"grid_template_columns\": null, \"grid_template_rows\": null, \"height\": null, \"justify_content\": null, \"justify_items\": null, \"left\": null, \"margin\": null, \"max_height\": null, \"max_width\": null, \"min_height\": null, \"min_width\": null, \"object_fit\": null, \"object_position\": null, \"order\": null, \"overflow\": null, \"overflow_x\": null, \"overflow_y\": null, \"padding\": null, \"right\": null, \"top\": null, \"visibility\": null, \"width\": null}}, \"22d11bdf36f44d8d958892bcffd8b6ef\": {\"model_name\": \"HBoxModel\", \"model_module\": \"@jupyter-widgets/controls\", \"model_module_version\": \"1.5.0\", \"state\": {\"_dom_classes\": [], \"_model_module\": \"@jupyter-widgets/controls\", \"_model_module_version\": \"1.5.0\", \"_model_name\": \"HBoxModel\", \"_view_count\": null, \"_view_module\": \"@jupyter-widgets/controls\", \"_view_module_version\": \"1.5.0\", \"_view_name\": \"HBoxView\", \"box_style\": \"\", \"children\": [\"IPY_MODEL_e007e373571d4281a75b1bf62d6ffb97\", \"IPY_MODEL_c1248ecc78514bff9aaae21392cad9ee\", \"IPY_MODEL_ae44c42bf6484c6cabb34aac0044dc7f\"], \"layout\": \"IPY_MODEL_fc3eb1337da44d63b6824cf39b7a9dff\"}}}, \"version_major\": 2, \"version_minor\": 0}","title":"Interpretation"},{"location":"05-casestudies/01-gameofthrones/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import pandas as pd import networkx as nx import community import numpy as np import matplotlib.pyplot as plt import warnings warnings . filterwarnings ( 'ignore' ) Introduction In this chapter, we will use Game of Thrones as a case study to practice our newly learnt skills of network analysis. It is suprising right? What is the relationship between a fatansy TV show/novel and network science or Python(not dragons). If you haven't heard of Game of Thrones, then you must be really good at hiding. Game of Thrones is a hugely popular television series by HBO based on the (also) hugely popular book series A Song of Ice and Fire by George R.R. Martin. In this notebook, we will analyze the co-occurrence network of the characters in the Game of Thrones books. Here, two characters are considered to co-occur if their names appear in the vicinity of 15 words from one another in the books. The figure below is a precusor of what we will analyse in this chapter. The dataset is publicly avaiable for the 5 books at https://github.com/mathbeveridge/asoiaf. This is an interaction network and were created by connecting two characters whenever their names (or nicknames) appeared within 15 words of one another in one of the books. The edge weight corresponds to the number of interactions. Blog: https://networkofthrones.wordpress.com from nams import load_data as cf books = cf . load_game_of_thrones_data () The resulting DataFrame books has 5 columns: Source, Target, Type, weight, and book. Source and target are the two nodes that are linked by an edge. As we know a network can have directed or undirected edges and in this network all the edges are undirected. The weight attribute of every edge tells us the number of interactions that the characters have had over the book, and the book column tells us the book number. Let's have a look at the data. # We also add this weight_inv to our dataset. # Why? we will discuss it in a later section. books [ 'weight_inv' ] = 1 / books . weight books . head () id Source Target Type weight book weight_inv 0 Addam-Marbrand Jaime-Lannister Undirected 3 1 0.333333 1 Addam-Marbrand Tywin-Lannister Undirected 6 1 0.166667 2 Aegon-I-Targaryen Daenerys-Targaryen Undirected 5 1 0.2 3 Aegon-I-Targaryen Eddard-Stark Undirected 4 1 0.25 4 Aemon-Targaryen-(Maester-Aemon) Alliser-Thorne Undirected 4 1 0.25 From the above data we can see that the characters Addam Marbrand and Tywin Lannister have interacted 6 times in the first book. We can investigate this data by using the pandas DataFrame. Let's find all the interactions of Robb Stark in the third book. robbstark = ( books . query ( \"book == 3\" ) . query ( \"Source == 'Robb-Stark' or Target == 'Robb-Stark'\" ) ) robbstark . head () id Source Target Type weight book weight_inv 1468 Aegon-Frey-(son-of-Stevron) Robb-Stark Undirected 5 3 0.2 1582 Arya-Stark Robb-Stark Undirected 14 3 0.0714286 1604 Balon-Greyjoy Robb-Stark Undirected 6 3 0.166667 1677 Bran-Stark Robb-Stark Undirected 18 3 0.0555556 1683 Brandon-Stark Robb-Stark Undirected 3 3 0.333333 As you can see this data easily translates to a network problem. Now it's time to create a network. We create a graph for each book. It's possible to create one MultiGraph (Graph with multiple edges between nodes) instead of 5 graphs, but it is easier to analyse and manipulate individual Graph objects rather than a MultiGraph . # example of creating a MultiGraph # all_books_multigraph = nx.from_pandas_edgelist( # books, source='Source', target='Target', # edge_attr=['weight', 'book'], # create_using=nx.MultiGraph) # we create a list of graph objects using # nx.from_pandas_edgelist and specifying # the edge attributes. graphs = [ nx . from_pandas_edgelist ( books [ books . book == i ], source = 'Source' , target = 'Target' , edge_attr = [ 'weight' , 'weight_inv' ]) for i in range ( 1 , 6 )] # The Graph object associated with the first book. graphs [ 0 ] <networkx.classes.graph.Graph at 0x7f6e903d2310> # To access the relationship edges in the graph with # the edge attribute weight data (data=True) relationships = list ( graphs [ 0 ] . edges ( data = True )) relationships [ 0 : 3 ] [('Addam-Marbrand', 'Jaime-Lannister', {'weight': 3, 'weight_inv': 0.3333333333333333}), ('Addam-Marbrand', 'Tywin-Lannister', {'weight': 6, 'weight_inv': 0.16666666666666666}), ('Jaime-Lannister', 'Aerys-II-Targaryen', {'weight': 5, 'weight_inv': 0.2})] Finding the most important node i.e character in these networks. Let's use our network analysis knowledge to decrypt these Graphs that we have just created. Is it Jon Snow, Tyrion, Daenerys, or someone else? Let's see! Network Science offers us many different metrics to measure the importance of a node in a network as we saw in the first part of the tutorial. Note that there is no \"correct\" way of calculating the most important node in a network, every metric has a different meaning. First, let's measure the importance of a node in a network by looking at the number of neighbors it has, that is, the number of nodes it is connected to. For example, an influential account on Twitter, where the follower-followee relationship forms the network, is an account which has a high number of followers. This measure of importance is called degree centrality. Using this measure, let's extract the top ten important characters from the first book ( graphs[0] ) and the fifth book ( graphs[4] ). NOTE: We are using zero-indexing and that's why the graph of the first book is acceseed by graphs[0] . # We use the in-built degree_centrality method deg_cen_book1 = nx . degree_centrality ( graphs [ 0 ]) deg_cen_book5 = nx . degree_centrality ( graphs [ 4 ]) degree_centrality returns a dictionary and to access the results we can directly use the name of the character. deg_cen_book1 [ 'Daenerys-Targaryen' ] 0.11290322580645162 Top 5 important characters in the first book according to degree centrality. # The following expression sorts the dictionary by # degree centrality and returns the top 5 from a graph sorted ( deg_cen_book1 . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('Eddard-Stark', 0.3548387096774194), ('Robert-Baratheon', 0.2688172043010753), ('Tyrion-Lannister', 0.24731182795698928), ('Catelyn-Stark', 0.23118279569892475), ('Jon-Snow', 0.19892473118279572)] Top 5 important characters in the fifth book according to degree centrality. sorted ( deg_cen_book5 . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('Jon-Snow', 0.1962025316455696), ('Daenerys-Targaryen', 0.18354430379746836), ('Stannis-Baratheon', 0.14873417721518986), ('Tyrion-Lannister', 0.10443037974683544), ('Theon-Greyjoy', 0.10443037974683544)] To visualize the distribution of degree centrality let's plot a histogram of degree centrality. plt . hist ( deg_cen_book1 . values (), bins = 30 ) plt . show () The above plot shows something that is expected, a high portion of characters aren't connected to lot of other characters while some characters are highly connected all through the network. A close real world example of this is a social network like Twitter where a few people have millions of connections(followers) but majority of users aren't connected to that many other users. This exponential decay like property resembles power law in real life networks. # A log-log plot to show the \"signature\" of power law in graphs. from collections import Counter hist = Counter ( deg_cen_book1 . values ()) plt . scatter ( np . log2 ( list ( hist . keys ())), np . log2 ( list ( hist . values ())), alpha = 0.9 ) plt . show () Exercise Create a new centrality measure, weighted_degree(Graph, weight) which takes in Graph and the weight attribute and returns a weighted degree dictionary. Weighted degree is calculated by summing the weight of the all edges of a node and find the top five characters according to this measure. from nams.solutions.got import weighted_degree plt . hist ( list ( weighted_degree ( graphs [ 0 ], 'weight' ) . values ()), bins = 30 ) plt . show () sorted ( weighted_degree ( graphs [ 0 ], 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('Eddard-Stark', 1284), ('Robert-Baratheon', 941), ('Jon-Snow', 784), ('Tyrion-Lannister', 650), ('Sansa-Stark', 545)] Betweeness centrality Let's do this for Betweeness centrality and check if this makes any difference. As different centrality method use different measures underneath, they find nodes which are important in the network. A centrality method like Betweeness centrality finds nodes which are structurally important to the network, which binds the network together and densely. # First check unweighted (just the structure) sorted ( nx . betweenness_centrality ( graphs [ 0 ]) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.2696038913836117), ('Robert-Baratheon', 0.21403028397371796), ('Tyrion-Lannister', 0.1902124972697492), ('Jon-Snow', 0.17158135899829566), ('Catelyn-Stark', 0.1513952715347627), ('Daenerys-Targaryen', 0.08627015537511595), ('Robb-Stark', 0.07298399629664767), ('Drogo', 0.06481224290874964), ('Bran-Stark', 0.05579958811784442), ('Sansa-Stark', 0.03714483664326785)] # Let's care about interactions now sorted ( nx . betweenness_centrality ( graphs [ 0 ], weight = 'weight_inv' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.5926474861958733), ('Catelyn-Stark', 0.36855565242662014), ('Jon-Snow', 0.3514094739901191), ('Robert-Baratheon', 0.3329991281604185), ('Tyrion-Lannister', 0.27137460040685846), ('Daenerys-Targaryen', 0.202615518744551), ('Bran-Stark', 0.0945655332752107), ('Robb-Stark', 0.09177564661435629), ('Arya-Stark', 0.06939843068875327), ('Sansa-Stark', 0.06870095902353966)] We can see there are some differences between the unweighted and weighted centrality measures. Another thing to note is that we are using the weight_inv attribute instead of weight(the number of interactions between characters). This decision is based on the way we want to assign the notion of \"importance\" of a character. The basic idea behind betweenness centrality is to find nodes which are essential to the structure of the network. As betweenness centrality computes shortest paths underneath, in the case of weighted betweenness centrality it will end up penalising characters with high number of interactions. By using weight_inv we will prop up the characters with high interactions with other characters. PageRank The billion dollar algorithm, PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. NOTE: We don't need to worry about weight and weight_inv in PageRank as the algorithm uses weights in the opposite sense (larger weights are better). This may seem confusing as different centrality measures have different definition of weights. So it is always better to have a look at documentation before using weights in a centrality measure. # by default weight attribute in PageRank is weight # so we use weight=None to find the unweighted results sorted ( nx . pagerank_numpy ( graphs [ 0 ], weight = None ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.04552079222830667), ('Tyrion-Lannister', 0.03301362462493266), ('Catelyn-Stark', 0.03019310528663196), ('Robert-Baratheon', 0.029834742227736733), ('Jon-Snow', 0.026834499522066214), ('Robb-Stark', 0.02156294129724751), ('Sansa-Stark', 0.02000803404286464), ('Bran-Stark', 0.01994578678623833), ('Jaime-Lannister', 0.017507847202846878), ('Cersei-Lannister', 0.01708260458475807)] sorted ( nx . pagerank_numpy ( graphs [ 0 ], weight = 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.07239401100498243), ('Robert-Baratheon', 0.048517275705099325), ('Jon-Snow', 0.047706890624749115), ('Tyrion-Lannister', 0.04367437892706291), ('Catelyn-Stark', 0.034667034701307387), ('Bran-Stark', 0.029774200539800195), ('Robb-Stark', 0.02921618364519685), ('Daenerys-Targaryen', 0.02708962251302115), ('Sansa-Stark', 0.02696177891568309), ('Cersei-Lannister', 0.021631679397418963)] Exercise Is there a correlation between these techniques? Find the correlation between these four techniques. pagerank (weight = 'weight') betweenness_centrality (weight = 'weight_inv') weighted_degree degree centrality HINT: Use pandas correlation from nams.solutions.got import correlation_centrality correlation_centrality ( graphs [ 0 ]) 0 1 2 3 0 1 0.910352 0.992166 0.949307 1 0.910352 1 0.87924 0.790526 2 0.992166 0.87924 1 0.95506 3 0.949307 0.790526 0.95506 1 Evolution of importance of characters over the books According to degree centrality the most important character in the first book is Eddard Stark but he is not even in the top 10 of the fifth book. The importance changes over the course of five books, because you know stuff happens ;) Let's look at the evolution of degree centrality of a couple of characters like Eddard Stark, Jon Snow, Tyrion which showed up in the top 10 of degree centrality in first book. We create a dataframe with character columns and index as books where every entry is the degree centrality of the character in that particular book and plot the evolution of degree centrality Eddard Stark, Jon Snow and Tyrion. We can see that the importance of Eddard Stark in the network dies off and with Jon Snow there is a drop in the fourth book but a sudden rise in the fifth book evol = [ nx . degree_centrality ( graph ) for graph in graphs ] evol_df = pd . DataFrame . from_records ( evol ) . fillna ( 0 ) evol_df [[ 'Eddard-Stark' , 'Tyrion-Lannister' , 'Jon-Snow' ]] . plot () plt . show () set_of_char = set () for i in range ( 5 ): set_of_char |= set ( list ( evol_df . T [ i ] . sort_values ( ascending = False )[ 0 : 5 ] . index )) set_of_char {'Arya-Stark', 'Brienne-of-Tarth', 'Catelyn-Stark', 'Cersei-Lannister', 'Daenerys-Targaryen', 'Eddard-Stark', 'Jaime-Lannister', 'Joffrey-Baratheon', 'Jon-Snow', 'Margaery-Tyrell', 'Robb-Stark', 'Robert-Baratheon', 'Sansa-Stark', 'Stannis-Baratheon', 'Theon-Greyjoy', 'Tyrion-Lannister'} Exercise Plot the evolution of betweenness centrality of the above mentioned characters over the 5 books. from nams.solutions.got import evol_betweenness evol_betweenness ( graphs ) So what's up with Stannis Baratheon? sorted ( nx . degree_centrality ( graphs [ 4 ]) . items (), key = lambda x : x [ 1 ], reverse = True )[: 5 ] [('Jon-Snow', 0.1962025316455696), ('Daenerys-Targaryen', 0.18354430379746836), ('Stannis-Baratheon', 0.14873417721518986), ('Tyrion-Lannister', 0.10443037974683544), ('Theon-Greyjoy', 0.10443037974683544)] sorted ( nx . betweenness_centrality ( graphs [ 4 ]) . items (), key = lambda x : x [ 1 ], reverse = True )[: 5 ] [('Stannis-Baratheon', 0.45283060689247934), ('Daenerys-Targaryen', 0.2959459062106149), ('Jon-Snow', 0.24484873673158666), ('Tyrion-Lannister', 0.20961613179551256), ('Robert-Baratheon', 0.17716906651536968)] nx . draw ( nx . barbell_graph ( 5 , 1 ), with_labels = True ) As we know the a higher betweenness centrality means that the node is crucial for the structure of the network, and in the case of Stannis Baratheon in the fifth book it seems like Stannis Baratheon has characterstics similar to that of node 5 in the above example as it seems to be the holding the network together. As evident from the betweenness centrality scores of the above example of barbell graph, node 5 is the most important node in this network. nx . betweenness_centrality ( nx . barbell_graph ( 5 , 1 )) {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.5333333333333333, 6: 0.5333333333333333, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 5: 0.5555555555555556} Community detection in Networks A network is said to have community structure if the nodes of the network can be easily grouped into (potentially overlapping) sets of nodes such that each set of nodes is densely connected internally. There are multiple algorithms and definitions to calculate these communites in a network. We will use louvain community detection algorithm to find the modules in our graph. import nxviz as nv from nxviz import annotate plt . figure ( figsize = ( 8 , 8 )) partition = community . best_partition ( graphs [ 0 ], randomize = False ) # Annotate nodes' partitions for n in graphs [ 0 ] . nodes (): graphs [ 0 ] . nodes [ n ][ \"partition\" ] = partition [ n ] graphs [ 0 ] . nodes [ n ][ \"degree\" ] = graphs [ 0 ] . degree ( n ) nv . matrix ( graphs [ 0 ], group_by = \"partition\" , sort_by = \"degree\" , node_color_by = \"partition\" ) annotate . matrix_block ( graphs [ 0 ], group_by = \"partition\" , color_by = \"partition\" ) annotate . matrix_group ( graphs [ 0 ], group_by = \"partition\" , offset =- 8 ) A common defining quality of a community is that the within-community edges are denser than the between-community edges. # louvain community detection find us 8 different set of communities partition_dict = {} for character , par in partition . items (): if par in partition_dict : partition_dict [ par ] . append ( character ) else : partition_dict [ par ] = [ character ] len ( partition_dict ) 8 partition_dict [ 2 ] ['Bran-Stark', 'Rickon-Stark', 'Robb-Stark', 'Luwin', 'Theon-Greyjoy', 'Hali', 'Hallis-Mollen', 'Hodor', 'Hullen', 'Joseth', 'Nan', 'Osha', 'Rickard-Karstark', 'Rickard-Stark', 'Stiv', 'Jon-Umber-(Greatjon)', 'Galbart-Glover', 'Roose-Bolton', 'Maege-Mormont'] If we plot these communities of the network we see a denser network as compared to the original network which contains all the characters. nx . draw ( nx . subgraph ( graphs [ 0 ], partition_dict [ 3 ])) nx . draw ( nx . subgraph ( graphs [ 0 ], partition_dict [ 1 ])) We can test this by calculating the density of the network and the community. Like in the following example the network between characters in a community is 5 times more dense than the original network. nx . density ( nx . subgraph ( graphs [ 0 ], partition_dict [ 4 ]) ) / nx . density ( graphs [ 0 ]) 25.42543859649123 Exercise Find the most important node in the partitions according to degree centrality of the nodes using the partition_dict we have already created. from nams.solutions.got import most_important_node_in_partition most_important_node_in_partition ( graphs [ 0 ], partition_dict ) {7: 'Tyrion-Lannister', 1: 'Daenerys-Targaryen', 6: 'Eddard-Stark', 3: 'Jon-Snow', 5: 'Sansa-Stark', 2: 'Robb-Stark', 0: 'Waymar-Royce', 4: 'Danwell-Frey'} Solutions Here are the solutions to the exercises above. from nams.solutions import got import inspect print ( inspect . getsource ( got )) import pandas as pd import networkx as nx def weighted_degree(G, weight): result = dict() for node in G.nodes(): weight_degree = 0 for n in G.edges([node], data=True): weight_degree += n[2][\"weight\"] result[node] = weight_degree return result def correlation_centrality(G): cor = pd.DataFrame.from_records( [ nx.pagerank_numpy(G, weight=\"weight\"), nx.betweenness_centrality(G, weight=\"weight_inv\"), weighted_degree(G, \"weight\"), nx.degree_centrality(G), ] ) return cor.T.corr() def evol_betweenness(graphs): evol = [nx.betweenness_centrality(graph, weight=\"weight_inv\") for graph in graphs] evol_df = pd.DataFrame.from_records(evol).fillna(0) set_of_char = set() for i in range(5): set_of_char |= set(list(evol_df.T[i].sort_values(ascending=False)[0:5].index)) evol_df[list(set_of_char)].plot(figsize=(19, 10)) def most_important_node_in_partition(graph, partition_dict): max_d = {} deg = nx.degree_centrality(graph) for group in partition_dict: temp = 0 for character in partition_dict[group]: if deg[character] > temp: max_d[group] = character temp = deg[character] return max_d","title":"Chapter 12: Game of Thrones"},{"location":"05-casestudies/01-gameofthrones/#introduction","text":"In this chapter, we will use Game of Thrones as a case study to practice our newly learnt skills of network analysis. It is suprising right? What is the relationship between a fatansy TV show/novel and network science or Python(not dragons). If you haven't heard of Game of Thrones, then you must be really good at hiding. Game of Thrones is a hugely popular television series by HBO based on the (also) hugely popular book series A Song of Ice and Fire by George R.R. Martin. In this notebook, we will analyze the co-occurrence network of the characters in the Game of Thrones books. Here, two characters are considered to co-occur if their names appear in the vicinity of 15 words from one another in the books. The figure below is a precusor of what we will analyse in this chapter. The dataset is publicly avaiable for the 5 books at https://github.com/mathbeveridge/asoiaf. This is an interaction network and were created by connecting two characters whenever their names (or nicknames) appeared within 15 words of one another in one of the books. The edge weight corresponds to the number of interactions. Blog: https://networkofthrones.wordpress.com from nams import load_data as cf books = cf . load_game_of_thrones_data () The resulting DataFrame books has 5 columns: Source, Target, Type, weight, and book. Source and target are the two nodes that are linked by an edge. As we know a network can have directed or undirected edges and in this network all the edges are undirected. The weight attribute of every edge tells us the number of interactions that the characters have had over the book, and the book column tells us the book number. Let's have a look at the data. # We also add this weight_inv to our dataset. # Why? we will discuss it in a later section. books [ 'weight_inv' ] = 1 / books . weight books . head () id Source Target Type weight book weight_inv 0 Addam-Marbrand Jaime-Lannister Undirected 3 1 0.333333 1 Addam-Marbrand Tywin-Lannister Undirected 6 1 0.166667 2 Aegon-I-Targaryen Daenerys-Targaryen Undirected 5 1 0.2 3 Aegon-I-Targaryen Eddard-Stark Undirected 4 1 0.25 4 Aemon-Targaryen-(Maester-Aemon) Alliser-Thorne Undirected 4 1 0.25 From the above data we can see that the characters Addam Marbrand and Tywin Lannister have interacted 6 times in the first book. We can investigate this data by using the pandas DataFrame. Let's find all the interactions of Robb Stark in the third book. robbstark = ( books . query ( \"book == 3\" ) . query ( \"Source == 'Robb-Stark' or Target == 'Robb-Stark'\" ) ) robbstark . head () id Source Target Type weight book weight_inv 1468 Aegon-Frey-(son-of-Stevron) Robb-Stark Undirected 5 3 0.2 1582 Arya-Stark Robb-Stark Undirected 14 3 0.0714286 1604 Balon-Greyjoy Robb-Stark Undirected 6 3 0.166667 1677 Bran-Stark Robb-Stark Undirected 18 3 0.0555556 1683 Brandon-Stark Robb-Stark Undirected 3 3 0.333333 As you can see this data easily translates to a network problem. Now it's time to create a network. We create a graph for each book. It's possible to create one MultiGraph (Graph with multiple edges between nodes) instead of 5 graphs, but it is easier to analyse and manipulate individual Graph objects rather than a MultiGraph . # example of creating a MultiGraph # all_books_multigraph = nx.from_pandas_edgelist( # books, source='Source', target='Target', # edge_attr=['weight', 'book'], # create_using=nx.MultiGraph) # we create a list of graph objects using # nx.from_pandas_edgelist and specifying # the edge attributes. graphs = [ nx . from_pandas_edgelist ( books [ books . book == i ], source = 'Source' , target = 'Target' , edge_attr = [ 'weight' , 'weight_inv' ]) for i in range ( 1 , 6 )] # The Graph object associated with the first book. graphs [ 0 ] <networkx.classes.graph.Graph at 0x7f6e903d2310> # To access the relationship edges in the graph with # the edge attribute weight data (data=True) relationships = list ( graphs [ 0 ] . edges ( data = True )) relationships [ 0 : 3 ] [('Addam-Marbrand', 'Jaime-Lannister', {'weight': 3, 'weight_inv': 0.3333333333333333}), ('Addam-Marbrand', 'Tywin-Lannister', {'weight': 6, 'weight_inv': 0.16666666666666666}), ('Jaime-Lannister', 'Aerys-II-Targaryen', {'weight': 5, 'weight_inv': 0.2})]","title":"Introduction"},{"location":"05-casestudies/01-gameofthrones/#finding-the-most-important-node-ie-character-in-these-networks","text":"Let's use our network analysis knowledge to decrypt these Graphs that we have just created. Is it Jon Snow, Tyrion, Daenerys, or someone else? Let's see! Network Science offers us many different metrics to measure the importance of a node in a network as we saw in the first part of the tutorial. Note that there is no \"correct\" way of calculating the most important node in a network, every metric has a different meaning. First, let's measure the importance of a node in a network by looking at the number of neighbors it has, that is, the number of nodes it is connected to. For example, an influential account on Twitter, where the follower-followee relationship forms the network, is an account which has a high number of followers. This measure of importance is called degree centrality. Using this measure, let's extract the top ten important characters from the first book ( graphs[0] ) and the fifth book ( graphs[4] ). NOTE: We are using zero-indexing and that's why the graph of the first book is acceseed by graphs[0] . # We use the in-built degree_centrality method deg_cen_book1 = nx . degree_centrality ( graphs [ 0 ]) deg_cen_book5 = nx . degree_centrality ( graphs [ 4 ]) degree_centrality returns a dictionary and to access the results we can directly use the name of the character. deg_cen_book1 [ 'Daenerys-Targaryen' ] 0.11290322580645162 Top 5 important characters in the first book according to degree centrality. # The following expression sorts the dictionary by # degree centrality and returns the top 5 from a graph sorted ( deg_cen_book1 . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('Eddard-Stark', 0.3548387096774194), ('Robert-Baratheon', 0.2688172043010753), ('Tyrion-Lannister', 0.24731182795698928), ('Catelyn-Stark', 0.23118279569892475), ('Jon-Snow', 0.19892473118279572)] Top 5 important characters in the fifth book according to degree centrality. sorted ( deg_cen_book5 . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('Jon-Snow', 0.1962025316455696), ('Daenerys-Targaryen', 0.18354430379746836), ('Stannis-Baratheon', 0.14873417721518986), ('Tyrion-Lannister', 0.10443037974683544), ('Theon-Greyjoy', 0.10443037974683544)] To visualize the distribution of degree centrality let's plot a histogram of degree centrality. plt . hist ( deg_cen_book1 . values (), bins = 30 ) plt . show () The above plot shows something that is expected, a high portion of characters aren't connected to lot of other characters while some characters are highly connected all through the network. A close real world example of this is a social network like Twitter where a few people have millions of connections(followers) but majority of users aren't connected to that many other users. This exponential decay like property resembles power law in real life networks. # A log-log plot to show the \"signature\" of power law in graphs. from collections import Counter hist = Counter ( deg_cen_book1 . values ()) plt . scatter ( np . log2 ( list ( hist . keys ())), np . log2 ( list ( hist . values ())), alpha = 0.9 ) plt . show ()","title":"Finding the most important node i.e character in these networks."},{"location":"05-casestudies/01-gameofthrones/#exercise","text":"Create a new centrality measure, weighted_degree(Graph, weight) which takes in Graph and the weight attribute and returns a weighted degree dictionary. Weighted degree is calculated by summing the weight of the all edges of a node and find the top five characters according to this measure. from nams.solutions.got import weighted_degree plt . hist ( list ( weighted_degree ( graphs [ 0 ], 'weight' ) . values ()), bins = 30 ) plt . show () sorted ( weighted_degree ( graphs [ 0 ], 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('Eddard-Stark', 1284), ('Robert-Baratheon', 941), ('Jon-Snow', 784), ('Tyrion-Lannister', 650), ('Sansa-Stark', 545)]","title":"Exercise"},{"location":"05-casestudies/01-gameofthrones/#betweeness-centrality","text":"Let's do this for Betweeness centrality and check if this makes any difference. As different centrality method use different measures underneath, they find nodes which are important in the network. A centrality method like Betweeness centrality finds nodes which are structurally important to the network, which binds the network together and densely. # First check unweighted (just the structure) sorted ( nx . betweenness_centrality ( graphs [ 0 ]) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.2696038913836117), ('Robert-Baratheon', 0.21403028397371796), ('Tyrion-Lannister', 0.1902124972697492), ('Jon-Snow', 0.17158135899829566), ('Catelyn-Stark', 0.1513952715347627), ('Daenerys-Targaryen', 0.08627015537511595), ('Robb-Stark', 0.07298399629664767), ('Drogo', 0.06481224290874964), ('Bran-Stark', 0.05579958811784442), ('Sansa-Stark', 0.03714483664326785)] # Let's care about interactions now sorted ( nx . betweenness_centrality ( graphs [ 0 ], weight = 'weight_inv' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.5926474861958733), ('Catelyn-Stark', 0.36855565242662014), ('Jon-Snow', 0.3514094739901191), ('Robert-Baratheon', 0.3329991281604185), ('Tyrion-Lannister', 0.27137460040685846), ('Daenerys-Targaryen', 0.202615518744551), ('Bran-Stark', 0.0945655332752107), ('Robb-Stark', 0.09177564661435629), ('Arya-Stark', 0.06939843068875327), ('Sansa-Stark', 0.06870095902353966)] We can see there are some differences between the unweighted and weighted centrality measures. Another thing to note is that we are using the weight_inv attribute instead of weight(the number of interactions between characters). This decision is based on the way we want to assign the notion of \"importance\" of a character. The basic idea behind betweenness centrality is to find nodes which are essential to the structure of the network. As betweenness centrality computes shortest paths underneath, in the case of weighted betweenness centrality it will end up penalising characters with high number of interactions. By using weight_inv we will prop up the characters with high interactions with other characters.","title":"Betweeness centrality"},{"location":"05-casestudies/01-gameofthrones/#pagerank","text":"The billion dollar algorithm, PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. NOTE: We don't need to worry about weight and weight_inv in PageRank as the algorithm uses weights in the opposite sense (larger weights are better). This may seem confusing as different centrality measures have different definition of weights. So it is always better to have a look at documentation before using weights in a centrality measure. # by default weight attribute in PageRank is weight # so we use weight=None to find the unweighted results sorted ( nx . pagerank_numpy ( graphs [ 0 ], weight = None ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.04552079222830667), ('Tyrion-Lannister', 0.03301362462493266), ('Catelyn-Stark', 0.03019310528663196), ('Robert-Baratheon', 0.029834742227736733), ('Jon-Snow', 0.026834499522066214), ('Robb-Stark', 0.02156294129724751), ('Sansa-Stark', 0.02000803404286464), ('Bran-Stark', 0.01994578678623833), ('Jaime-Lannister', 0.017507847202846878), ('Cersei-Lannister', 0.01708260458475807)] sorted ( nx . pagerank_numpy ( graphs [ 0 ], weight = 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('Eddard-Stark', 0.07239401100498243), ('Robert-Baratheon', 0.048517275705099325), ('Jon-Snow', 0.047706890624749115), ('Tyrion-Lannister', 0.04367437892706291), ('Catelyn-Stark', 0.034667034701307387), ('Bran-Stark', 0.029774200539800195), ('Robb-Stark', 0.02921618364519685), ('Daenerys-Targaryen', 0.02708962251302115), ('Sansa-Stark', 0.02696177891568309), ('Cersei-Lannister', 0.021631679397418963)]","title":"PageRank"},{"location":"05-casestudies/01-gameofthrones/#exercise_1","text":"","title":"Exercise"},{"location":"05-casestudies/01-gameofthrones/#is-there-a-correlation-between-these-techniques","text":"Find the correlation between these four techniques. pagerank (weight = 'weight') betweenness_centrality (weight = 'weight_inv') weighted_degree degree centrality HINT: Use pandas correlation from nams.solutions.got import correlation_centrality correlation_centrality ( graphs [ 0 ]) 0 1 2 3 0 1 0.910352 0.992166 0.949307 1 0.910352 1 0.87924 0.790526 2 0.992166 0.87924 1 0.95506 3 0.949307 0.790526 0.95506 1","title":"Is there a correlation between these techniques?"},{"location":"05-casestudies/01-gameofthrones/#evolution-of-importance-of-characters-over-the-books","text":"According to degree centrality the most important character in the first book is Eddard Stark but he is not even in the top 10 of the fifth book. The importance changes over the course of five books, because you know stuff happens ;) Let's look at the evolution of degree centrality of a couple of characters like Eddard Stark, Jon Snow, Tyrion which showed up in the top 10 of degree centrality in first book. We create a dataframe with character columns and index as books where every entry is the degree centrality of the character in that particular book and plot the evolution of degree centrality Eddard Stark, Jon Snow and Tyrion. We can see that the importance of Eddard Stark in the network dies off and with Jon Snow there is a drop in the fourth book but a sudden rise in the fifth book evol = [ nx . degree_centrality ( graph ) for graph in graphs ] evol_df = pd . DataFrame . from_records ( evol ) . fillna ( 0 ) evol_df [[ 'Eddard-Stark' , 'Tyrion-Lannister' , 'Jon-Snow' ]] . plot () plt . show () set_of_char = set () for i in range ( 5 ): set_of_char |= set ( list ( evol_df . T [ i ] . sort_values ( ascending = False )[ 0 : 5 ] . index )) set_of_char {'Arya-Stark', 'Brienne-of-Tarth', 'Catelyn-Stark', 'Cersei-Lannister', 'Daenerys-Targaryen', 'Eddard-Stark', 'Jaime-Lannister', 'Joffrey-Baratheon', 'Jon-Snow', 'Margaery-Tyrell', 'Robb-Stark', 'Robert-Baratheon', 'Sansa-Stark', 'Stannis-Baratheon', 'Theon-Greyjoy', 'Tyrion-Lannister'}","title":"Evolution of importance of characters over the books"},{"location":"05-casestudies/01-gameofthrones/#exercise_2","text":"Plot the evolution of betweenness centrality of the above mentioned characters over the 5 books. from nams.solutions.got import evol_betweenness evol_betweenness ( graphs )","title":"Exercise"},{"location":"05-casestudies/01-gameofthrones/#so-whats-up-with-stannis-baratheon","text":"sorted ( nx . degree_centrality ( graphs [ 4 ]) . items (), key = lambda x : x [ 1 ], reverse = True )[: 5 ] [('Jon-Snow', 0.1962025316455696), ('Daenerys-Targaryen', 0.18354430379746836), ('Stannis-Baratheon', 0.14873417721518986), ('Tyrion-Lannister', 0.10443037974683544), ('Theon-Greyjoy', 0.10443037974683544)] sorted ( nx . betweenness_centrality ( graphs [ 4 ]) . items (), key = lambda x : x [ 1 ], reverse = True )[: 5 ] [('Stannis-Baratheon', 0.45283060689247934), ('Daenerys-Targaryen', 0.2959459062106149), ('Jon-Snow', 0.24484873673158666), ('Tyrion-Lannister', 0.20961613179551256), ('Robert-Baratheon', 0.17716906651536968)] nx . draw ( nx . barbell_graph ( 5 , 1 ), with_labels = True ) As we know the a higher betweenness centrality means that the node is crucial for the structure of the network, and in the case of Stannis Baratheon in the fifth book it seems like Stannis Baratheon has characterstics similar to that of node 5 in the above example as it seems to be the holding the network together. As evident from the betweenness centrality scores of the above example of barbell graph, node 5 is the most important node in this network. nx . betweenness_centrality ( nx . barbell_graph ( 5 , 1 )) {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.5333333333333333, 6: 0.5333333333333333, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 5: 0.5555555555555556}","title":"So what's up with Stannis Baratheon?"},{"location":"05-casestudies/01-gameofthrones/#community-detection-in-networks","text":"A network is said to have community structure if the nodes of the network can be easily grouped into (potentially overlapping) sets of nodes such that each set of nodes is densely connected internally. There are multiple algorithms and definitions to calculate these communites in a network. We will use louvain community detection algorithm to find the modules in our graph. import nxviz as nv from nxviz import annotate plt . figure ( figsize = ( 8 , 8 )) partition = community . best_partition ( graphs [ 0 ], randomize = False ) # Annotate nodes' partitions for n in graphs [ 0 ] . nodes (): graphs [ 0 ] . nodes [ n ][ \"partition\" ] = partition [ n ] graphs [ 0 ] . nodes [ n ][ \"degree\" ] = graphs [ 0 ] . degree ( n ) nv . matrix ( graphs [ 0 ], group_by = \"partition\" , sort_by = \"degree\" , node_color_by = \"partition\" ) annotate . matrix_block ( graphs [ 0 ], group_by = \"partition\" , color_by = \"partition\" ) annotate . matrix_group ( graphs [ 0 ], group_by = \"partition\" , offset =- 8 ) A common defining quality of a community is that the within-community edges are denser than the between-community edges. # louvain community detection find us 8 different set of communities partition_dict = {} for character , par in partition . items (): if par in partition_dict : partition_dict [ par ] . append ( character ) else : partition_dict [ par ] = [ character ] len ( partition_dict ) 8 partition_dict [ 2 ] ['Bran-Stark', 'Rickon-Stark', 'Robb-Stark', 'Luwin', 'Theon-Greyjoy', 'Hali', 'Hallis-Mollen', 'Hodor', 'Hullen', 'Joseth', 'Nan', 'Osha', 'Rickard-Karstark', 'Rickard-Stark', 'Stiv', 'Jon-Umber-(Greatjon)', 'Galbart-Glover', 'Roose-Bolton', 'Maege-Mormont'] If we plot these communities of the network we see a denser network as compared to the original network which contains all the characters. nx . draw ( nx . subgraph ( graphs [ 0 ], partition_dict [ 3 ])) nx . draw ( nx . subgraph ( graphs [ 0 ], partition_dict [ 1 ])) We can test this by calculating the density of the network and the community. Like in the following example the network between characters in a community is 5 times more dense than the original network. nx . density ( nx . subgraph ( graphs [ 0 ], partition_dict [ 4 ]) ) / nx . density ( graphs [ 0 ]) 25.42543859649123","title":"Community detection in Networks"},{"location":"05-casestudies/01-gameofthrones/#exercise_3","text":"Find the most important node in the partitions according to degree centrality of the nodes using the partition_dict we have already created. from nams.solutions.got import most_important_node_in_partition most_important_node_in_partition ( graphs [ 0 ], partition_dict ) {7: 'Tyrion-Lannister', 1: 'Daenerys-Targaryen', 6: 'Eddard-Stark', 3: 'Jon-Snow', 5: 'Sansa-Stark', 2: 'Robb-Stark', 0: 'Waymar-Royce', 4: 'Danwell-Frey'}","title":"Exercise"},{"location":"05-casestudies/01-gameofthrones/#solutions","text":"Here are the solutions to the exercises above. from nams.solutions import got import inspect print ( inspect . getsource ( got )) import pandas as pd import networkx as nx def weighted_degree(G, weight): result = dict() for node in G.nodes(): weight_degree = 0 for n in G.edges([node], data=True): weight_degree += n[2][\"weight\"] result[node] = weight_degree return result def correlation_centrality(G): cor = pd.DataFrame.from_records( [ nx.pagerank_numpy(G, weight=\"weight\"), nx.betweenness_centrality(G, weight=\"weight_inv\"), weighted_degree(G, \"weight\"), nx.degree_centrality(G), ] ) return cor.T.corr() def evol_betweenness(graphs): evol = [nx.betweenness_centrality(graph, weight=\"weight_inv\") for graph in graphs] evol_df = pd.DataFrame.from_records(evol).fillna(0) set_of_char = set() for i in range(5): set_of_char |= set(list(evol_df.T[i].sort_values(ascending=False)[0:5].index)) evol_df[list(set_of_char)].plot(figsize=(19, 10)) def most_important_node_in_partition(graph, partition_dict): max_d = {} deg = nx.degree_centrality(graph) for group in partition_dict: temp = 0 for character in partition_dict[group]: if deg[character] > temp: max_d[group] = character temp = deg[character] return max_d","title":"Solutions"},{"location":"05-casestudies/02-airport/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' import networkx as nx import pandas as pd import matplotlib.pyplot as plt import numpy as np import warnings warnings . filterwarnings ( 'ignore' ) Introduction In this chapter, we will analyse the evolution of US Airport Network between 1990 and 2015. This dataset contains data for 25 years[1995-2015] of flights between various US airports and metadata about these routes. Taken from Bureau of Transportation Statistics, United States Department of Transportation. Let's see what can we make out of this! from nams import load_data as cf pass_air_data = cf . load_airports_data () In the pass_air_data dataframe we have the information of number of people that fly every year on a particular route on the list of airlines that fly that route. pass_air_data . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 0 1990 ABE ACY {'US Airways Inc.'} 73 1 1990 ABE ATL {'Eastern Air Lines Inc.'} 73172 2 1990 ABE AVL {'Westair Airlines Inc.'} 0 3 1990 ABE AVP {'Westair Airlines Inc.', 'US Airways Inc.', 'E... 8397 4 1990 ABE BHM {'Eastern Air Lines Inc.'} 59 Every row in this dataset is a unique route between 2 airports in United States territory in a particular year. Let's see how many people flew from New York JFK to Austin in 2006. NOTE: This will be a fun chapter if you are an aviation geek and like guessing airport IATA codes. jfk_aus_2006 = ( pass_air_data . query ( 'YEAR == 2006' ) . query ( \"ORIGIN == 'JFK' and DEST == 'AUS'\" )) jfk_aus_2006 . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 215634 2006 JFK AUS {'Shuttle America Corp.', 'Ameristar Air Cargo'... 105290 From the above pandas query we see that according to this dataset 105290 passengers travelled from JFK to AUS in the year 2006. But how does this dataset translate to an applied network analysis problem? In the previous chapter we created different graph objects for every book. Let's create a graph object which encompasses all the edges. NetworkX provides us with Multi(Di)Graphs to model networks with multiple edges between two nodes. In this case every row in the dataframe represents a directed edge between two airports, common sense suggests that if there is a flight from airport A to airport B there should definitely be a flight from airport B to airport A, i.e direction of the edge shouldn't matter. But in this dataset we have data for individual directions (A -> B and B -> A) so we create a MultiDiGraph. passenger_graph = nx . from_pandas_edgelist ( pass_air_data , source = 'ORIGIN' , target = 'DEST' , edge_key = 'YEAR' , edge_attr = [ 'PASSENGERS' , 'UNIQUE_CARRIER_NAME' ], create_using = nx . MultiDiGraph ()) We have created a MultiDiGraph object passenger_graph which contains all the information from the dataframe pass_air_data . ORIGIN and DEST represent the column names in the dataframe pass_air_data used to construct the edge. As this is a MultiDiGraph we can also give a name/key to the multiple edges between two nodes and edge_key is used to represent that name and in this graph YEAR is used to distinguish between multiple edges between two nodes. PASSENGERS and UNIQUE_CARRIER_NAME are added as edge attributes which can be accessed using the nodes and the key form the MultiDiGraph object. Let's check if can access the same information (the 2006 route between JFK and AUS) using our passenger_graph . To check an edge between two nodes in a Graph we can use the syntax GraphObject[source][target] and further specify the edge attribute using GraphObject[source][target][attribute] . passenger_graph [ 'JFK' ][ 'AUS' ][ 2006 ] {'PASSENGERS': 105290.0, 'UNIQUE_CARRIER_NAME': \"{'Shuttle America Corp.', 'Ameristar Air Cargo', 'JetBlue Airways', 'United Parcel Service'}\"} Now let's use our new constructed passenger graph to look at the evolution of passenger load over 25 years. # Route betweeen New York-JFK and SFO values = [( year , attr [ 'PASSENGERS' ]) for year , attr in passenger_graph [ 'JFK' ][ 'SFO' ] . items ()] x , y = zip ( * values ) plt . plot ( x , y ) plt . show () We see some usual trends all across the datasets like steep drops in 2001 (after 9/11) and 2008 (recession). # Route betweeen SFO and Chicago-ORD values = [( year , attr [ 'PASSENGERS' ]) for year , attr in passenger_graph [ 'SFO' ][ 'ORD' ] . items ()] x , y = zip ( * values ) plt . plot ( x , y ) plt . show () To find the overall trend, we can use our pass_air_data dataframe to calculate total passengers flown in a year. pass_air_data . groupby ( [ 'YEAR' ]) . sum ()[ 'PASSENGERS' ] . plot () plt . show () Exercise Find the busiest route in 1990 and in 2015 according to number of passengers, and plot the time series of number of passengers on these routes. You can use the DataFrame instead of working with the network. It will be faster :) from nams.solutions.airport import busiest_route , plot_time_series busiest_route ( pass_air_data , 1990 ) . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 3917 1990 LAX HNL {'Heavylift Cargo Airlines Lt', 'Hawaiian Airli... 1.82716e+06 plot_time_series ( pass_air_data , 'LAX' , 'HNL' ) busiest_route ( pass_air_data , 2015 ) . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 445978 2015 LAX SFO {'Hawaiian Airlines Inc.', 'Delta Air Lines Inc... 1.86907e+06 plot_time_series ( pass_air_data , 'LAX' , 'SFO' ) Before moving to the next part of the chapter let's create a method to extract edges from passenger_graph for a particular year so we can better analyse the data on a granular scale. def year_network ( G , year ): \"\"\" Extract edges for a particular year from a MultiGraph. The edge is also populated with two attributes, weight and weight_inv where weight is the number of passengers and weight_inv the inverse of it. \"\"\" year_network = nx . DiGraph () for edge in G . edges : source , target , edge_year = edge if edge_year == year : attr = G [ source ][ target ][ edge_year ] year_network . add_edge ( source , target , weight = attr [ 'PASSENGERS' ], weight_inv = 1 / ( attr [ 'PASSENGERS' ] if attr [ 'PASSENGERS' ] != 0.0 else 1 ), airlines = attr [ 'UNIQUE_CARRIER_NAME' ]) return year_network pass_2015_network = year_network ( passenger_graph , 2015 ) # Extracted a Directed Graph from the Multi Directed Graph # Number of nodes = airports # Number of edges = routes print ( nx . info ( pass_2015_network )) Name: Type: DiGraph Number of nodes: 1258 Number of edges: 25354 Average in degree: 20.1542 Average out degree: 20.1542 Visualise the airports # Loadin the GPS coordinates of all the airports from nams import load_data as cf lat_long = cf . load_airports_GPS_data () lat_long . columns = [ \"CODE4\" , \"CODE3\" , \"CITY\" , \"PROVINCE\" , \"COUNTRY\" , \"UNKNOWN1\" , \"UNKNOWN2\" , \"UNKNOWN3\" , \"UNKNOWN4\" , \"UNKNOWN5\" , \"UNKNOWN6\" , \"UNKNOWN7\" , \"UNKNOWN8\" , \"UNKNOWN9\" , \"LATITUDE\" , \"LONGITUDE\" ] lat_long wanted_nodes = list ( pass_2015_network . nodes ()) us_airports = lat_long . query ( \"CODE3 in @wanted_nodes\" ) . drop_duplicates ( subset = [ \"CODE3\" ]) . set_index ( \"CODE3\" ) us_airports . head () # us_airports CODE3 CODE4 CITY PROVINCE COUNTRY UNKNOWN1 UNKNOWN2 UNKNOWN3 UNKNOWN4 UNKNOWN5 UNKNOWN6 UNKNOWN7 UNKNOWN8 UNKNOWN9 LATITUDE LONGITUDE ABI KABI ABILENE RGNL ABILENE USA 32 24 40 N 99 40 54 W 546 32.411 -99.682 ABQ KABQ nan ALBUQUERQUE USA 0 0 0 U 0 0 0 U 0 0 0 ACK KACK NANTUCKET MEM NANTUCKET USA 41 15 10 N 70 3 36 W 15 41.253 -70.06 ACT KACT WACO RGNL WACO USA 31 36 40 N 97 13 49 W 158 31.611 -97.23 ACY KACY ATLANTIC CITY INTERNATIONAL ATLANTIC CITY USA 39 27 27 N 74 34 37 W 23 39.458 -74.577 # Annotate graph with latitude and longitude no_gps = [] for n , d in pass_2015_network . nodes ( data = True ): try : pass_2015_network . nodes [ n ][ \"longitude\" ] = us_airports . loc [ n , \"LONGITUDE\" ] pass_2015_network . nodes [ n ][ \"latitude\" ] = us_airports . loc [ n , \"LATITUDE\" ] pass_2015_network . nodes [ n ][ \"degree\" ] = pass_2015_network . degree ( n ) # Some of the nodes are not represented except KeyError : no_gps . append ( n ) # Get subgraph of nodes that do have GPS coords has_gps = set ( pass_2015_network . nodes ()) . difference ( no_gps ) g = pass_2015_network . subgraph ( has_gps ) Let's first plot only the nodes, i.e airports. Places like Guam, US Virgin Islands are also included in this dataset as they are treated as domestic airports in this dataset. import nxviz as nv from nxviz import nodes , plots , edges plt . figure ( figsize = ( 20 , 9 )) pos = nodes . geo ( g , encodings_kwargs = { \"size_scale\" : 1 }) plots . aspect_equal () plots . despine () Let's also plot the routes(edges). import nxviz as nv from nxviz import nodes , plots , edges , annotate plt . figure ( figsize = ( 20 , 9 )) pos = nodes . geo ( g , color_by = \"degree\" , encodings_kwargs = { \"size_scale\" : 1 }) edges . line ( g , pos , encodings_kwargs = { \"alpha_scale\" : 0.1 }) annotate . node_colormapping ( g , color_by = \"degree\" ) plots . aspect_equal () plots . despine () Before we proceed further, let's take a detour to briefly discuss directed networks and PageRank. Directed Graphs and PageRank The figure below explains the basic idea behind the PageRank algorithm. The \"importance\" of the node depends on the incoming links to the node, i.e if an \"important\" node A points towards a node B it will increase the PageRank score of node B, and this is run iteratively. In the given figure, even though node C is only connected to one node it is considered \"important\" as the connection is to node B, which is an \"important\" node. Source: Wikipedia To better understand this let's work through an example. # Create an empty directed graph object G = nx . DiGraph () # Add an edge from 1 to 2 with weight 4 G . add_edge ( 1 , 2 , weight = 4 ) print ( G . edges ( data = True )) [(1, 2, {'weight': 4})] # Access edge from 1 to 2 G [ 1 ][ 2 ] {'weight': 4} What happens when we try to access the edge from 2 to 1? G [ 2 ][ 1 ] --------------------------------------------------------------------------- KeyError Traceback ( most recent call last ) < ipython - input - 137 - d6b8db3142ef > in < module > 1 # Access edge from 2 to 1 ----> 2 G [ 2 ][ 1 ] ~/ miniconda3 / envs / nams / lib / python3 .7 / site - packages / networkx / classes / coreviews . py in __getitem__ ( self , key ) 52 53 def __getitem__ ( self , key ): ---> 54 return self . _atlas [ key ] 55 56 def copy ( self ): KeyError : 1 As expected we get an error when we try to access the edge between 2 to 1 as this is a directed graph. G . add_edges_from ([( 1 , 2 ), ( 3 , 2 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 2 ), ( 7 , 2 )]) nx . draw_circular ( G , with_labels = True ) # nv.circos(G, node_enc_kwargs={\"size_scale\": 0.3}) Just by looking at the example above, we can conclude that node 2 should have the highest PageRank score as all the nodes are pointing towards it. This is confirmed by calculating the PageRank of this graph. nx . pagerank ( G ) {1: 0.0826448180198328, 2: 0.5041310918810031, 3: 0.0826448180198328, 4: 0.0826448180198328, 5: 0.0826448180198328, 6: 0.0826448180198328, 7: 0.0826448180198328} What happens when we add an edge from node 5 to node 6. G . add_edge ( 5 , 6 ) # nv.circos(G, node_enc_kwargs={\"size_scale\": 0.3}) nx . draw_circular ( G , with_labels = True ) nx . pagerank ( G ) {1: 0.08024854052495894, 2: 0.4844028780560986, 3: 0.08024854052495894, 4: 0.08024854052495894, 5: 0.08024854052495894, 6: 0.11435441931910648, 7: 0.08024854052495894} As expected there was some change in the scores (an increase for 6) but the overall trend stays the same, with node 2 leading the pack. G . add_edge ( 2 , 8 ) nx . draw_circular ( G , with_labels = True ) Now we have an added an edge from 2 to a new node 8. As node 2 already has a high PageRank score, this should be passed on node 8. Let's see how much difference this can make. nx . pagerank ( G ) {1: 0.05378612718073915, 2: 0.3246687852772877, 3: 0.05378612718073915, 4: 0.05378612718073915, 5: 0.05378612718073915, 6: 0.0766454192258098, 7: 0.05378612718073915, 8: 0.3297551595932067} In this example, node 8 is now even more \"important\" than node 2 even though node 8 has only incoming connection. Let's move back to Airports and use this knowledge to analyse the network. Importants Hubs in the Airport Network So let's have a look at the important nodes in this network, i.e. important airports in this network. We'll use centrality measures like pagerank, betweenness centrality and degree centrality which we gone through in this book. Let's try to calculate the PageRank of passenger_graph . nx . pagerank ( passenger_graph ) --------------------------------------------------------------------------- NetworkXNotImplemented Traceback ( most recent call last ) < ipython - input - 144 - 15 a6f513bf9b > in < module > 1 # Let's try to calulate the PageRank measures of this graph. ----> 2 nx . pagerank ( passenger_graph ) < decorator - gen - 435 > in pagerank ( G , alpha , personalization , max_iter , tol , nstart , weight , dangling ) ~/ miniconda3 / envs / nams / lib / python3 .7 / site - packages / networkx / utils / decorators . py in _not_implemented_for ( not_implement_for_func , * args , ** kwargs ) 78 if match : 79 msg = 'not implemented for %s type' % ' ' . join ( graph_types ) ---> 80 raise nx . NetworkXNotImplemented ( msg ) 81 else : 82 return not_implement_for_func ( * args , ** kwargs ) NetworkXNotImplemented : not implemented for multigraph type As PageRank isn't defined for a MultiGraph in NetworkX we need to use our extracted yearly sub networks. # As pagerank will take weighted measure # by default we pass in None to make this # calculation for unweighted network PR_2015_scores = nx . pagerank ( pass_2015_network , weight = None ) # Let's check the PageRank score for JFK PR_2015_scores [ 'JFK' ] 0.0036376572979606586 # top 10 airports according to unweighted PageRank top_10_pr = sorted ( PR_2015_scores . items (), key = lambda x : x [ 1 ], reverse = True )[: 10 ] # top 10 airports according to unweighted betweenness centrality top_10_bc = sorted ( nx . betweenness_centrality ( pass_2015_network , weight = None ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] # top 10 airports according to degree centrality top_10_dc = sorted ( nx . degree_centrality ( pass_2015_network ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] Before looking at the results do think about what we just calculated and try to guess which airports should come out at the top and be ready to be surprised :D # PageRank top_10_pr [('ANC', 0.010425531156396332), ('HPN', 0.008715287139161587), ('FAI', 0.007865131822111036), ('DFW', 0.007168038232113773), ('DEN', 0.006557279519803018), ('ATL', 0.006367579588749718), ('ORD', 0.006178836107660135), ('YIP', 0.005821525504523931), ('ADQ', 0.005482597083474197), ('MSP', 0.005481962582230961)] # Betweenness Centrality top_10_bc [('ANC', 0.28907458480586606), ('FAI', 0.08042857784594384), ('SEA', 0.06745549919241699), ('HPN', 0.06046810178534726), ('ORD', 0.045544143864829294), ('ADQ', 0.040170160000905696), ('DEN', 0.038543251364241436), ('BFI', 0.03811277548952854), ('MSP', 0.03774809342340624), ('TEB', 0.036229439542316354)] # Degree Centrality top_10_dc [('ATL', 0.3643595863166269), ('ORD', 0.354813046937152), ('DFW', 0.3420843277645187), ('MSP', 0.3261734287987271), ('DEN', 0.31821797931583135), ('ANC', 0.3046937151949085), ('MEM', 0.29196499602227527), ('LAX', 0.2840095465393795), ('IAH', 0.28082736674622116), ('DTW', 0.27446300715990457)] The Degree Centrality results do make sense at first glance, ATL is Atlanta, ORD is Chicago, these are defintely airports one would expect to be at the top of a list which calculates \"importance\" of an airport. But when we look at PageRank and Betweenness Centrality we have an unexpected airport 'ANC'. Do think about measures like PageRank and Betweenness Centrality and what they calculate. Do note that currently we have used the core structure of the network, no other metadata like number of passengers. These are calculations on the unweighted network. 'ANC' is the airport code of Anchorage airport, a place in Alaska, and according to pagerank and betweenness centrality it is the most important airport in this network. Isn't that weird? Thoughts? Looks like 'ANC' is essential to the core structure of the network, as it is the main airport connecting Alaska with other parts of US. This explains the high Betweenness Centrality score and there are flights from other major airports to 'ANC' which explains the high PageRank score. Related blog post: https://toreopsahl.com/2011/08/12/why-anchorage-is-not-that-important-binary-ties-and-sample-selection/ Let's look at weighted version, i.e taking into account the number of people flying to these places. # Recall from the last chapter we use weight_inv # while calculating betweenness centrality sorted ( nx . betweenness_centrality ( pass_2015_network , weight = 'weight_inv' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('SEA', 0.4192179843829966), ('ATL', 0.3589665389741017), ('ANC', 0.32425767084369994), ('LAX', 0.2668567170342895), ('ORD', 0.10008664852621497), ('DEN', 0.0964658422388763), ('MSP', 0.09300021788810685), ('DFW', 0.0926644126226465), ('FAI', 0.08824779747216016), ('BOS', 0.08259764427486331)] sorted ( nx . pagerank ( pass_2015_network , weight = 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('ATL', 0.037535963029303135), ('ORD', 0.028329766122739346), ('SEA', 0.028274564067008245), ('ANC', 0.027127866647567035), ('DFW', 0.02570050418889442), ('DEN', 0.025260024346433315), ('LAX', 0.02394043498608451), ('PHX', 0.018373176636420224), ('CLT', 0.01780703930063076), ('LAS', 0.017649683141049966)] When we adjust for number of passengers we see that we have a reshuffle in the \"importance\" rankings, and they do make a bit more sense now. According to weighted PageRank, Atlanta, Chicago, Seattle the top 3 airports while Anchorage is at 4th rank now. To get an even better picture of this we should do the analyse with more metadata about the routes not just the number of passengers. How reachable is this network? Let's assume you are the Head of Data Science of an airline and your job is to make your airline network as \"connected\" as possible. To translate this problem statement to network science, we calculate the average shortest path length of this network, it gives us an idea about the number of jumps we need to make around the network to go from one airport to any other airport in this network on average. We can use the inbuilt networkx method average_shortest_path_length to find the average shortest path length of a network. nx . average_shortest_path_length ( pass_2015_network ) --------------------------------------------------------------------------- NetworkXError Traceback ( most recent call last ) < ipython - input - 157 - acfe9bf3572a > in < module > ----> 1 nx . average_shortest_path_length ( pass_2015_network ) ~/ miniconda3 / envs / nams / lib / python3 .7 / site - packages / networkx / algorithms / shortest_paths / generic . py in average_shortest_path_length ( G , weight , method ) 401 # Shortest path length is undefined if the graph is disconnected. 402 if G . is_directed () and not nx . is_weakly_connected ( G ): --> 403 raise nx . NetworkXError ( \"Graph is not weakly connected.\" ) 404 if not G . is_directed () and not nx . is_connected ( G ): 405 raise nx . NetworkXError ( \"Graph is not connected.\" ) NetworkXError : Graph is not weakly connected . Wait, What? This network is not \"connected\" (ignore the term weakly for the moment). That seems weird. It means that there are nodes which aren't reachable from other set of nodes, which isn't good news in especially a transporation network. Let's have a look at these far flung airports which aren't reachable. components = list ( nx . weakly_connected_components ( pass_2015_network )) # There are 3 weakly connected components in the network. for c in components : print ( len ( c )) 1255 2 1 # Let's look at the component with 2 and 1 airports respectively. print ( components [ 1 ]) print ( components [ 2 ]) {'SSB', 'SPB'} {'AIK'} The airports SSB and SPB are codes for Seaplanes airports and they have flights to each other so it makes sense that they aren't connected to the larger network of airports. The airport is even more weird as it is in a component in itself, i.e there is a flight from AIK to AIK . After investigating further it just seems like an anomaly in this dataset. AIK_DEST_2015 = pass_air_data [ ( pass_air_data [ 'YEAR' ] == 2015 ) & ( pass_air_data [ 'DEST' ] == 'AIK' )] AIK_DEST_2015 . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 433338 2015 AIK AIK {'Wright Air Service'} 0 # Let's get rid of them, we don't like them pass_2015_network . remove_nodes_from ( [ 'SPB' , 'SSB' , 'AIK' ]) # Our network is now weakly connected nx . is_weakly_connected ( pass_2015_network ) True # It's not strongly connected nx . is_strongly_connected ( pass_2015_network ) False Strongly vs weakly connected graphs. Let's go through an example to understand weakly and strongly connected directed graphs. # NOTE: The notion of strongly and weakly exists only for directed graphs. G = nx . DiGraph () # Let's create a cycle directed graph, 1 -> 2 -> 3 -> 1 G . add_edge ( 1 , 2 ) G . add_edge ( 2 , 3 ) G . add_edge ( 3 , 1 ) nx . draw ( G , with_labels = True ) In the above example we can reach any node irrespective of where we start traversing the network, if we start from 2 we can reach 1 via 3. In this network every node is \"reachable\" from one another, i.e the network is strongly connected. nx . is_strongly_connected ( G ) True # Let's add a new connection G . add_edge ( 3 , 4 ) nx . draw ( G , with_labels = True ) It's evident from the example above that we can't traverse the network graph. If we start from node 4 we are stuck at the node, we don't have any way of leaving node 4. This is assuming we strictly follow the direction of edges. In this case the network isn't strongly connected but if we look at the structure and assume the directions of edges don't matter than we can go to any other node in the network even if we start from node 4. In the case an undirected copy of directed network is connected we call the directed network as weakly connected. nx . is_strongly_connected ( G ) False nx . is_weakly_connected ( G ) True Let's go back to our airport network of 2015. After removing those 3 airports the network is weakly connected. nx . is_weakly_connected ( pass_2015_network ) True nx . is_strongly_connected ( pass_2015_network ) False But our network is still not strongly connected, which essentially means there are airports in the network where you can fly into but not fly back, which doesn't really seem okay strongly_connected_components = list ( nx . strongly_connected_components ( pass_2015_network )) # Let's look at one of the examples of a strong connected component strongly_connected_components [ 0 ] {'BCE'} bce_2015 = ( pass_air_data . query ( 'YEAR == 2015' ) . query ( \"ORIGIN == 'BCE' or DEST == 'BCE'\" )) bce_2015 id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 451074 2015 PGA BCE {'Grand Canyon Airlines, Inc. d/b/a Grand Canyo... 8 As we can see above you can fly into BCE but can't fly out, weird indeed. These airport are small airports with one off schedules flights. For the purposes of our analyses we can ignore such airports. # Let's find the biggest strongly connected component pass_2015_strong_nodes = max ( strongly_connected_components , key = len ) # Create a subgraph with the nodes in the # biggest strongly connected component pass_2015_strong = pass_2015_network . subgraph ( nodes = pass_2015_strong_nodes ) nx . is_strongly_connected ( pass_2015_strong ) True After removing multiple airports we now have a strongly connected airport network. We can now travel from one airport to any other airport in the network. # We started with 1258 airports len ( pass_2015_strong ) 1190 nx . average_shortest_path_length ( pass_2015_strong ) 3.174661992635574 The 3.17 number above represents the average length between 2 airports in the network which means that it's possible to go from one airport to another in this network under 3 layovers, which sounds nice. A more reachable network is better, not necessearily in terms of revenue for the airline but for social health of the air transport network. Exercise How can we decrease the average shortest path length of this network? Think of an effective way to add new edges to decrease the average shortest path length. Let's see if we can come up with a nice way to do this. The rules are simple: - You can't add more than 2% of the current edges( ~500 edges) from nams.solutions.airport import add_opinated_edges new_routes_network = add_opinated_edges ( pass_2015_strong ) nx . average_shortest_path_length ( new_routes_network ) 3.0888508809747615 Using an opinionated heuristic we were able to reduce the average shortest path length of the network. Check the solution below to understand the idea behind the heuristic, do try to come up with your own heuristics. Can we find airline specific reachability? Let's see how we can use the airline metadata to calculate the reachability of a specific airline. # We have access to the airlines that fly the route in the edge attribute airlines pass_2015_network [ 'JFK' ][ 'SFO' ] {'weight': 1179941.0, 'weight_inv': 8.4750000211875e-07, 'airlines': \"{'Delta Air Lines Inc.', 'Virgin America', 'American Airlines Inc.', 'Sun Country Airlines d/b/a MN Airlines', 'JetBlue Airways', 'Vision Airlines', 'United Air Lines Inc.'}\"} # A helper function to extract the airlines names from the edge attribute def str_to_list ( a ): return a [ 1 : - 1 ] . split ( ', ' ) for origin , dest in pass_2015_network . edges (): pass_2015_network [ origin ][ dest ][ 'airlines_list' ] = str_to_list ( ( pass_2015_network [ origin ][ dest ][ 'airlines' ])) Let's extract the network of United Airlines from our airport network. united_network = nx . DiGraph () for origin , dest in pass_2015_network . edges (): if \"'United Air Lines Inc.'\" in pass_2015_network [ origin ][ dest ][ 'airlines_list' ]: united_network . add_edge ( origin , dest , weight = pass_2015_network [ origin ][ dest ][ 'weight' ]) # number of nodes -> airports # number of edges -> routes print ( nx . info ( united_network )) Name: Type: DiGraph Number of nodes: 194 Number of edges: 1894 Average in degree: 9.7629 Average out degree: 9.7629 # Let's find United Hubs according to PageRank sorted ( nx . pagerank ( united_network , weight = 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('ORD', 0.08385772266571424), ('DEN', 0.06816244850418422), ('LAX', 0.053065234147240105), ('IAH', 0.044410609028379185), ('SFO', 0.04326197030283029)] # Let's find United Hubs according to Degree Centrality sorted ( nx . degree_centrality ( united_network ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('ORD', 1.0), ('IAH', 0.9274611398963731), ('DEN', 0.8756476683937824), ('EWR', 0.8134715025906736), ('SFO', 0.6839378238341969)] Solutions Here are the solutions to the exercises above. from nams.solutions import airport import inspect print ( inspect . getsource ( airport )) import networkx as nx import pandas as pd def busiest_route(pass_air_data, year): return pass_air_data[ pass_air_data.groupby([\"YEAR\"])[\"PASSENGERS\"].transform(max) == pass_air_data[\"PASSENGERS\"] ].query(f\"YEAR == {year}\") def plot_time_series(pass_air_data, origin, dest): pass_air_data.query(f\"ORIGIN == '{origin}' and DEST == '{dest}'\").plot( \"YEAR\", \"PASSENGERS\" ) def add_opinated_edges(G): G = nx.DiGraph(G) sort_degree = sorted( nx.degree_centrality(G).items(), key=lambda x: x[1], reverse=True ) top_count = 0 for n, v in sort_degree: count = 0 for node, val in sort_degree: if node != n: if node not in G._adj[n]: G.add_edge(n, node) count += 1 if count == 25: break top_count += 1 if top_count == 20: break return G","title":"Chapter 13: Airport Network"},{"location":"05-casestudies/02-airport/#introduction","text":"In this chapter, we will analyse the evolution of US Airport Network between 1990 and 2015. This dataset contains data for 25 years[1995-2015] of flights between various US airports and metadata about these routes. Taken from Bureau of Transportation Statistics, United States Department of Transportation. Let's see what can we make out of this! from nams import load_data as cf pass_air_data = cf . load_airports_data () In the pass_air_data dataframe we have the information of number of people that fly every year on a particular route on the list of airlines that fly that route. pass_air_data . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 0 1990 ABE ACY {'US Airways Inc.'} 73 1 1990 ABE ATL {'Eastern Air Lines Inc.'} 73172 2 1990 ABE AVL {'Westair Airlines Inc.'} 0 3 1990 ABE AVP {'Westair Airlines Inc.', 'US Airways Inc.', 'E... 8397 4 1990 ABE BHM {'Eastern Air Lines Inc.'} 59 Every row in this dataset is a unique route between 2 airports in United States territory in a particular year. Let's see how many people flew from New York JFK to Austin in 2006. NOTE: This will be a fun chapter if you are an aviation geek and like guessing airport IATA codes. jfk_aus_2006 = ( pass_air_data . query ( 'YEAR == 2006' ) . query ( \"ORIGIN == 'JFK' and DEST == 'AUS'\" )) jfk_aus_2006 . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 215634 2006 JFK AUS {'Shuttle America Corp.', 'Ameristar Air Cargo'... 105290 From the above pandas query we see that according to this dataset 105290 passengers travelled from JFK to AUS in the year 2006. But how does this dataset translate to an applied network analysis problem? In the previous chapter we created different graph objects for every book. Let's create a graph object which encompasses all the edges. NetworkX provides us with Multi(Di)Graphs to model networks with multiple edges between two nodes. In this case every row in the dataframe represents a directed edge between two airports, common sense suggests that if there is a flight from airport A to airport B there should definitely be a flight from airport B to airport A, i.e direction of the edge shouldn't matter. But in this dataset we have data for individual directions (A -> B and B -> A) so we create a MultiDiGraph. passenger_graph = nx . from_pandas_edgelist ( pass_air_data , source = 'ORIGIN' , target = 'DEST' , edge_key = 'YEAR' , edge_attr = [ 'PASSENGERS' , 'UNIQUE_CARRIER_NAME' ], create_using = nx . MultiDiGraph ()) We have created a MultiDiGraph object passenger_graph which contains all the information from the dataframe pass_air_data . ORIGIN and DEST represent the column names in the dataframe pass_air_data used to construct the edge. As this is a MultiDiGraph we can also give a name/key to the multiple edges between two nodes and edge_key is used to represent that name and in this graph YEAR is used to distinguish between multiple edges between two nodes. PASSENGERS and UNIQUE_CARRIER_NAME are added as edge attributes which can be accessed using the nodes and the key form the MultiDiGraph object. Let's check if can access the same information (the 2006 route between JFK and AUS) using our passenger_graph . To check an edge between two nodes in a Graph we can use the syntax GraphObject[source][target] and further specify the edge attribute using GraphObject[source][target][attribute] . passenger_graph [ 'JFK' ][ 'AUS' ][ 2006 ] {'PASSENGERS': 105290.0, 'UNIQUE_CARRIER_NAME': \"{'Shuttle America Corp.', 'Ameristar Air Cargo', 'JetBlue Airways', 'United Parcel Service'}\"} Now let's use our new constructed passenger graph to look at the evolution of passenger load over 25 years. # Route betweeen New York-JFK and SFO values = [( year , attr [ 'PASSENGERS' ]) for year , attr in passenger_graph [ 'JFK' ][ 'SFO' ] . items ()] x , y = zip ( * values ) plt . plot ( x , y ) plt . show () We see some usual trends all across the datasets like steep drops in 2001 (after 9/11) and 2008 (recession). # Route betweeen SFO and Chicago-ORD values = [( year , attr [ 'PASSENGERS' ]) for year , attr in passenger_graph [ 'SFO' ][ 'ORD' ] . items ()] x , y = zip ( * values ) plt . plot ( x , y ) plt . show () To find the overall trend, we can use our pass_air_data dataframe to calculate total passengers flown in a year. pass_air_data . groupby ( [ 'YEAR' ]) . sum ()[ 'PASSENGERS' ] . plot () plt . show ()","title":"Introduction"},{"location":"05-casestudies/02-airport/#exercise","text":"Find the busiest route in 1990 and in 2015 according to number of passengers, and plot the time series of number of passengers on these routes. You can use the DataFrame instead of working with the network. It will be faster :) from nams.solutions.airport import busiest_route , plot_time_series busiest_route ( pass_air_data , 1990 ) . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 3917 1990 LAX HNL {'Heavylift Cargo Airlines Lt', 'Hawaiian Airli... 1.82716e+06 plot_time_series ( pass_air_data , 'LAX' , 'HNL' ) busiest_route ( pass_air_data , 2015 ) . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 445978 2015 LAX SFO {'Hawaiian Airlines Inc.', 'Delta Air Lines Inc... 1.86907e+06 plot_time_series ( pass_air_data , 'LAX' , 'SFO' ) Before moving to the next part of the chapter let's create a method to extract edges from passenger_graph for a particular year so we can better analyse the data on a granular scale. def year_network ( G , year ): \"\"\" Extract edges for a particular year from a MultiGraph. The edge is also populated with two attributes, weight and weight_inv where weight is the number of passengers and weight_inv the inverse of it. \"\"\" year_network = nx . DiGraph () for edge in G . edges : source , target , edge_year = edge if edge_year == year : attr = G [ source ][ target ][ edge_year ] year_network . add_edge ( source , target , weight = attr [ 'PASSENGERS' ], weight_inv = 1 / ( attr [ 'PASSENGERS' ] if attr [ 'PASSENGERS' ] != 0.0 else 1 ), airlines = attr [ 'UNIQUE_CARRIER_NAME' ]) return year_network pass_2015_network = year_network ( passenger_graph , 2015 ) # Extracted a Directed Graph from the Multi Directed Graph # Number of nodes = airports # Number of edges = routes print ( nx . info ( pass_2015_network )) Name: Type: DiGraph Number of nodes: 1258 Number of edges: 25354 Average in degree: 20.1542 Average out degree: 20.1542","title":"Exercise"},{"location":"05-casestudies/02-airport/#visualise-the-airports","text":"# Loadin the GPS coordinates of all the airports from nams import load_data as cf lat_long = cf . load_airports_GPS_data () lat_long . columns = [ \"CODE4\" , \"CODE3\" , \"CITY\" , \"PROVINCE\" , \"COUNTRY\" , \"UNKNOWN1\" , \"UNKNOWN2\" , \"UNKNOWN3\" , \"UNKNOWN4\" , \"UNKNOWN5\" , \"UNKNOWN6\" , \"UNKNOWN7\" , \"UNKNOWN8\" , \"UNKNOWN9\" , \"LATITUDE\" , \"LONGITUDE\" ] lat_long wanted_nodes = list ( pass_2015_network . nodes ()) us_airports = lat_long . query ( \"CODE3 in @wanted_nodes\" ) . drop_duplicates ( subset = [ \"CODE3\" ]) . set_index ( \"CODE3\" ) us_airports . head () # us_airports CODE3 CODE4 CITY PROVINCE COUNTRY UNKNOWN1 UNKNOWN2 UNKNOWN3 UNKNOWN4 UNKNOWN5 UNKNOWN6 UNKNOWN7 UNKNOWN8 UNKNOWN9 LATITUDE LONGITUDE ABI KABI ABILENE RGNL ABILENE USA 32 24 40 N 99 40 54 W 546 32.411 -99.682 ABQ KABQ nan ALBUQUERQUE USA 0 0 0 U 0 0 0 U 0 0 0 ACK KACK NANTUCKET MEM NANTUCKET USA 41 15 10 N 70 3 36 W 15 41.253 -70.06 ACT KACT WACO RGNL WACO USA 31 36 40 N 97 13 49 W 158 31.611 -97.23 ACY KACY ATLANTIC CITY INTERNATIONAL ATLANTIC CITY USA 39 27 27 N 74 34 37 W 23 39.458 -74.577 # Annotate graph with latitude and longitude no_gps = [] for n , d in pass_2015_network . nodes ( data = True ): try : pass_2015_network . nodes [ n ][ \"longitude\" ] = us_airports . loc [ n , \"LONGITUDE\" ] pass_2015_network . nodes [ n ][ \"latitude\" ] = us_airports . loc [ n , \"LATITUDE\" ] pass_2015_network . nodes [ n ][ \"degree\" ] = pass_2015_network . degree ( n ) # Some of the nodes are not represented except KeyError : no_gps . append ( n ) # Get subgraph of nodes that do have GPS coords has_gps = set ( pass_2015_network . nodes ()) . difference ( no_gps ) g = pass_2015_network . subgraph ( has_gps ) Let's first plot only the nodes, i.e airports. Places like Guam, US Virgin Islands are also included in this dataset as they are treated as domestic airports in this dataset. import nxviz as nv from nxviz import nodes , plots , edges plt . figure ( figsize = ( 20 , 9 )) pos = nodes . geo ( g , encodings_kwargs = { \"size_scale\" : 1 }) plots . aspect_equal () plots . despine () Let's also plot the routes(edges). import nxviz as nv from nxviz import nodes , plots , edges , annotate plt . figure ( figsize = ( 20 , 9 )) pos = nodes . geo ( g , color_by = \"degree\" , encodings_kwargs = { \"size_scale\" : 1 }) edges . line ( g , pos , encodings_kwargs = { \"alpha_scale\" : 0.1 }) annotate . node_colormapping ( g , color_by = \"degree\" ) plots . aspect_equal () plots . despine () Before we proceed further, let's take a detour to briefly discuss directed networks and PageRank.","title":"Visualise the airports"},{"location":"05-casestudies/02-airport/#directed-graphs-and-pagerank","text":"The figure below explains the basic idea behind the PageRank algorithm. The \"importance\" of the node depends on the incoming links to the node, i.e if an \"important\" node A points towards a node B it will increase the PageRank score of node B, and this is run iteratively. In the given figure, even though node C is only connected to one node it is considered \"important\" as the connection is to node B, which is an \"important\" node. Source: Wikipedia To better understand this let's work through an example. # Create an empty directed graph object G = nx . DiGraph () # Add an edge from 1 to 2 with weight 4 G . add_edge ( 1 , 2 , weight = 4 ) print ( G . edges ( data = True )) [(1, 2, {'weight': 4})] # Access edge from 1 to 2 G [ 1 ][ 2 ] {'weight': 4} What happens when we try to access the edge from 2 to 1? G [ 2 ][ 1 ] --------------------------------------------------------------------------- KeyError Traceback ( most recent call last ) < ipython - input - 137 - d6b8db3142ef > in < module > 1 # Access edge from 2 to 1 ----> 2 G [ 2 ][ 1 ] ~/ miniconda3 / envs / nams / lib / python3 .7 / site - packages / networkx / classes / coreviews . py in __getitem__ ( self , key ) 52 53 def __getitem__ ( self , key ): ---> 54 return self . _atlas [ key ] 55 56 def copy ( self ): KeyError : 1 As expected we get an error when we try to access the edge between 2 to 1 as this is a directed graph. G . add_edges_from ([( 1 , 2 ), ( 3 , 2 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 2 ), ( 7 , 2 )]) nx . draw_circular ( G , with_labels = True ) # nv.circos(G, node_enc_kwargs={\"size_scale\": 0.3}) Just by looking at the example above, we can conclude that node 2 should have the highest PageRank score as all the nodes are pointing towards it. This is confirmed by calculating the PageRank of this graph. nx . pagerank ( G ) {1: 0.0826448180198328, 2: 0.5041310918810031, 3: 0.0826448180198328, 4: 0.0826448180198328, 5: 0.0826448180198328, 6: 0.0826448180198328, 7: 0.0826448180198328} What happens when we add an edge from node 5 to node 6. G . add_edge ( 5 , 6 ) # nv.circos(G, node_enc_kwargs={\"size_scale\": 0.3}) nx . draw_circular ( G , with_labels = True ) nx . pagerank ( G ) {1: 0.08024854052495894, 2: 0.4844028780560986, 3: 0.08024854052495894, 4: 0.08024854052495894, 5: 0.08024854052495894, 6: 0.11435441931910648, 7: 0.08024854052495894} As expected there was some change in the scores (an increase for 6) but the overall trend stays the same, with node 2 leading the pack. G . add_edge ( 2 , 8 ) nx . draw_circular ( G , with_labels = True ) Now we have an added an edge from 2 to a new node 8. As node 2 already has a high PageRank score, this should be passed on node 8. Let's see how much difference this can make. nx . pagerank ( G ) {1: 0.05378612718073915, 2: 0.3246687852772877, 3: 0.05378612718073915, 4: 0.05378612718073915, 5: 0.05378612718073915, 6: 0.0766454192258098, 7: 0.05378612718073915, 8: 0.3297551595932067} In this example, node 8 is now even more \"important\" than node 2 even though node 8 has only incoming connection. Let's move back to Airports and use this knowledge to analyse the network.","title":"Directed Graphs and PageRank"},{"location":"05-casestudies/02-airport/#importants-hubs-in-the-airport-network","text":"So let's have a look at the important nodes in this network, i.e. important airports in this network. We'll use centrality measures like pagerank, betweenness centrality and degree centrality which we gone through in this book. Let's try to calculate the PageRank of passenger_graph . nx . pagerank ( passenger_graph ) --------------------------------------------------------------------------- NetworkXNotImplemented Traceback ( most recent call last ) < ipython - input - 144 - 15 a6f513bf9b > in < module > 1 # Let's try to calulate the PageRank measures of this graph. ----> 2 nx . pagerank ( passenger_graph ) < decorator - gen - 435 > in pagerank ( G , alpha , personalization , max_iter , tol , nstart , weight , dangling ) ~/ miniconda3 / envs / nams / lib / python3 .7 / site - packages / networkx / utils / decorators . py in _not_implemented_for ( not_implement_for_func , * args , ** kwargs ) 78 if match : 79 msg = 'not implemented for %s type' % ' ' . join ( graph_types ) ---> 80 raise nx . NetworkXNotImplemented ( msg ) 81 else : 82 return not_implement_for_func ( * args , ** kwargs ) NetworkXNotImplemented : not implemented for multigraph type As PageRank isn't defined for a MultiGraph in NetworkX we need to use our extracted yearly sub networks. # As pagerank will take weighted measure # by default we pass in None to make this # calculation for unweighted network PR_2015_scores = nx . pagerank ( pass_2015_network , weight = None ) # Let's check the PageRank score for JFK PR_2015_scores [ 'JFK' ] 0.0036376572979606586 # top 10 airports according to unweighted PageRank top_10_pr = sorted ( PR_2015_scores . items (), key = lambda x : x [ 1 ], reverse = True )[: 10 ] # top 10 airports according to unweighted betweenness centrality top_10_bc = sorted ( nx . betweenness_centrality ( pass_2015_network , weight = None ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] # top 10 airports according to degree centrality top_10_dc = sorted ( nx . degree_centrality ( pass_2015_network ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] Before looking at the results do think about what we just calculated and try to guess which airports should come out at the top and be ready to be surprised :D # PageRank top_10_pr [('ANC', 0.010425531156396332), ('HPN', 0.008715287139161587), ('FAI', 0.007865131822111036), ('DFW', 0.007168038232113773), ('DEN', 0.006557279519803018), ('ATL', 0.006367579588749718), ('ORD', 0.006178836107660135), ('YIP', 0.005821525504523931), ('ADQ', 0.005482597083474197), ('MSP', 0.005481962582230961)] # Betweenness Centrality top_10_bc [('ANC', 0.28907458480586606), ('FAI', 0.08042857784594384), ('SEA', 0.06745549919241699), ('HPN', 0.06046810178534726), ('ORD', 0.045544143864829294), ('ADQ', 0.040170160000905696), ('DEN', 0.038543251364241436), ('BFI', 0.03811277548952854), ('MSP', 0.03774809342340624), ('TEB', 0.036229439542316354)] # Degree Centrality top_10_dc [('ATL', 0.3643595863166269), ('ORD', 0.354813046937152), ('DFW', 0.3420843277645187), ('MSP', 0.3261734287987271), ('DEN', 0.31821797931583135), ('ANC', 0.3046937151949085), ('MEM', 0.29196499602227527), ('LAX', 0.2840095465393795), ('IAH', 0.28082736674622116), ('DTW', 0.27446300715990457)] The Degree Centrality results do make sense at first glance, ATL is Atlanta, ORD is Chicago, these are defintely airports one would expect to be at the top of a list which calculates \"importance\" of an airport. But when we look at PageRank and Betweenness Centrality we have an unexpected airport 'ANC'. Do think about measures like PageRank and Betweenness Centrality and what they calculate. Do note that currently we have used the core structure of the network, no other metadata like number of passengers. These are calculations on the unweighted network. 'ANC' is the airport code of Anchorage airport, a place in Alaska, and according to pagerank and betweenness centrality it is the most important airport in this network. Isn't that weird? Thoughts? Looks like 'ANC' is essential to the core structure of the network, as it is the main airport connecting Alaska with other parts of US. This explains the high Betweenness Centrality score and there are flights from other major airports to 'ANC' which explains the high PageRank score. Related blog post: https://toreopsahl.com/2011/08/12/why-anchorage-is-not-that-important-binary-ties-and-sample-selection/ Let's look at weighted version, i.e taking into account the number of people flying to these places. # Recall from the last chapter we use weight_inv # while calculating betweenness centrality sorted ( nx . betweenness_centrality ( pass_2015_network , weight = 'weight_inv' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('SEA', 0.4192179843829966), ('ATL', 0.3589665389741017), ('ANC', 0.32425767084369994), ('LAX', 0.2668567170342895), ('ORD', 0.10008664852621497), ('DEN', 0.0964658422388763), ('MSP', 0.09300021788810685), ('DFW', 0.0926644126226465), ('FAI', 0.08824779747216016), ('BOS', 0.08259764427486331)] sorted ( nx . pagerank ( pass_2015_network , weight = 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 10 ] [('ATL', 0.037535963029303135), ('ORD', 0.028329766122739346), ('SEA', 0.028274564067008245), ('ANC', 0.027127866647567035), ('DFW', 0.02570050418889442), ('DEN', 0.025260024346433315), ('LAX', 0.02394043498608451), ('PHX', 0.018373176636420224), ('CLT', 0.01780703930063076), ('LAS', 0.017649683141049966)] When we adjust for number of passengers we see that we have a reshuffle in the \"importance\" rankings, and they do make a bit more sense now. According to weighted PageRank, Atlanta, Chicago, Seattle the top 3 airports while Anchorage is at 4th rank now. To get an even better picture of this we should do the analyse with more metadata about the routes not just the number of passengers.","title":"Importants Hubs in the Airport Network"},{"location":"05-casestudies/02-airport/#how-reachable-is-this-network","text":"Let's assume you are the Head of Data Science of an airline and your job is to make your airline network as \"connected\" as possible. To translate this problem statement to network science, we calculate the average shortest path length of this network, it gives us an idea about the number of jumps we need to make around the network to go from one airport to any other airport in this network on average. We can use the inbuilt networkx method average_shortest_path_length to find the average shortest path length of a network. nx . average_shortest_path_length ( pass_2015_network ) --------------------------------------------------------------------------- NetworkXError Traceback ( most recent call last ) < ipython - input - 157 - acfe9bf3572a > in < module > ----> 1 nx . average_shortest_path_length ( pass_2015_network ) ~/ miniconda3 / envs / nams / lib / python3 .7 / site - packages / networkx / algorithms / shortest_paths / generic . py in average_shortest_path_length ( G , weight , method ) 401 # Shortest path length is undefined if the graph is disconnected. 402 if G . is_directed () and not nx . is_weakly_connected ( G ): --> 403 raise nx . NetworkXError ( \"Graph is not weakly connected.\" ) 404 if not G . is_directed () and not nx . is_connected ( G ): 405 raise nx . NetworkXError ( \"Graph is not connected.\" ) NetworkXError : Graph is not weakly connected . Wait, What? This network is not \"connected\" (ignore the term weakly for the moment). That seems weird. It means that there are nodes which aren't reachable from other set of nodes, which isn't good news in especially a transporation network. Let's have a look at these far flung airports which aren't reachable. components = list ( nx . weakly_connected_components ( pass_2015_network )) # There are 3 weakly connected components in the network. for c in components : print ( len ( c )) 1255 2 1 # Let's look at the component with 2 and 1 airports respectively. print ( components [ 1 ]) print ( components [ 2 ]) {'SSB', 'SPB'} {'AIK'} The airports SSB and SPB are codes for Seaplanes airports and they have flights to each other so it makes sense that they aren't connected to the larger network of airports. The airport is even more weird as it is in a component in itself, i.e there is a flight from AIK to AIK . After investigating further it just seems like an anomaly in this dataset. AIK_DEST_2015 = pass_air_data [ ( pass_air_data [ 'YEAR' ] == 2015 ) & ( pass_air_data [ 'DEST' ] == 'AIK' )] AIK_DEST_2015 . head () id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 433338 2015 AIK AIK {'Wright Air Service'} 0 # Let's get rid of them, we don't like them pass_2015_network . remove_nodes_from ( [ 'SPB' , 'SSB' , 'AIK' ]) # Our network is now weakly connected nx . is_weakly_connected ( pass_2015_network ) True # It's not strongly connected nx . is_strongly_connected ( pass_2015_network ) False","title":"How reachable is this network?"},{"location":"05-casestudies/02-airport/#strongly-vs-weakly-connected-graphs","text":"Let's go through an example to understand weakly and strongly connected directed graphs. # NOTE: The notion of strongly and weakly exists only for directed graphs. G = nx . DiGraph () # Let's create a cycle directed graph, 1 -> 2 -> 3 -> 1 G . add_edge ( 1 , 2 ) G . add_edge ( 2 , 3 ) G . add_edge ( 3 , 1 ) nx . draw ( G , with_labels = True ) In the above example we can reach any node irrespective of where we start traversing the network, if we start from 2 we can reach 1 via 3. In this network every node is \"reachable\" from one another, i.e the network is strongly connected. nx . is_strongly_connected ( G ) True # Let's add a new connection G . add_edge ( 3 , 4 ) nx . draw ( G , with_labels = True ) It's evident from the example above that we can't traverse the network graph. If we start from node 4 we are stuck at the node, we don't have any way of leaving node 4. This is assuming we strictly follow the direction of edges. In this case the network isn't strongly connected but if we look at the structure and assume the directions of edges don't matter than we can go to any other node in the network even if we start from node 4. In the case an undirected copy of directed network is connected we call the directed network as weakly connected. nx . is_strongly_connected ( G ) False nx . is_weakly_connected ( G ) True Let's go back to our airport network of 2015. After removing those 3 airports the network is weakly connected. nx . is_weakly_connected ( pass_2015_network ) True nx . is_strongly_connected ( pass_2015_network ) False But our network is still not strongly connected, which essentially means there are airports in the network where you can fly into but not fly back, which doesn't really seem okay strongly_connected_components = list ( nx . strongly_connected_components ( pass_2015_network )) # Let's look at one of the examples of a strong connected component strongly_connected_components [ 0 ] {'BCE'} bce_2015 = ( pass_air_data . query ( 'YEAR == 2015' ) . query ( \"ORIGIN == 'BCE' or DEST == 'BCE'\" )) bce_2015 id YEAR ORIGIN DEST UNIQUE_CARRIER_NAME PASSENGERS 451074 2015 PGA BCE {'Grand Canyon Airlines, Inc. d/b/a Grand Canyo... 8 As we can see above you can fly into BCE but can't fly out, weird indeed. These airport are small airports with one off schedules flights. For the purposes of our analyses we can ignore such airports. # Let's find the biggest strongly connected component pass_2015_strong_nodes = max ( strongly_connected_components , key = len ) # Create a subgraph with the nodes in the # biggest strongly connected component pass_2015_strong = pass_2015_network . subgraph ( nodes = pass_2015_strong_nodes ) nx . is_strongly_connected ( pass_2015_strong ) True After removing multiple airports we now have a strongly connected airport network. We can now travel from one airport to any other airport in the network. # We started with 1258 airports len ( pass_2015_strong ) 1190 nx . average_shortest_path_length ( pass_2015_strong ) 3.174661992635574 The 3.17 number above represents the average length between 2 airports in the network which means that it's possible to go from one airport to another in this network under 3 layovers, which sounds nice. A more reachable network is better, not necessearily in terms of revenue for the airline but for social health of the air transport network.","title":"Strongly vs weakly connected graphs."},{"location":"05-casestudies/02-airport/#exercise_1","text":"How can we decrease the average shortest path length of this network? Think of an effective way to add new edges to decrease the average shortest path length. Let's see if we can come up with a nice way to do this. The rules are simple: - You can't add more than 2% of the current edges( ~500 edges) from nams.solutions.airport import add_opinated_edges new_routes_network = add_opinated_edges ( pass_2015_strong ) nx . average_shortest_path_length ( new_routes_network ) 3.0888508809747615 Using an opinionated heuristic we were able to reduce the average shortest path length of the network. Check the solution below to understand the idea behind the heuristic, do try to come up with your own heuristics.","title":"Exercise"},{"location":"05-casestudies/02-airport/#can-we-find-airline-specific-reachability","text":"Let's see how we can use the airline metadata to calculate the reachability of a specific airline. # We have access to the airlines that fly the route in the edge attribute airlines pass_2015_network [ 'JFK' ][ 'SFO' ] {'weight': 1179941.0, 'weight_inv': 8.4750000211875e-07, 'airlines': \"{'Delta Air Lines Inc.', 'Virgin America', 'American Airlines Inc.', 'Sun Country Airlines d/b/a MN Airlines', 'JetBlue Airways', 'Vision Airlines', 'United Air Lines Inc.'}\"} # A helper function to extract the airlines names from the edge attribute def str_to_list ( a ): return a [ 1 : - 1 ] . split ( ', ' ) for origin , dest in pass_2015_network . edges (): pass_2015_network [ origin ][ dest ][ 'airlines_list' ] = str_to_list ( ( pass_2015_network [ origin ][ dest ][ 'airlines' ])) Let's extract the network of United Airlines from our airport network. united_network = nx . DiGraph () for origin , dest in pass_2015_network . edges (): if \"'United Air Lines Inc.'\" in pass_2015_network [ origin ][ dest ][ 'airlines_list' ]: united_network . add_edge ( origin , dest , weight = pass_2015_network [ origin ][ dest ][ 'weight' ]) # number of nodes -> airports # number of edges -> routes print ( nx . info ( united_network )) Name: Type: DiGraph Number of nodes: 194 Number of edges: 1894 Average in degree: 9.7629 Average out degree: 9.7629 # Let's find United Hubs according to PageRank sorted ( nx . pagerank ( united_network , weight = 'weight' ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('ORD', 0.08385772266571424), ('DEN', 0.06816244850418422), ('LAX', 0.053065234147240105), ('IAH', 0.044410609028379185), ('SFO', 0.04326197030283029)] # Let's find United Hubs according to Degree Centrality sorted ( nx . degree_centrality ( united_network ) . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 : 5 ] [('ORD', 1.0), ('IAH', 0.9274611398963731), ('DEN', 0.8756476683937824), ('EWR', 0.8134715025906736), ('SFO', 0.6839378238341969)]","title":"Can we find airline specific reachability?"},{"location":"05-casestudies/02-airport/#solutions","text":"Here are the solutions to the exercises above. from nams.solutions import airport import inspect print ( inspect . getsource ( airport )) import networkx as nx import pandas as pd def busiest_route(pass_air_data, year): return pass_air_data[ pass_air_data.groupby([\"YEAR\"])[\"PASSENGERS\"].transform(max) == pass_air_data[\"PASSENGERS\"] ].query(f\"YEAR == {year}\") def plot_time_series(pass_air_data, origin, dest): pass_air_data.query(f\"ORIGIN == '{origin}' and DEST == '{dest}'\").plot( \"YEAR\", \"PASSENGERS\" ) def add_opinated_edges(G): G = nx.DiGraph(G) sort_degree = sorted( nx.degree_centrality(G).items(), key=lambda x: x[1], reverse=True ) top_count = 0 for n, v in sort_degree: count = 0 for node, val in sort_degree: if node != n: if node not in G._adj[n]: G.add_edge(n, node) count += 1 if count == 25: break top_count += 1 if top_count == 20: break return G","title":"Solutions"},{"location":"devdocs/style/","text":"This is the style guide for writing notebooks and markdown files for the book. Intended as a guide when there is ambiguity in how to format something. Updated it when new decisions are made for uncertain circumstances. Notebooks Headers Jupyter notebook headers should begin at the 2nd level. In other words: ## Introdction (this is correct!) should be the first header, and not: # Introdction (this is wrong!) This allows mkdocs to insert the \"Chapter X\" heading at the top of the compiled Markdown document. Exercises Exercises should be at the 3rd level of headers. For exercises that yield a plot, allow the exercise cell to be executed. For exercises that modify an object that is used later, allow the exercise cell to be executed. For exercises that are implementation-oriented, and do not affect notebook state, it is recommended that the execution be commented out to save on execution time. For exercises that require answering a question, wrap the answer in a triple quote string, use the markdown package to parse it into HTML, and then use IPython's HTML display facility to show the answer in beautiful HTML. A convenience function called render_html is provided. Here's an example: from nams.functions import render_html def bipartite_degree_centrality_denominator (): ans = \"\"\" Some answer goes here! Written in **Markdown**. \"\"\" return render_html ( ans ) Indentation is super important! Left indentation on the answer string cannot be present, otherwise the answer will not render correctly in HTML form! Solutions Exercise solutions should be placed in the corresponding nams.solutions.<notebook_name_without_extension> Python submodule. Code solutions should always be present at the bottom of the notebook. Use the following code block to help: import inspect from nams.solutions import {{ notebook_name }} print ( inspect ({{ notebook_name }})) Execution Notebooks should run from top-to-bottom without erroring out. Notebooks ideally should run in under 10 seconds. However, if a notebook needs up to 30 seconds to finish execution, that is acceptable. No notebook should take on the order of minutes to finish.","title":"Style Guide"},{"location":"devdocs/style/#notebooks","text":"","title":"Notebooks"},{"location":"devdocs/style/#headers","text":"Jupyter notebook headers should begin at the 2nd level. In other words: ## Introdction (this is correct!) should be the first header, and not: # Introdction (this is wrong!) This allows mkdocs to insert the \"Chapter X\" heading at the top of the compiled Markdown document.","title":"Headers"},{"location":"devdocs/style/#exercises","text":"Exercises should be at the 3rd level of headers. For exercises that yield a plot, allow the exercise cell to be executed. For exercises that modify an object that is used later, allow the exercise cell to be executed. For exercises that are implementation-oriented, and do not affect notebook state, it is recommended that the execution be commented out to save on execution time. For exercises that require answering a question, wrap the answer in a triple quote string, use the markdown package to parse it into HTML, and then use IPython's HTML display facility to show the answer in beautiful HTML. A convenience function called render_html is provided. Here's an example: from nams.functions import render_html def bipartite_degree_centrality_denominator (): ans = \"\"\" Some answer goes here! Written in **Markdown**. \"\"\" return render_html ( ans ) Indentation is super important! Left indentation on the answer string cannot be present, otherwise the answer will not render correctly in HTML form!","title":"Exercises"},{"location":"devdocs/style/#solutions","text":"Exercise solutions should be placed in the corresponding nams.solutions.<notebook_name_without_extension> Python submodule. Code solutions should always be present at the bottom of the notebook. Use the following code block to help: import inspect from nams.solutions import {{ notebook_name }} print ( inspect ({{ notebook_name }}))","title":"Solutions"},{"location":"devdocs/style/#execution","text":"Notebooks should run from top-to-bottom without erroring out. Notebooks ideally should run in under 10 seconds. However, if a notebook needs up to 30 seconds to finish execution, that is acceptable. No notebook should take on the order of minutes to finish.","title":"Execution"}]}